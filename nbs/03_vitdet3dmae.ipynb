{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp vitdet3dmae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE using VitDet3D architecture for encoding\n",
    "#### (and simple ViT for decoding)\n",
    "\n",
    "- [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527)\n",
    "- [Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality](https://arxiv.org/abs/2205.10063)\n",
    "- [Swin MAE: Masked Autoencoders for Small Datasets](https://arxiv.org/abs/2212.13805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of code:\n",
    "(Recommended extension: https://marketplace.visualstudio.com/items?itemName=bierner.markdown-mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph ViTDet3DMAEModel\n",
    "        subgraph ViTDet3DMAEEmbeddings\n",
    "            subgraph ViTDet3DMAEPatchEmbeddings\n",
    "                patch_embeddings[Patchify scan]\n",
    "            end\n",
    "            positional_embeddings[Add Positional Embeddings]\n",
    "            uniform_sampling[Uniform Sampling]\n",
    "            masking[Mask]\n",
    "        end\n",
    "        ViTDet3DMAEEncoder\n",
    "    end\n",
    "\n",
    "    Start -- 3D input scan --> patch_embeddings\n",
    "    patch_embeddings --> positional_embeddings\n",
    "    positional_embeddings --> uniform_sampling\n",
    "    positional_embeddings -- Embeddings --> ViTDet3DMAEEncoder\n",
    "    uniform_sampling -- 25% mask / 12.5% mask --> masking\n",
    "    masking --> ViTDet3DMAEEncoder\n",
    "\n",
    "    ViTDet3DMAEEncoder --> End\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange\n",
    "from medct.vitdet3d import VitDetConfig, VitDet3dEncoder, VitDet3dPreTrainedModel\n",
    "from torch import nn\n",
    "from transformers.models.vit_mae.modeling_vit_mae import ViTMAELayer\n",
    "from transformers.utils import ModelOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "ViTMAEConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"attention_probs_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"decoder_hidden_size\"\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
       "  \u001b[32m\"decoder_intermediate_size\"\u001b[0m: \u001b[1;36m2048\u001b[0m,\n",
       "  \u001b[32m\"decoder_num_attention_heads\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "  \u001b[32m\"decoder_num_hidden_layers\"\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "  \u001b[32m\"hidden_act\"\u001b[0m: \u001b[32m\"gelu\"\u001b[0m,\n",
       "  \u001b[32m\"hidden_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"hidden_size\"\u001b[0m: \u001b[1;36m768\u001b[0m,\n",
       "  \u001b[32m\"image_size\"\u001b[0m: \u001b[1;36m224\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"intermediate_size\"\u001b[0m: \u001b[1;36m3072\u001b[0m,\n",
       "  \u001b[32m\"layer_norm_eps\"\u001b[0m: \u001b[1;36m1e-12\u001b[0m,\n",
       "  \u001b[32m\"mask_ratio\"\u001b[0m: \u001b[1;36m0.75\u001b[0m,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"vit_mae\"\u001b[0m,\n",
       "  \u001b[32m\"norm_pix_loss\"\u001b[0m: false,\n",
       "  \u001b[32m\"num_attention_heads\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"num_channels\"\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "  \u001b[32m\"num_hidden_layers\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"patch_size\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "  \u001b[32m\"qkv_bias\"\u001b[0m: true,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.36.0\"\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.vit_mae import ViTMAEConfig\n",
    "\n",
    "ViTMAEConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# Config inherits hyperparameters from both ViTDet and ViTMAE\n",
    "class ViTDet3DMAEConfig(VitDetConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "        decoder_hidden_size=384,\n",
    "        decoder_intermediate_size=1536,\n",
    "        decoder_learnable_position_embeddings=False,\n",
    "        decoder_num_attention_heads=16,\n",
    "        decoder_num_hidden_layers=8,\n",
    "        hidden_dropout_prob=0.0,\n",
    "        hidden_size=768,\n",
    "        image_size=(64, 512, 512),\n",
    "        intermediate_size=3072,\n",
    "        learnable_position_embeddings=False,\n",
    "        mask_ratio=0.875,\n",
    "        norm_pix_loss=False,\n",
    "        patch_size=(2, 32, 32),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.model_type = \"vitdet3dmae\"\n",
    "\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.decoder_intermediate_size = decoder_intermediate_size\n",
    "        self.decoder_learnable_position_embeddings = decoder_learnable_position_embeddings\n",
    "        self.decoder_num_attention_heads = decoder_num_attention_heads\n",
    "        self.decoder_num_hidden_layers = decoder_num_hidden_layers\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.hidden_size = hidden_size\n",
    "        self.image_size = image_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.learnable_position_embeddings = learnable_position_embeddings\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.grid_size = (\n",
    "            self.image_size[0] // self.patch_size[0],\n",
    "            self.image_size[1] // self.patch_size[1],\n",
    "            self.image_size[2] // self.patch_size[2],\n",
    "        )\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "ViTDet3DMAEConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"attention_probs_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"decoder_hidden_size\"\u001b[0m: \u001b[1;36m384\u001b[0m,\n",
       "  \u001b[32m\"decoder_intermediate_size\"\u001b[0m: \u001b[1;36m1536\u001b[0m,\n",
       "  \u001b[32m\"decoder_learnable_position_embeddings\"\u001b[0m: false,\n",
       "  \u001b[32m\"decoder_num_attention_heads\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "  \u001b[32m\"decoder_num_hidden_layers\"\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "  \u001b[32m\"drop_path_rate\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"grid_size\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m32\u001b[0m,\n",
       "    \u001b[1;36m16\u001b[0m,\n",
       "    \u001b[1;36m16\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"hidden_act\"\u001b[0m: \u001b[32m\"gelu\"\u001b[0m,\n",
       "  \u001b[32m\"hidden_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"hidden_size\"\u001b[0m: \u001b[1;36m768\u001b[0m,\n",
       "  \u001b[32m\"image_size\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m64\u001b[0m,\n",
       "    \u001b[1;36m512\u001b[0m,\n",
       "    \u001b[1;36m512\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"intermediate_size\"\u001b[0m: \u001b[1;36m3072\u001b[0m,\n",
       "  \u001b[32m\"layer_norm_eps\"\u001b[0m: \u001b[1;36m1e-06\u001b[0m,\n",
       "  \u001b[32m\"learnable_position_embeddings\"\u001b[0m: false,\n",
       "  \u001b[32m\"mask_ratio\"\u001b[0m: \u001b[1;36m0.875\u001b[0m,\n",
       "  \u001b[32m\"mlp_ratio\"\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"vitdet\"\u001b[0m,\n",
       "  \u001b[32m\"norm_pix_loss\"\u001b[0m: false,\n",
       "  \u001b[32m\"num_attention_heads\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"num_channels\"\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "  \u001b[32m\"num_hidden_layers\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"num_patches\"\u001b[0m: \u001b[1;36m8192\u001b[0m,\n",
       "  \u001b[32m\"out_features\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"stage12\"\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"out_indices\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m12\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"patch_size\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m2\u001b[0m,\n",
       "    \u001b[1;36m32\u001b[0m,\n",
       "    \u001b[1;36m32\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"pretrain_image_size\"\u001b[0m: \u001b[1;36m224\u001b[0m,\n",
       "  \u001b[32m\"qkv_bias\"\u001b[0m: true,\n",
       "  \u001b[32m\"residual_block_indices\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"stage_names\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"stem\"\u001b[0m,\n",
       "    \u001b[32m\"stage1\"\u001b[0m,\n",
       "    \u001b[32m\"stage2\"\u001b[0m,\n",
       "    \u001b[32m\"stage3\"\u001b[0m,\n",
       "    \u001b[32m\"stage4\"\u001b[0m,\n",
       "    \u001b[32m\"stage5\"\u001b[0m,\n",
       "    \u001b[32m\"stage6\"\u001b[0m,\n",
       "    \u001b[32m\"stage7\"\u001b[0m,\n",
       "    \u001b[32m\"stage8\"\u001b[0m,\n",
       "    \u001b[32m\"stage9\"\u001b[0m,\n",
       "    \u001b[32m\"stage10\"\u001b[0m,\n",
       "    \u001b[32m\"stage11\"\u001b[0m,\n",
       "    \u001b[32m\"stage12\"\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.36.0\"\u001b[0m,\n",
       "  \u001b[32m\"use_absolute_position_embeddings\"\u001b[0m: true,\n",
       "  \u001b[32m\"use_relative_position_embeddings\"\u001b[0m: false,\n",
       "  \u001b[32m\"window_block_indices\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"window_size\"\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTDet3DMAEConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_config = ViTDet3DMAEConfig(\n",
    "    image_size=(32, 128, 128),\n",
    "    patch_size=(2, 16, 32),  # num patches = 16*8*4 = 512\n",
    "    num_channels=1,\n",
    "    mask_ratio=0.75,  # or set it to 0.875\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_3d_position_embeddings(embedding_size, grid_size):\n",
    "    if embedding_size % 6 != 0:\n",
    "        raise ValueError(\"embed_dim must be divisible by 6\")\n",
    "\n",
    "    d, h, w = grid_size\n",
    "\n",
    "    grid_d = np.arange(d, dtype=np.float32)\n",
    "    grid_h = np.arange(h, dtype=np.float32)\n",
    "    grid_w = np.arange(w, dtype=np.float32)\n",
    "\n",
    "    grid = np.meshgrid(grid_w, grid_h, grid_d)\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([3, 1, d, h, w])\n",
    "\n",
    "    omega = np.arange(embedding_size // 6, dtype=float)\n",
    "    omega /= embedding_size / 6.0\n",
    "    omega = 1.0 / 10000**omega\n",
    "\n",
    "    position_embeddings = []\n",
    "    for grid_subset in grid:\n",
    "        grid_subset = grid_subset.reshape(-1)\n",
    "        out = np.einsum(\"m,d->md\", grid_subset, omega)\n",
    "\n",
    "        emb_sin = np.sin(out)\n",
    "        emb_cos = np.cos(out)\n",
    "\n",
    "        emb = np.concatenate([emb_sin, emb_cos], axis=1)\n",
    "        position_embeddings.append(emb)\n",
    "\n",
    "    position_embeddings = np.concatenate(position_embeddings, axis=1)\n",
    "\n",
    "    return position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEPatchEmbeddings(nn.Module):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        image_size, patch_size = config.image_size, config.patch_size\n",
    "        num_channels, hidden_size = config.num_channels, config.hidden_size\n",
    "\n",
    "        if not isinstance(image_size, collections.abc.Iterable) or len(image_size) != 3:\n",
    "            raise ValueError(\"image_size must be given as 3D iterable\")\n",
    "        if not isinstance(patch_size, collections.abc.Iterable) or len(patch_size) != 3:\n",
    "            raise ValueError(\"patch_size must be given as 3D iterable\")\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.projection = nn.Conv3d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor):\n",
    "        num_channels = pixel_values.shape[1]\n",
    "        if num_channels != self.num_channels:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n",
    "                f\" Expected {self.num_channels} but got {num_channels}.\"\n",
    "            )\n",
    "\n",
    "        if self.image_size != pixel_values.shape[2:]:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the spatial dimensions of the pixel values match with the ones set in the configuration.\"\n",
    "                f\" Expected {self.image_size} but got {pixel_values.shape[2:]}.\"\n",
    "            )\n",
    "\n",
    "        # (b, c, z, y, x)\n",
    "        embeddings = self.projection(pixel_values)\n",
    "        # (b, hidden_size, z, y, x)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scan = torch.rand(1, 1, 32, 128, 128)\n",
    "\n",
    "output = ViTDet3DMAEPatchEmbeddings(sample_config)(sample_scan)\n",
    "\n",
    "assert np.prod(output.shape[2:]) == 512\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overall embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEEmbeddings(nn.Module):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.patch_embeddings = ViTDet3DMAEPatchEmbeddings(config)\n",
    "\n",
    "        # Positional embeddings\n",
    "        num_positions = self.config.num_patches  # Not adding class token as the patches can't be flattened in ViTDet3D\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.zeros(1, config.hidden_size, num_positions), requires_grad=config.learnable_position_embeddings\n",
    "        )\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        if not self.config.learnable_position_embeddings:\n",
    "            # initialize (and freeze) position embeddings by sin-cos embedding\n",
    "            position_embeddings = get_3d_position_embeddings(self.config.hidden_size, self.config.grid_size)\n",
    "            self.position_embeddings.data.copy_(torch.from_numpy(position_embeddings.T).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embeddings like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embeddings.projection.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "    def uniform_sampling(self, embeddings: torch.Tensor):\n",
    "        # embeddings: (b, hidden_size, z, y, x)\n",
    "        _, _, Z, Y, X = embeddings.shape\n",
    "        assert Z % 2 == 0 and Y % 2 == 0 and X % 2 == 0, \"Z, Y, X must be even\"\n",
    "\n",
    "        # Only works if mask_ratio is 0.75 or 0.875\n",
    "        assert self.config.mask_ratio in {0.75, 0.875}, \"Mask ratio must be 0.75 or 0.875\"\n",
    "\n",
    "        masks = []\n",
    "        for _ in range(len(embeddings)):\n",
    "            # Perform uniform sampling. Select one patch in a 2x2x2 cube.\n",
    "            # Also add the diagonally opposite patch if mask_ratio == 0.75.\n",
    "            num_decisions = X * Y * Z // 8\n",
    "            if self.config.mask_ratio == 0.75:\n",
    "                mask1 = np.eye(4, dtype=\"int\").repeat(num_decisions, axis=0)  # (num_decisions * 4, 4)\n",
    "                np.random.shuffle(mask1)\n",
    "                mask1 = mask1[:num_decisions]  # (num_decisions, 4)\n",
    "                mask2 = mask1.copy()\n",
    "\n",
    "                mask2[:, (0, 3)] = mask2[:, (3, 0)]\n",
    "                mask2[:, (1, 2)] = mask2[:, (2, 1)]\n",
    "                mask = np.concatenate([mask1, mask2], axis=1)  # (num_decisions, 8)\n",
    "            else:\n",
    "                mask = np.eye(8, dtype=\"int\").repeat(num_decisions, axis=0)  # (num_decisions * 8, 8)\n",
    "                np.random.shuffle(mask)\n",
    "                mask = mask[:num_decisions]  # (num_decisions, 8)\n",
    "\n",
    "            mask = rearrange(\n",
    "                mask, \"(d h w) (p1 p2 p3) -> (d p1) (h p2) (w p3)\", d=Z // 2, h=Y // 2, w=X // 2, p1=2, p2=2, p3=2\n",
    "            )\n",
    "            masks.append(mask)\n",
    "\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        masks = torch.tensor(masks, device=embeddings.device, dtype=torch.int8)\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor):\n",
    "        # Get patch embeddings\n",
    "        embeddings = self.patch_embeddings(pixel_values)\n",
    "        # (b, hidden_size, z, y, x)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        B, C, Z, Y, X = embeddings.shape\n",
    "        embeddings = embeddings + self.position_embeddings.reshape(1, C, Z, Y, X)\n",
    "\n",
    "        # Uniform sampling\n",
    "        masks = self.uniform_sampling(embeddings)\n",
    "\n",
    "        # Get masked embeddings\n",
    "        new_Z, new_Y, new_X = Z // 2, Y // 2, X // 2\n",
    "        if self.config.mask_ratio == 0.75:\n",
    "            new_Z = Z\n",
    "        embeddings = torch.masked_select(embeddings, masks.unsqueeze(1).bool()).reshape(B, C, new_Z, new_Y, new_X)\n",
    "\n",
    "        return embeddings, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scan = torch.rand(2, 1, 32, 128, 128)\n",
    "\n",
    "embeddings, masks = ViTDet3DMAEEmbeddings(sample_config)(sample_scan)\n",
    "\n",
    "embeddings.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int8\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int8\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(masks[0, :2, :4, :4])\n",
    "display(masks[1, :2, :4, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from qer.ai.core.preprocessing.load_scan import load_scan\n",
    "from neuro_utils.visualize import plot_scans\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan = load_scan(r\"/home/users/arjun.agarwal/projects/temp/1.2.840.113619.2.81.290.1.3689.20170516.222854\")\n",
    "scan = sitk.GetArrayFromImage(scan)\n",
    "scan = np.clip(scan, 0, 80)\n",
    "scan = np.pad(scan, ((12, 12), (0, 0), (0, 0)))\n",
    "scan = torch.from_numpy(scan).float()\n",
    "scan = scan.unsqueeze(0).unsqueeze(0)\n",
    "scan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, masks = ViTDet3DMAEEmbeddings(ViTDet3DMAEConfig(num_channels=1))(scan)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_masks = masks.repeat_interleave(2, dim=1).repeat_interleave(32, dim=2).repeat_interleave(32, dim=3)\n",
    "expanded_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317002dc0c894ae5802ec117bf462b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='z', max=63), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scan = scan.squeeze(0).squeeze(0)\n",
    "masked_scan = scan * expanded_masks.squeeze(0)\n",
    "plot_scans([scan, masked_scan])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEEncoder(VitDet3dEncoder):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ViTDet3DMAEModelOutput(ModelOutput):\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    masks: torch.LongTensor = None\n",
    "    hidden_states: tuple[torch.FloatTensor] = None\n",
    "    attentions: tuple[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEPreTrainedModel(VitDet3dPreTrainedModel):\n",
    "    config_class = ViTDet3DMAEConfig\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        super()._init_weights(module)\n",
    "\n",
    "        if isinstance(module, ViTDet3DMAEEmbeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config.initializer_range,\n",
    "            ).to(module.position_embeddings.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEModel(ViTDet3DMAEPreTrainedModel):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = ViTDet3DMAEEmbeddings(config)\n",
    "        self.encoder = ViTDet3DMAEEncoder(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        head_mask: torch.FloatTensor = None,\n",
    "        output_attentions: bool = None,\n",
    "        output_hidden_states: bool = None,\n",
    "        return_dict: bool = None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output, masks = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output,) + encoder_outputs[1:]\n",
    "\n",
    "        return ViTDet3DMAEModelOutput(\n",
    "            last_hidden_state=sequence_output,\n",
    "            masks=masks,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scan = torch.rand(2, 1, 32, 128, 128)\n",
    "\n",
    "model = ViTDet3DMAEModel(sample_config)\n",
    "output = model(sample_scan)\n",
    "\n",
    "output.last_hidden_state.shape, output.masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m86234880\u001b[0m"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x.numel() for x in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ViTDet3DMAEDecoderOutput(ModelOutput):\n",
    "    logits: torch.FloatTensor\n",
    "    hidden_states: tuple[torch.FloatTensor] = None\n",
    "    attentions: tuple[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEDecoder(nn.Module):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig, grid_size: tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.decoder_embed = nn.Linear(config.hidden_size, config.decoder_hidden_size, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, config.decoder_hidden_size, 1, 1, 1))\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.zeros(1, config.decoder_hidden_size, grid_size[0] * grid_size[1] * grid_size[2]),\n",
    "            requires_grad=config.decoder_learnable_position_embeddings,\n",
    "        )\n",
    "\n",
    "        decoder_config = deepcopy(config)\n",
    "        decoder_config.hidden_size = config.decoder_hidden_size\n",
    "        decoder_config.num_hidden_layers = config.decoder_num_hidden_layers\n",
    "        decoder_config.num_attention_heads = config.decoder_num_attention_heads\n",
    "        decoder_config.intermediate_size = config.decoder_intermediate_size\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [ViTMAELayer(decoder_config) for _ in range(config.decoder_num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "        self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size, eps=config.layer_norm_eps)\n",
    "        self.decoder_pred = nn.Linear(\n",
    "            config.decoder_hidden_size, np.prod(config.patch_size) * config.num_channels, bias=True\n",
    "        )  # encoder to decoder\n",
    "        self.gradient_checkpointing = False\n",
    "        self.config = config\n",
    "        self.initialize_weights(grid_size)\n",
    "\n",
    "    def initialize_weights(self, grid_size):\n",
    "        if not self.config.decoder_learnable_position_embeddings:\n",
    "            # initialize (and freeze) position embeddings by sin-cos embedding\n",
    "            position_embeddings = get_3d_position_embeddings(self.config.decoder_hidden_size, grid_size)\n",
    "            self.position_embeddings.data.copy_(torch.from_numpy(position_embeddings.T).float().unsqueeze(0))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.mask_token, std=self.config.initializer_range)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        masks,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        # embed tokens\n",
    "        (B, C, nD, nH, nW) = hidden_states.shape\n",
    "        hidden_states = hidden_states.permute(0, 2, 3, 4, 1).reshape(-1, C)\n",
    "        x = self.decoder_embed(hidden_states)\n",
    "        x = x.reshape(B, nD, nH, nW, self.config.decoder_hidden_size)\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "\n",
    "        # Create mask tokens array\n",
    "        D, H, W = nD * 2, nH * 2, nW * 2\n",
    "        if self.config.mask_ratio == 0.75:\n",
    "            D = nD\n",
    "        mask_tokens = self.mask_token.repeat(B, 1, D, H, W)\n",
    "\n",
    "        # Add hidden states to this\n",
    "        x = mask_tokens.masked_scatter(masks.unsqueeze(1).bool(), x)\n",
    "\n",
    "        # add pos embed\n",
    "        hidden_states = x + self.position_embeddings.reshape(1, -1, D, H, W)\n",
    "\n",
    "        # Flatten hidden states and reorder them\n",
    "        hidden_states = hidden_states.reshape(B, self.config.decoder_hidden_size, -1)\n",
    "        hidden_states = hidden_states.permute(0, 2, 1)\n",
    "\n",
    "        # apply Transformer layers (blocks)\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        for i, layer_module in enumerate(self.decoder_layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    hidden_states,\n",
    "                    None,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(hidden_states, head_mask=None, output_attentions=output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        hidden_states = self.decoder_norm(hidden_states)\n",
    "\n",
    "        # predictor projection\n",
    "        logits = self.decoder_pred(hidden_states)\n",
    "        logits = logits.permute(0, 2, 1)\n",
    "\n",
    "        # Reshaping back to image size\n",
    "        logits = logits.reshape(B, -1, D, H, W)\n",
    "        # p1, p2, p3 = self.config.patch_size\n",
    "        # logits = rearrange(\n",
    "        #     logits,\n",
    "        #     \"b (c p1 p2 p3) nD nH nW -> b c (nD p1) (nH p2) (nW p3)\",\n",
    "        #     c=self.config.num_channels,\n",
    "        #     p1=p1,\n",
    "        #     p2=p2,\n",
    "        #     p3=p3,\n",
    "        # )  # This reshapes the logits to the original pixel_values shape\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [logits, all_hidden_states, all_self_attentions] if v is not None)\n",
    "\n",
    "        return ViTDet3DMAEDecoderOutput(\n",
    "            logits=logits,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = ViTDet3DMAEDecoder(sample_config, (16, 8, 4))\n",
    "decoded = decoder(output.last_hidden_state, output.masks)\n",
    "\n",
    "decoded.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViTDet3DMAEForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ViTDet3DMAEForPreTrainingOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    masks: torch.LongTensor = None\n",
    "    hidden_states: tuple[torch.FloatTensor] = None\n",
    "    attentions: tuple[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEForPreTraining(ViTDet3DMAEPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.vitdet3d = ViTDet3DMAEModel(config)\n",
    "        self.decoder = ViTDet3DMAEDecoder(config, grid_size=config.grid_size)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward_loss(self, pixel_values, pred, masks_inverse):\n",
    "        B, C, Z, Y, X = pixel_values.shape\n",
    "        nD, nH, nW = self.config.grid_size\n",
    "        D, H, W = self.config.patch_size\n",
    "\n",
    "        target = pixel_values.reshape(B, C, nD, D, nH, H, nW, W)\n",
    "        target = rearrange(target, \"b c nD D nH H nW W -> b c nD nH nW D H W\")\n",
    "        target = target.reshape(*pred.shape)\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * masks_inverse).sum() / masks_inverse.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        head_mask: torch.FloatTensor = None,\n",
    "        output_attentions: bool = None,\n",
    "        output_hidden_states: bool = None,\n",
    "        return_dict: bool = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.vitdet3d(\n",
    "            pixel_values,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        latent = outputs.last_hidden_state\n",
    "        masks = outputs.masks\n",
    "\n",
    "        decoder_outputs = self.decoder(latent, masks)\n",
    "        logits = decoder_outputs.logits\n",
    "\n",
    "        loss = self.forward_loss(pixel_values, logits, 1 - masks)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits, masks) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return ViTDet3DMAEForPreTrainingOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            masks=masks,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.4998\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mDivBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViTDet3DMAEForPreTraining(sample_config)\n",
    "output = model(sample_scan)\n",
    "\n",
    "output.loss, output.logits.shape, output.masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m3242752\u001b[0m"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = torch.rand(5, 1, 32, 128, 128)\n",
    "sample_config = ViTDet3DMAEConfig(\n",
    "    decoder_hidden_size=96,\n",
    "    decoder_intermediate_size=384,\n",
    "    decoder_num_attention_heads=2,\n",
    "    decoder_num_hidden_layers=2,\n",
    "    hidden_size=192,\n",
    "    intermediate_size=768,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    #\n",
    "    image_size=(32, 128, 128),\n",
    "    patch_size=(4, 32, 32),\n",
    "    num_channels=1,\n",
    "    mask_ratio=0.875,\n",
    ")\n",
    "\n",
    "model = ViTDet3DMAEForPreTraining(sample_config)\n",
    "\n",
    "sum(x.numel() for x in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04d346877d64f048697290ff7201f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.377929\tLR: 1.000000\n",
      "Loss: 0.355454\tLR: 1.000000\n",
      "Loss: 0.334494\tLR: 1.000000\n",
      "Loss: 0.315483\tLR: 1.000000\n",
      "Loss: 0.297968\tLR: 1.000000\n",
      "Loss: 0.281640\tLR: 0.900000\n",
      "Loss: 0.267757\tLR: 0.900000\n",
      "Loss: 0.254762\tLR: 0.900000\n",
      "Loss: 0.242536\tLR: 0.900000\n",
      "Loss: 0.231009\tLR: 0.900000\n",
      "Loss: 0.220180\tLR: 0.810000\n",
      "Loss: 0.211066\tLR: 0.810000\n",
      "Loss: 0.202335\tLR: 0.810000\n",
      "Loss: 0.194160\tLR: 0.810000\n",
      "Loss: 0.186445\tLR: 0.810000\n",
      "Loss: 0.179131\tLR: 0.729000\n",
      "Loss: 0.172979\tLR: 0.729000\n",
      "Loss: 0.167132\tLR: 0.729000\n",
      "Loss: 0.161721\tLR: 0.729000\n",
      "Loss: 0.156476\tLR: 0.729000\n",
      "Loss: 0.151539\tLR: 0.656100\n",
      "Loss: 0.147430\tLR: 0.656100\n",
      "Loss: 0.143555\tLR: 0.656100\n",
      "Loss: 0.139819\tLR: 0.656100\n",
      "Loss: 0.136296\tLR: 0.656100\n",
      "Loss: 0.132875\tLR: 0.590490\n",
      "Loss: 0.130188\tLR: 0.590490\n",
      "Loss: 0.127571\tLR: 0.590490\n",
      "Loss: 0.125022\tLR: 0.590490\n",
      "Loss: 0.122547\tLR: 0.590490\n",
      "Loss: 0.120340\tLR: 0.531441\n",
      "Loss: 0.118487\tLR: 0.531441\n",
      "Loss: 0.116557\tLR: 0.531441\n",
      "Loss: 0.114839\tLR: 0.531441\n",
      "Loss: 0.113183\tLR: 0.531441\n",
      "Loss: 0.111592\tLR: 0.478297\n",
      "Loss: 0.110267\tLR: 0.478297\n",
      "Loss: 0.109003\tLR: 0.478297\n",
      "Loss: 0.107695\tLR: 0.478297\n",
      "Loss: 0.106604\tLR: 0.478297\n",
      "Loss: 0.105494\tLR: 0.430467\n",
      "Loss: 0.104460\tLR: 0.430467\n",
      "Loss: 0.103638\tLR: 0.430467\n",
      "Loss: 0.102768\tLR: 0.430467\n",
      "Loss: 0.101903\tLR: 0.430467\n",
      "Loss: 0.101020\tLR: 0.387420\n",
      "Loss: 0.100381\tLR: 0.387420\n",
      "Loss: 0.099766\tLR: 0.387420\n",
      "Loss: 0.099093\tLR: 0.387420\n",
      "Loss: 0.098463\tLR: 0.387420\n",
      "Loss: 0.097935\tLR: 0.348678\n",
      "Loss: 0.097481\tLR: 0.348678\n",
      "Loss: 0.096919\tLR: 0.348678\n",
      "Loss: 0.096442\tLR: 0.348678\n",
      "Loss: 0.096064\tLR: 0.348678\n",
      "Loss: 0.095605\tLR: 0.313811\n",
      "Loss: 0.095242\tLR: 0.313811\n",
      "Loss: 0.094850\tLR: 0.313811\n",
      "Loss: 0.094477\tLR: 0.313811\n",
      "Loss: 0.094160\tLR: 0.313811\n",
      "Loss: 0.093800\tLR: 0.282430\n",
      "Loss: 0.093567\tLR: 0.282430\n",
      "Loss: 0.093240\tLR: 0.282430\n",
      "Loss: 0.093020\tLR: 0.282430\n",
      "Loss: 0.092734\tLR: 0.282430\n",
      "Loss: 0.092443\tLR: 0.254187\n",
      "Loss: 0.092277\tLR: 0.254187\n",
      "Loss: 0.091985\tLR: 0.254187\n",
      "Loss: 0.091801\tLR: 0.254187\n",
      "Loss: 0.091612\tLR: 0.254187\n",
      "Loss: 0.091378\tLR: 0.228768\n",
      "Loss: 0.091219\tLR: 0.228768\n",
      "Loss: 0.091055\tLR: 0.228768\n",
      "Loss: 0.090873\tLR: 0.228768\n",
      "Loss: 0.090733\tLR: 0.228768\n",
      "Loss: 0.090587\tLR: 0.205891\n",
      "Loss: 0.090417\tLR: 0.205891\n",
      "Loss: 0.090301\tLR: 0.205891\n",
      "Loss: 0.090206\tLR: 0.205891\n",
      "Loss: 0.090069\tLR: 0.205891\n",
      "Loss: 0.089911\tLR: 0.185302\n",
      "Loss: 0.089798\tLR: 0.185302\n",
      "Loss: 0.089714\tLR: 0.185302\n",
      "Loss: 0.089593\tLR: 0.185302\n",
      "Loss: 0.089466\tLR: 0.185302\n",
      "Loss: 0.089388\tLR: 0.166772\n",
      "Loss: 0.089308\tLR: 0.166772\n",
      "Loss: 0.089208\tLR: 0.166772\n",
      "Loss: 0.089142\tLR: 0.166772\n",
      "Loss: 0.089013\tLR: 0.166772\n",
      "Loss: 0.088981\tLR: 0.150095\n",
      "Loss: 0.088899\tLR: 0.150095\n",
      "Loss: 0.088808\tLR: 0.150095\n",
      "Loss: 0.088731\tLR: 0.150095\n",
      "Loss: 0.088683\tLR: 0.150095\n",
      "Loss: 0.088624\tLR: 0.135085\n",
      "Loss: 0.088583\tLR: 0.135085\n",
      "Loss: 0.088520\tLR: 0.135085\n",
      "Loss: 0.088495\tLR: 0.135085\n",
      "Loss: 0.088364\tLR: 0.135085\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(100)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_batch)\n",
    "    print(f\"Loss: {output.loss:f}\\tLR: {scheduler.get_last_lr()[0]:f}\")\n",
    "    output.loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
