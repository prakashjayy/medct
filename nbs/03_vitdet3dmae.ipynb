{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp vitdet3dmae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE using VitDet3D architecture for encoding\n",
    "#### (and simple ViT for decoding)\n",
    "\n",
    "- [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527)\n",
    "- [Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality](https://arxiv.org/abs/2205.10063)\n",
    "- [Swin MAE: Masked Autoencoders for Small Datasets](https://arxiv.org/abs/2212.13805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of code:\n",
    "(Recommended extension: https://marketplace.visualstudio.com/items?itemName=bierner.markdown-mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph ViTDet3DMAEModel\n",
    "        subgraph ViTDet3DMAEEmbeddings\n",
    "            subgraph ViTDet3DMAEPatchEmbeddings\n",
    "                patch_embeddings[Patchify scan]\n",
    "            end\n",
    "            positional_embeddings[Add Positional Embeddings]\n",
    "            uniform_sampling[Uniform Sampling]\n",
    "            masking[Mask]\n",
    "        end\n",
    "        ViTDet3DMAEEncoder\n",
    "    end\n",
    "\n",
    "    Start -- 3D input scan --> patch_embeddings\n",
    "    patch_embeddings --> positional_embeddings\n",
    "    positional_embeddings --> uniform_sampling\n",
    "    positional_embeddings -- Embeddings --> ViTDet3DMAEEncoder\n",
    "    uniform_sampling -- 25% mask / 12.5% mask --> masking\n",
    "    masking --> ViTDet3DMAEEncoder\n",
    "\n",
    "    ViTDet3DMAEEncoder --> End\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange\n",
    "from medct.vitdet3d import VitDetConfig, VitDet3dEncoder, VitDet3dPreTrainedModel\n",
    "from torch import nn\n",
    "from transformers.models.vit_mae.modeling_vit_mae import ViTMAELayer\n",
    "from transformers.utils import ModelOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "ViTMAEConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"attention_probs_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"decoder_hidden_size\"\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
       "  \u001b[32m\"decoder_intermediate_size\"\u001b[0m: \u001b[1;36m2048\u001b[0m,\n",
       "  \u001b[32m\"decoder_num_attention_heads\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "  \u001b[32m\"decoder_num_hidden_layers\"\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "  \u001b[32m\"hidden_act\"\u001b[0m: \u001b[32m\"gelu\"\u001b[0m,\n",
       "  \u001b[32m\"hidden_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"hidden_size\"\u001b[0m: \u001b[1;36m768\u001b[0m,\n",
       "  \u001b[32m\"image_size\"\u001b[0m: \u001b[1;36m224\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"intermediate_size\"\u001b[0m: \u001b[1;36m3072\u001b[0m,\n",
       "  \u001b[32m\"layer_norm_eps\"\u001b[0m: \u001b[1;36m1e-12\u001b[0m,\n",
       "  \u001b[32m\"mask_ratio\"\u001b[0m: \u001b[1;36m0.75\u001b[0m,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"vit_mae\"\u001b[0m,\n",
       "  \u001b[32m\"norm_pix_loss\"\u001b[0m: false,\n",
       "  \u001b[32m\"num_attention_heads\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"num_channels\"\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "  \u001b[32m\"num_hidden_layers\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"patch_size\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "  \u001b[32m\"qkv_bias\"\u001b[0m: true,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.36.0\"\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.vit_mae import ViTMAEConfig\n",
    "\n",
    "ViTMAEConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# Config inherits hyperparameters from both ViTDet and ViTMAE\n",
    "class ViTDet3DMAEConfig(VitDetConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "        decoder_hidden_size=384,\n",
    "        decoder_intermediate_size=1536,\n",
    "        decoder_learnable_position_embeddings=False,\n",
    "        decoder_num_attention_heads=16,\n",
    "        decoder_num_hidden_layers=8,\n",
    "        hidden_dropout_prob=0.0,\n",
    "        hidden_size=768,\n",
    "        image_size=(64, 512, 512),\n",
    "        intermediate_size=3072,\n",
    "        learnable_position_embeddings=False,\n",
    "        mask_ratio=0.875,\n",
    "        norm_pix_loss=False,\n",
    "        patch_size=(2, 32, 32),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.model_type = \"vitdet3dmae\"\n",
    "\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.decoder_intermediate_size = decoder_intermediate_size\n",
    "        self.decoder_learnable_position_embeddings = decoder_learnable_position_embeddings\n",
    "        self.decoder_num_attention_heads = decoder_num_attention_heads\n",
    "        self.decoder_num_hidden_layers = decoder_num_hidden_layers\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.hidden_size = hidden_size\n",
    "        self.image_size = image_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.learnable_position_embeddings = learnable_position_embeddings\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.grid_size = (\n",
    "            self.image_size[0] // self.patch_size[0],\n",
    "            self.image_size[1] // self.patch_size[1],\n",
    "            self.image_size[2] // self.patch_size[2],\n",
    "        )\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "ViTDet3DMAEConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"attention_probs_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"decoder_hidden_size\"\u001b[0m: \u001b[1;36m384\u001b[0m,\n",
       "  \u001b[32m\"decoder_intermediate_size\"\u001b[0m: \u001b[1;36m1536\u001b[0m,\n",
       "  \u001b[32m\"decoder_learnable_position_embeddings\"\u001b[0m: false,\n",
       "  \u001b[32m\"decoder_num_attention_heads\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "  \u001b[32m\"decoder_num_hidden_layers\"\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "  \u001b[32m\"drop_path_rate\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"grid_size\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m32\u001b[0m,\n",
       "    \u001b[1;36m16\u001b[0m,\n",
       "    \u001b[1;36m16\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"hidden_act\"\u001b[0m: \u001b[32m\"gelu\"\u001b[0m,\n",
       "  \u001b[32m\"hidden_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"hidden_size\"\u001b[0m: \u001b[1;36m768\u001b[0m,\n",
       "  \u001b[32m\"image_size\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m64\u001b[0m,\n",
       "    \u001b[1;36m512\u001b[0m,\n",
       "    \u001b[1;36m512\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"intermediate_size\"\u001b[0m: \u001b[1;36m3072\u001b[0m,\n",
       "  \u001b[32m\"layer_norm_eps\"\u001b[0m: \u001b[1;36m1e-06\u001b[0m,\n",
       "  \u001b[32m\"learnable_position_embeddings\"\u001b[0m: false,\n",
       "  \u001b[32m\"mask_ratio\"\u001b[0m: \u001b[1;36m0.875\u001b[0m,\n",
       "  \u001b[32m\"mlp_ratio\"\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"vitdet\"\u001b[0m,\n",
       "  \u001b[32m\"norm_pix_loss\"\u001b[0m: false,\n",
       "  \u001b[32m\"num_attention_heads\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"num_channels\"\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "  \u001b[32m\"num_hidden_layers\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"num_patches\"\u001b[0m: \u001b[1;36m8192\u001b[0m,\n",
       "  \u001b[32m\"out_features\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"stage12\"\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"out_indices\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m12\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"patch_size\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m2\u001b[0m,\n",
       "    \u001b[1;36m32\u001b[0m,\n",
       "    \u001b[1;36m32\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"pretrain_image_size\"\u001b[0m: \u001b[1;36m224\u001b[0m,\n",
       "  \u001b[32m\"qkv_bias\"\u001b[0m: true,\n",
       "  \u001b[32m\"residual_block_indices\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"stage_names\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"stem\"\u001b[0m,\n",
       "    \u001b[32m\"stage1\"\u001b[0m,\n",
       "    \u001b[32m\"stage2\"\u001b[0m,\n",
       "    \u001b[32m\"stage3\"\u001b[0m,\n",
       "    \u001b[32m\"stage4\"\u001b[0m,\n",
       "    \u001b[32m\"stage5\"\u001b[0m,\n",
       "    \u001b[32m\"stage6\"\u001b[0m,\n",
       "    \u001b[32m\"stage7\"\u001b[0m,\n",
       "    \u001b[32m\"stage8\"\u001b[0m,\n",
       "    \u001b[32m\"stage9\"\u001b[0m,\n",
       "    \u001b[32m\"stage10\"\u001b[0m,\n",
       "    \u001b[32m\"stage11\"\u001b[0m,\n",
       "    \u001b[32m\"stage12\"\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.36.0\"\u001b[0m,\n",
       "  \u001b[32m\"use_absolute_position_embeddings\"\u001b[0m: true,\n",
       "  \u001b[32m\"use_relative_position_embeddings\"\u001b[0m: false,\n",
       "  \u001b[32m\"window_block_indices\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"window_size\"\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTDet3DMAEConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_config = ViTDet3DMAEConfig(\n",
    "    image_size=(32, 128, 128),\n",
    "    patch_size=(2, 16, 32),  # num patches = 16*8*4 = 512\n",
    "    num_channels=1,\n",
    "    mask_ratio=0.75,  # or set it to 0.875\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_3d_position_embeddings(embedding_size, grid_size):\n",
    "    if embedding_size % 6 != 0:\n",
    "        raise ValueError(\"embed_dim must be divisible by 6\")\n",
    "\n",
    "    d, h, w = grid_size\n",
    "\n",
    "    grid_d = np.arange(d, dtype=np.float32)\n",
    "    grid_h = np.arange(h, dtype=np.float32)\n",
    "    grid_w = np.arange(w, dtype=np.float32)\n",
    "\n",
    "    grid = np.meshgrid(grid_w, grid_h, grid_d)\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([3, 1, d, h, w])\n",
    "\n",
    "    omega = np.arange(embedding_size // 6, dtype=float)\n",
    "    omega /= embedding_size / 6.0\n",
    "    omega = 1.0 / 10000**omega\n",
    "\n",
    "    position_embeddings = []\n",
    "    for grid_subset in grid:\n",
    "        grid_subset = grid_subset.reshape(-1)\n",
    "        out = np.einsum(\"m,d->md\", grid_subset, omega)\n",
    "\n",
    "        emb_sin = np.sin(out)\n",
    "        emb_cos = np.cos(out)\n",
    "\n",
    "        emb = np.concatenate([emb_sin, emb_cos], axis=1)\n",
    "        position_embeddings.append(emb)\n",
    "\n",
    "    position_embeddings = np.concatenate(position_embeddings, axis=1)\n",
    "\n",
    "    return position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEPatchEmbeddings(nn.Module):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        image_size, patch_size = config.image_size, config.patch_size\n",
    "        num_channels, hidden_size = config.num_channels, config.hidden_size\n",
    "\n",
    "        if not isinstance(image_size, collections.abc.Iterable) or len(image_size) != 3:\n",
    "            raise ValueError(\"image_size must be given as 3D iterable\")\n",
    "        if not isinstance(patch_size, collections.abc.Iterable) or len(patch_size) != 3:\n",
    "            raise ValueError(\"patch_size must be given as 3D iterable\")\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.projection = nn.Conv3d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor):\n",
    "        num_channels = pixel_values.shape[1]\n",
    "        if num_channels != self.num_channels:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n",
    "                f\" Expected {self.num_channels} but got {num_channels}.\"\n",
    "            )\n",
    "\n",
    "        if self.image_size != pixel_values.shape[2:]:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the spatial dimensions of the pixel values match with the ones set in the configuration.\"\n",
    "                f\" Expected {self.image_size} but got {pixel_values.shape[2:]}.\"\n",
    "            )\n",
    "\n",
    "        # (b, c, z, y, x)\n",
    "        embeddings = self.projection(pixel_values)\n",
    "        # (b, hidden_size, z, y, x)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scan = torch.rand(1, 1, 32, 128, 128)\n",
    "\n",
    "output = ViTDet3DMAEPatchEmbeddings(sample_config)(sample_scan)\n",
    "\n",
    "assert np.prod(output.shape[2:]) == 512\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overall embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEEmbeddings(nn.Module):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.patch_embeddings = ViTDet3DMAEPatchEmbeddings(config)\n",
    "\n",
    "        # Positional embeddings\n",
    "        num_positions = self.config.num_patches  # Not adding class token as the patches can't be flattened in ViTDet3D\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.zeros(1, config.hidden_size, num_positions), requires_grad=config.learnable_position_embeddings\n",
    "        )\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        if not self.config.learnable_position_embeddings:\n",
    "            # initialize (and freeze) position embeddings by sin-cos embedding\n",
    "            position_embeddings = get_3d_position_embeddings(self.config.hidden_size, self.config.grid_size)\n",
    "            self.position_embeddings.data.copy_(torch.from_numpy(position_embeddings.T).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embeddings like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embeddings.projection.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "    def uniform_sampling(self, embeddings: torch.Tensor):\n",
    "        # embeddings: (b, hidden_size, z, y, x)\n",
    "        _, _, Z, Y, X = embeddings.shape\n",
    "        assert Z % 2 == 0 and Y % 2 == 0 and X % 2 == 0, \"Z, Y, X must be even\"\n",
    "\n",
    "        # Only works if mask_ratio is 0.75 or 0.875\n",
    "        assert self.config.mask_ratio in {0.75, 0.875}, \"Mask ratio must be 0.75 or 0.875\"\n",
    "\n",
    "        masks = []\n",
    "        for _ in range(len(embeddings)):\n",
    "            # Perform uniform sampling. Select one patch in a 2x2x2 cube.\n",
    "            # Also add the diagonally opposite patch if mask_ratio == 0.75.\n",
    "            num_decisions = X * Y * Z // 8\n",
    "            if self.config.mask_ratio == 0.75:\n",
    "                mask1 = np.eye(4, dtype=\"int\").repeat(num_decisions, axis=0)  # (num_decisions * 4, 4)\n",
    "                np.random.shuffle(mask1)\n",
    "                mask1 = mask1[:num_decisions]  # (num_decisions, 4)\n",
    "                mask2 = mask1.copy()\n",
    "\n",
    "                mask2[:, (0, 3)] = mask2[:, (3, 0)]\n",
    "                mask2[:, (1, 2)] = mask2[:, (2, 1)]\n",
    "                mask = np.concatenate([mask1, mask2], axis=1)  # (num_decisions, 8)\n",
    "            else:\n",
    "                mask = np.eye(8, dtype=\"int\").repeat(num_decisions, axis=0)  # (num_decisions * 8, 8)\n",
    "                np.random.shuffle(mask)\n",
    "                mask = mask[:num_decisions]  # (num_decisions, 8)\n",
    "\n",
    "            mask = rearrange(\n",
    "                mask, \"(d h w) (p1 p2 p3) -> (d p1) (h p2) (w p3)\", d=Z // 2, h=Y // 2, w=X // 2, p1=2, p2=2, p3=2\n",
    "            )\n",
    "            masks.append(mask)\n",
    "\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        masks = torch.tensor(masks, device=embeddings.device, dtype=torch.int8)\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor):\n",
    "        # Get patch embeddings\n",
    "        embeddings = self.patch_embeddings(pixel_values)\n",
    "        # (b, hidden_size, z, y, x)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        B, C, Z, Y, X = embeddings.shape\n",
    "        embeddings = embeddings + self.position_embeddings.reshape(1, C, Z, Y, X)\n",
    "\n",
    "        # Uniform sampling\n",
    "        masks = self.uniform_sampling(embeddings)\n",
    "\n",
    "        # Get masked embeddings\n",
    "        new_Z, new_Y, new_X = Z // 2, Y // 2, X // 2\n",
    "        if self.config.mask_ratio == 0.75:\n",
    "            new_Z = Z\n",
    "        embeddings = torch.masked_select(embeddings, masks.unsqueeze(1).bool()).reshape(B, C, new_Z, new_Y, new_X)\n",
    "\n",
    "        return embeddings, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scan = torch.rand(2, 1, 32, 128, 128)\n",
    "\n",
    "embeddings, masks = ViTDet3DMAEEmbeddings(sample_config)(sample_scan)\n",
    "\n",
    "embeddings.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int8\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int8\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(masks[0, :2, :4, :4])\n",
    "display(masks[1, :2, :4, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from qer.ai.core.preprocessing.load_scan import load_scan\n",
    "from neuro_utils.visualize import plot_scans\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan = load_scan(r\"/home/users/arjun.agarwal/projects/temp/1.2.840.113619.2.81.290.1.3689.20170516.222854\")\n",
    "scan = sitk.GetArrayFromImage(scan)\n",
    "scan = np.clip(scan, 0, 80)\n",
    "scan = np.pad(scan, ((12, 12), (0, 0), (0, 0)))\n",
    "scan = torch.from_numpy(scan).float()\n",
    "scan = scan.unsqueeze(0).unsqueeze(0)\n",
    "scan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, masks = ViTDet3DMAEEmbeddings(ViTDet3DMAEConfig(num_channels=1))(scan)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_masks = masks.repeat_interleave(2, dim=1).repeat_interleave(32, dim=2).repeat_interleave(32, dim=3)\n",
    "expanded_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247ca8a15aee455fbc14093454f49415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='z', max=63), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scan = scan.squeeze(0).squeeze(0)\n",
    "masked_scan = scan * expanded_masks.squeeze(0)\n",
    "plot_scans([scan, masked_scan])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEEncoder(VitDet3dEncoder):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ViTDet3DMAEModelOutput(ModelOutput):\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    masks: torch.LongTensor = None\n",
    "    hidden_states: tuple[torch.FloatTensor] = None\n",
    "    attentions: tuple[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEPreTrainedModel(VitDet3dPreTrainedModel):\n",
    "    config_class = ViTDet3DMAEConfig\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        super()._init_weights(module)\n",
    "\n",
    "        if isinstance(module, ViTDet3DMAEEmbeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config.initializer_range,\n",
    "            ).to(module.position_embeddings.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEModel(ViTDet3DMAEPreTrainedModel):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = ViTDet3DMAEEmbeddings(config)\n",
    "        self.encoder = ViTDet3DMAEEncoder(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        head_mask: torch.FloatTensor = None,\n",
    "        output_attentions: bool = None,\n",
    "        output_hidden_states: bool = None,\n",
    "        return_dict: bool = None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output, masks = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output,) + encoder_outputs[1:]\n",
    "\n",
    "        return ViTDet3DMAEModelOutput(\n",
    "            last_hidden_state=sequence_output,\n",
    "            masks=masks,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scan = torch.rand(2, 1, 32, 128, 128)\n",
    "\n",
    "model = ViTDet3DMAEModel(sample_config)\n",
    "output = model(sample_scan)\n",
    "\n",
    "output.last_hidden_state.shape, output.masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m86234880\u001b[0m"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x.numel() for x in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ViTDet3DMAEDecoderOutput(ModelOutput):\n",
    "    logits: torch.FloatTensor\n",
    "    hidden_states: tuple[torch.FloatTensor] = None\n",
    "    attentions: tuple[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEDecoder(nn.Module):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig, grid_size: tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.decoder_embed = nn.Linear(config.hidden_size, config.decoder_hidden_size, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, config.decoder_hidden_size, 1, 1, 1))\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.zeros(1, config.decoder_hidden_size, grid_size[0] * grid_size[1] * grid_size[2]),\n",
    "            requires_grad=config.decoder_learnable_position_embeddings,\n",
    "        )\n",
    "\n",
    "        decoder_config = deepcopy(config)\n",
    "        decoder_config.hidden_size = config.decoder_hidden_size\n",
    "        decoder_config.num_hidden_layers = config.decoder_num_hidden_layers\n",
    "        decoder_config.num_attention_heads = config.decoder_num_attention_heads\n",
    "        decoder_config.intermediate_size = config.decoder_intermediate_size\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [ViTMAELayer(decoder_config) for _ in range(config.decoder_num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "        self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size, eps=config.layer_norm_eps)\n",
    "        self.decoder_pred = nn.Linear(\n",
    "            config.decoder_hidden_size, config.num_channels * np.prod(config.patch_size), bias=True\n",
    "        )  # encoder to decoder\n",
    "        self.gradient_checkpointing = False\n",
    "        self.config = config\n",
    "        self.initialize_weights(grid_size)\n",
    "\n",
    "    def initialize_weights(self, grid_size):\n",
    "        if not self.config.decoder_learnable_position_embeddings:\n",
    "            # initialize (and freeze) position embeddings by sin-cos embedding\n",
    "            position_embeddings = get_3d_position_embeddings(self.config.decoder_hidden_size, grid_size)\n",
    "            self.position_embeddings.data.copy_(torch.from_numpy(position_embeddings.T).float().unsqueeze(0))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.mask_token, std=self.config.initializer_range)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        masks,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        # npZ etc. is the number of patches after uniform sampling\n",
    "        # npD etc. is the number of patches in the original image\n",
    "        # pZ etc. is the patch size\n",
    "        # C is the number of channels in th eoriginal image\n",
    "        # H can be any hidden size\n",
    "\n",
    "        # embed tokens\n",
    "        # (b, encoder_hidden_size, z, y, x)\n",
    "        hidden_states = rearrange(hidden_states, \"b h npZ npY npX -> b npZ npY npX h\")\n",
    "        x = self.decoder_embed(hidden_states)\n",
    "        x = rearrange(x, \"b npZ npY npX h -> b h npZ npY npX\")\n",
    "        # (b, decoder_hidden_size, z, y, x)\n",
    "\n",
    "        # Create mask tokens array\n",
    "        B, _, npZ, npY, npX = x.shape\n",
    "        npD, npH, npW = npZ * 2, npY * 2, npX * 2\n",
    "        if self.config.mask_ratio == 0.75:\n",
    "            npD = npZ\n",
    "        mask_tokens = self.mask_token.repeat(B, 1, npD, npH, npW)\n",
    "\n",
    "        # Add hidden states to this\n",
    "        x = mask_tokens.masked_scatter(masks.unsqueeze(1).bool(), x)\n",
    "\n",
    "        # add pos embed\n",
    "        hidden_states = x + self.position_embeddings.reshape(1, -1, npD, npH, npW)\n",
    "\n",
    "        # Flatten hidden states and reorder them\n",
    "        hidden_states = rearrange(hidden_states, \"b h npD npH npW -> b (npD npH npW) h\")\n",
    "\n",
    "        # apply Transformer layers (blocks)\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        for i, layer_module in enumerate(self.decoder_layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    hidden_states,\n",
    "                    None,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(hidden_states, head_mask=None, output_attentions=output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        hidden_states = self.decoder_norm(hidden_states)\n",
    "\n",
    "        # predictor projection\n",
    "        logits = self.decoder_pred(hidden_states)\n",
    "\n",
    "        # Reshaping back to image patches\n",
    "        pD, pH, pW = self.config.patch_size\n",
    "        logits = rearrange(logits, \"b (npD npH npW) h -> b h npD npH npW\", npD=npD, npH=npH, npW=npW)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [logits, all_hidden_states, all_self_attentions] if v is not None)\n",
    "\n",
    "        return ViTDet3DMAEDecoderOutput(\n",
    "            logits=logits,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = ViTDet3DMAEDecoder(sample_config, (16, 8, 4))\n",
    "decoded = decoder(output.last_hidden_state, output.masks)\n",
    "\n",
    "decoded.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViTDet3DMAEForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ViTDet3DMAEForPreTrainingOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    masks: torch.LongTensor = None\n",
    "    hidden_states: tuple[torch.FloatTensor] = None\n",
    "    attentions: tuple[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEForPreTraining(ViTDet3DMAEPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.vitdet3d = ViTDet3DMAEModel(config)\n",
    "        self.decoder = ViTDet3DMAEDecoder(config, grid_size=config.grid_size)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def patchify(self, pixel_values):\n",
    "        B, C, _, _, _ = pixel_values.shape\n",
    "        npZ, npY, npX = self.config.grid_size\n",
    "        pZ, pY, pX = self.config.patch_size\n",
    "\n",
    "        target = rearrange(\n",
    "            pixel_values,\n",
    "            \"b c (npZ pZ) (npY pY) (npX pX) -> b (c pZ pY pX) npZ npY npX\",\n",
    "            c=C,\n",
    "            npZ=npZ,\n",
    "            npY=npY,\n",
    "            npX=npX,\n",
    "            pZ=pZ,\n",
    "            pY=pY,\n",
    "            pX=pX,\n",
    "        )\n",
    "\n",
    "        return target\n",
    "\n",
    "    def unpatchify(self, logits):\n",
    "        C = self.config.num_channels\n",
    "        B, _, npZ, npY, npX = logits.shape\n",
    "        pZ, pY, pX = self.config.patch_size\n",
    "\n",
    "        target = rearrange(\n",
    "            logits,\n",
    "            \"b (c pZ pY pX) npZ npY npX -> b c (npZ pZ) (npY pY) (npX pX)\",\n",
    "            c=C,\n",
    "            npZ=npZ,\n",
    "            npY=npY,\n",
    "            npX=npX,\n",
    "            pZ=pZ,\n",
    "            pY=pY,\n",
    "            pX=pX,\n",
    "        )\n",
    "\n",
    "        return target\n",
    "\n",
    "    def forward_loss(self, pixel_values, pred, masks_inverse):\n",
    "        target = self.patchify(pixel_values)\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * masks_inverse).sum() / masks_inverse.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        head_mask: torch.FloatTensor = None,\n",
    "        output_attentions: bool = None,\n",
    "        output_hidden_states: bool = None,\n",
    "        return_dict: bool = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.vitdet3d(\n",
    "            pixel_values,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        latent = outputs.last_hidden_state\n",
    "        masks = outputs.masks\n",
    "\n",
    "        decoder_outputs = self.decoder(latent, masks)\n",
    "        logits = decoder_outputs.logits\n",
    "\n",
    "        loss = self.forward_loss(pixel_values, logits, 1 - masks)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits, masks) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return ViTDet3DMAEForPreTrainingOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            masks=masks,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.4933\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mDivBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViTDet3DMAEForPreTraining(sample_config)\n",
    "output = model(sample_scan)\n",
    "\n",
    "output.loss, output.logits.shape, output.masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m128\u001b[0m, \u001b[1;36m128\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.unpatchify(output.logits).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m3242752\u001b[0m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = torch.rand(5, 1, 32, 128, 128)\n",
    "sample_config = ViTDet3DMAEConfig(\n",
    "    decoder_hidden_size=96,\n",
    "    decoder_intermediate_size=384,\n",
    "    decoder_num_attention_heads=2,\n",
    "    decoder_num_hidden_layers=2,\n",
    "    hidden_size=192,\n",
    "    intermediate_size=768,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    #\n",
    "    image_size=(32, 128, 128),\n",
    "    patch_size=(4, 32, 32),\n",
    "    num_channels=1,\n",
    "    mask_ratio=0.875,\n",
    ")\n",
    "\n",
    "model = ViTDet3DMAEForPreTraining(sample_config)\n",
    "\n",
    "sum(x.numel() for x in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e68f818a484440494714eeff1d5ea21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.369966\tLR: 1.000000\n",
      "Loss: 0.348081\tLR: 1.000000\n",
      "Loss: 0.328018\tLR: 1.000000\n",
      "Loss: 0.309469\tLR: 1.000000\n",
      "Loss: 0.292224\tLR: 1.000000\n",
      "Loss: 0.276010\tLR: 0.900000\n",
      "Loss: 0.262676\tLR: 0.900000\n",
      "Loss: 0.250013\tLR: 0.900000\n",
      "Loss: 0.237907\tLR: 0.900000\n",
      "Loss: 0.226672\tLR: 0.900000\n",
      "Loss: 0.216101\tLR: 0.810000\n",
      "Loss: 0.207103\tLR: 0.810000\n",
      "Loss: 0.198482\tLR: 0.810000\n",
      "Loss: 0.190675\tLR: 0.810000\n",
      "Loss: 0.183007\tLR: 0.810000\n",
      "Loss: 0.176032\tLR: 0.729000\n",
      "Loss: 0.170005\tLR: 0.729000\n",
      "Loss: 0.164356\tLR: 0.729000\n",
      "Loss: 0.159000\tLR: 0.729000\n",
      "Loss: 0.153973\tLR: 0.729000\n",
      "Loss: 0.149173\tLR: 0.656100\n",
      "Loss: 0.145210\tLR: 0.656100\n",
      "Loss: 0.141314\tLR: 0.656100\n",
      "Loss: 0.137747\tLR: 0.656100\n",
      "Loss: 0.134289\tLR: 0.656100\n",
      "Loss: 0.131098\tLR: 0.590490\n",
      "Loss: 0.128373\tLR: 0.590490\n",
      "Loss: 0.125870\tLR: 0.590490\n",
      "Loss: 0.123408\tLR: 0.590490\n",
      "Loss: 0.121060\tLR: 0.590490\n",
      "Loss: 0.118872\tLR: 0.531441\n",
      "Loss: 0.117024\tLR: 0.531441\n",
      "Loss: 0.115240\tLR: 0.531441\n",
      "Loss: 0.113636\tLR: 0.531441\n",
      "Loss: 0.112015\tLR: 0.531441\n",
      "Loss: 0.110470\tLR: 0.478297\n",
      "Loss: 0.109165\tLR: 0.478297\n",
      "Loss: 0.107961\tLR: 0.478297\n",
      "Loss: 0.106796\tLR: 0.478297\n",
      "Loss: 0.105607\tLR: 0.478297\n",
      "Loss: 0.104589\tLR: 0.430467\n",
      "Loss: 0.103688\tLR: 0.430467\n",
      "Loss: 0.102812\tLR: 0.430467\n",
      "Loss: 0.101976\tLR: 0.430467\n",
      "Loss: 0.101119\tLR: 0.430467\n",
      "Loss: 0.100376\tLR: 0.387420\n",
      "Loss: 0.099719\tLR: 0.387420\n",
      "Loss: 0.099025\tLR: 0.387420\n",
      "Loss: 0.098474\tLR: 0.387420\n",
      "Loss: 0.097880\tLR: 0.387420\n",
      "Loss: 0.097313\tLR: 0.348678\n",
      "Loss: 0.096858\tLR: 0.348678\n",
      "Loss: 0.096349\tLR: 0.348678\n",
      "Loss: 0.095950\tLR: 0.348678\n",
      "Loss: 0.095517\tLR: 0.348678\n",
      "Loss: 0.095143\tLR: 0.313811\n",
      "Loss: 0.094760\tLR: 0.313811\n",
      "Loss: 0.094445\tLR: 0.313811\n",
      "Loss: 0.094060\tLR: 0.313811\n",
      "Loss: 0.093681\tLR: 0.313811\n",
      "Loss: 0.093461\tLR: 0.282430\n",
      "Loss: 0.093144\tLR: 0.282430\n",
      "Loss: 0.092866\tLR: 0.282430\n",
      "Loss: 0.092639\tLR: 0.282430\n",
      "Loss: 0.092420\tLR: 0.282430\n",
      "Loss: 0.092134\tLR: 0.254187\n",
      "Loss: 0.091917\tLR: 0.254187\n",
      "Loss: 0.091672\tLR: 0.254187\n",
      "Loss: 0.091513\tLR: 0.254187\n",
      "Loss: 0.091335\tLR: 0.254187\n",
      "Loss: 0.091187\tLR: 0.228768\n",
      "Loss: 0.090950\tLR: 0.228768\n",
      "Loss: 0.090825\tLR: 0.228768\n",
      "Loss: 0.090665\tLR: 0.228768\n",
      "Loss: 0.090518\tLR: 0.228768\n",
      "Loss: 0.090342\tLR: 0.205891\n",
      "Loss: 0.090228\tLR: 0.205891\n",
      "Loss: 0.090089\tLR: 0.205891\n",
      "Loss: 0.089974\tLR: 0.205891\n",
      "Loss: 0.089868\tLR: 0.205891\n",
      "Loss: 0.089754\tLR: 0.185302\n",
      "Loss: 0.089640\tLR: 0.185302\n",
      "Loss: 0.089555\tLR: 0.185302\n",
      "Loss: 0.089403\tLR: 0.185302\n",
      "Loss: 0.089321\tLR: 0.185302\n",
      "Loss: 0.089248\tLR: 0.166772\n",
      "Loss: 0.089159\tLR: 0.166772\n",
      "Loss: 0.089042\tLR: 0.166772\n",
      "Loss: 0.088976\tLR: 0.166772\n",
      "Loss: 0.088861\tLR: 0.166772\n",
      "Loss: 0.088824\tLR: 0.150095\n",
      "Loss: 0.088770\tLR: 0.150095\n",
      "Loss: 0.088660\tLR: 0.150095\n",
      "Loss: 0.088613\tLR: 0.150095\n",
      "Loss: 0.088550\tLR: 0.150095\n",
      "Loss: 0.088514\tLR: 0.135085\n",
      "Loss: 0.088455\tLR: 0.135085\n",
      "Loss: 0.088344\tLR: 0.135085\n",
      "Loss: 0.088327\tLR: 0.135085\n",
      "Loss: 0.088251\tLR: 0.135085\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(100)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_batch)\n",
    "    print(f\"Loss: {output.loss:f}\\tLR: {scheduler.get_last_lr()[0]:f}\")\n",
    "    output.loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
