{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp vitdet3dmae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE using VitDet3D architecture for encoding\n",
    "#### (and simple ViT for decoding)\n",
    "\n",
    "- [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527)\n",
    "- [Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality](https://arxiv.org/abs/2205.10063)\n",
    "- [Swin MAE: Masked Autoencoders for Small Datasets](https://arxiv.org/abs/2212.13805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of code:\n",
    "(Recommended extension: https://marketplace.visualstudio.com/items?itemName=bierner.markdown-mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph ViTDet3DMAEModel\n",
    "        subgraph ViTDet3DMAEEmbeddings\n",
    "            subgraph ViTDet3DMAEPatchEmbeddings\n",
    "                patch_embeddings[Patchify scan]\n",
    "            end\n",
    "            positional_embeddings[Add Positional Embeddings]\n",
    "            uniform_sampling[Uniform Sampling]\n",
    "            masking[Mask]\n",
    "        end\n",
    "        ViTDet3DMAEEncoder\n",
    "    end\n",
    "\n",
    "    Start -- 3D input scan --> patch_embeddings\n",
    "    patch_embeddings --> positional_embeddings\n",
    "    positional_embeddings --> uniform_sampling\n",
    "    positional_embeddings -- Embeddings --> ViTDet3DMAEEncoder\n",
    "    uniform_sampling -- 25% mask / 12.5% mask --> masking\n",
    "    masking --> ViTDet3DMAEEncoder\n",
    "\n",
    "    ViTDet3DMAEEncoder --> End\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange\n",
    "from medct.vitdet3d import VitDetConfig, VitDet3dEncoder, VitDet3dPreTrainedModel\n",
    "from torch import nn\n",
    "from transformers.models.vit_mae.modeling_vit_mae import ViTMAELayer\n",
    "from transformers.utils import ModelOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "ViTMAEConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"attention_probs_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"decoder_hidden_size\"\u001b[0m: \u001b[1;36m512\u001b[0m,\n",
       "  \u001b[32m\"decoder_intermediate_size\"\u001b[0m: \u001b[1;36m2048\u001b[0m,\n",
       "  \u001b[32m\"decoder_num_attention_heads\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "  \u001b[32m\"decoder_num_hidden_layers\"\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "  \u001b[32m\"hidden_act\"\u001b[0m: \u001b[32m\"gelu\"\u001b[0m,\n",
       "  \u001b[32m\"hidden_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"hidden_size\"\u001b[0m: \u001b[1;36m768\u001b[0m,\n",
       "  \u001b[32m\"image_size\"\u001b[0m: \u001b[1;36m224\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"intermediate_size\"\u001b[0m: \u001b[1;36m3072\u001b[0m,\n",
       "  \u001b[32m\"layer_norm_eps\"\u001b[0m: \u001b[1;36m1e-12\u001b[0m,\n",
       "  \u001b[32m\"mask_ratio\"\u001b[0m: \u001b[1;36m0.75\u001b[0m,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"vit_mae\"\u001b[0m,\n",
       "  \u001b[32m\"norm_pix_loss\"\u001b[0m: false,\n",
       "  \u001b[32m\"num_attention_heads\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"num_channels\"\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "  \u001b[32m\"num_hidden_layers\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"patch_size\"\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "  \u001b[32m\"qkv_bias\"\u001b[0m: true,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.38.1\"\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.vit_mae import ViTMAEConfig\n",
    "\n",
    "ViTMAEConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# Config inherits hyperparameters from both ViTDet and ViTMAE\n",
    "class ViTDet3DMAEConfig(VitDetConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "        decoder_hidden_size=384,\n",
    "        decoder_intermediate_size=1536,\n",
    "        decoder_learnable_position_embeddings=False,\n",
    "        decoder_num_attention_heads=12,\n",
    "        decoder_num_hidden_layers=1,\n",
    "        hidden_dropout_prob=0.0,\n",
    "        hidden_size=768,\n",
    "        image_size=(64, 512, 512),\n",
    "        intermediate_size=3072,\n",
    "        learnable_position_embeddings=False,\n",
    "        mask_ratio=0.875,\n",
    "        norm_pix_loss=False,\n",
    "        num_attention_heads=12,\n",
    "        num_hidden_layers=12,\n",
    "        patch_size=(2, 32, 32),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.model_type = \"vitdet3dmae\"\n",
    "\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.decoder_intermediate_size = decoder_intermediate_size\n",
    "        self.decoder_learnable_position_embeddings = decoder_learnable_position_embeddings\n",
    "        self.decoder_num_attention_heads = decoder_num_attention_heads\n",
    "        self.decoder_num_hidden_layers = decoder_num_hidden_layers\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.hidden_size = hidden_size\n",
    "        self.image_size = image_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.learnable_position_embeddings = learnable_position_embeddings\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.grid_size = (\n",
    "            self.image_size[0] // self.patch_size[0],\n",
    "            self.image_size[1] // self.patch_size[1],\n",
    "            self.image_size[2] // self.patch_size[2],\n",
    "        )\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "ViTDet3DMAEConfig \u001b[1m{\u001b[0m\n",
       "  \u001b[32m\"attention_probs_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"decoder_hidden_size\"\u001b[0m: \u001b[1;36m384\u001b[0m,\n",
       "  \u001b[32m\"decoder_intermediate_size\"\u001b[0m: \u001b[1;36m1536\u001b[0m,\n",
       "  \u001b[32m\"decoder_learnable_position_embeddings\"\u001b[0m: false,\n",
       "  \u001b[32m\"decoder_num_attention_heads\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"decoder_num_hidden_layers\"\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "  \u001b[32m\"drop_path_rate\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"grid_size\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m32\u001b[0m,\n",
       "    \u001b[1;36m16\u001b[0m,\n",
       "    \u001b[1;36m16\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"hidden_act\"\u001b[0m: \u001b[32m\"gelu\"\u001b[0m,\n",
       "  \u001b[32m\"hidden_dropout_prob\"\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "  \u001b[32m\"hidden_size\"\u001b[0m: \u001b[1;36m768\u001b[0m,\n",
       "  \u001b[32m\"image_size\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m64\u001b[0m,\n",
       "    \u001b[1;36m512\u001b[0m,\n",
       "    \u001b[1;36m512\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"initializer_range\"\u001b[0m: \u001b[1;36m0.02\u001b[0m,\n",
       "  \u001b[32m\"intermediate_size\"\u001b[0m: \u001b[1;36m3072\u001b[0m,\n",
       "  \u001b[32m\"layer_norm_eps\"\u001b[0m: \u001b[1;36m1e-06\u001b[0m,\n",
       "  \u001b[32m\"learnable_position_embeddings\"\u001b[0m: false,\n",
       "  \u001b[32m\"mask_ratio\"\u001b[0m: \u001b[1;36m0.875\u001b[0m,\n",
       "  \u001b[32m\"mlp_ratio\"\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "  \u001b[32m\"model_type\"\u001b[0m: \u001b[32m\"vitdet\"\u001b[0m,\n",
       "  \u001b[32m\"norm_pix_loss\"\u001b[0m: false,\n",
       "  \u001b[32m\"num_attention_heads\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"num_channels\"\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "  \u001b[32m\"num_hidden_layers\"\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "  \u001b[32m\"num_patches\"\u001b[0m: \u001b[1;36m8192\u001b[0m,\n",
       "  \u001b[32m\"out_features\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"stage12\"\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"out_indices\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m12\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"patch_size\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[1;36m2\u001b[0m,\n",
       "    \u001b[1;36m32\u001b[0m,\n",
       "    \u001b[1;36m32\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"pretrain_image_size\"\u001b[0m: \u001b[1;36m224\u001b[0m,\n",
       "  \u001b[32m\"qkv_bias\"\u001b[0m: true,\n",
       "  \u001b[32m\"residual_block_indices\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"stage_names\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"stem\"\u001b[0m,\n",
       "    \u001b[32m\"stage1\"\u001b[0m,\n",
       "    \u001b[32m\"stage2\"\u001b[0m,\n",
       "    \u001b[32m\"stage3\"\u001b[0m,\n",
       "    \u001b[32m\"stage4\"\u001b[0m,\n",
       "    \u001b[32m\"stage5\"\u001b[0m,\n",
       "    \u001b[32m\"stage6\"\u001b[0m,\n",
       "    \u001b[32m\"stage7\"\u001b[0m,\n",
       "    \u001b[32m\"stage8\"\u001b[0m,\n",
       "    \u001b[32m\"stage9\"\u001b[0m,\n",
       "    \u001b[32m\"stage10\"\u001b[0m,\n",
       "    \u001b[32m\"stage11\"\u001b[0m,\n",
       "    \u001b[32m\"stage12\"\u001b[0m\n",
       "  \u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"transformers_version\"\u001b[0m: \u001b[32m\"4.38.1\"\u001b[0m,\n",
       "  \u001b[32m\"use_absolute_position_embeddings\"\u001b[0m: true,\n",
       "  \u001b[32m\"use_relative_position_embeddings\"\u001b[0m: false,\n",
       "  \u001b[32m\"window_block_indices\"\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "  \u001b[32m\"window_size\"\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTDet3DMAEConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_config = ViTDet3DMAEConfig(\n",
    "    image_size=(32, 128, 128),\n",
    "    patch_size=(2, 16, 32),  # num patches = 16*8*4 = 512\n",
    "    num_channels=1,\n",
    "    mask_ratio=0.75,  # or set it to 0.875\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_3d_position_embeddings(embedding_size, grid_size):\n",
    "    if embedding_size % 6 != 0:\n",
    "        raise ValueError(\"embed_dim must be divisible by 6\")\n",
    "\n",
    "    d, h, w = grid_size\n",
    "\n",
    "    grid_d = np.arange(d, dtype=np.float32)\n",
    "    grid_h = np.arange(h, dtype=np.float32)\n",
    "    grid_w = np.arange(w, dtype=np.float32)\n",
    "\n",
    "    grid = np.meshgrid(grid_w, grid_h, grid_d)\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([3, 1, d, h, w])\n",
    "\n",
    "    omega = np.arange(embedding_size // 6, dtype=float)\n",
    "    omega /= embedding_size / 6.0\n",
    "    omega = 1.0 / 10000**omega\n",
    "\n",
    "    position_embeddings = []\n",
    "    for grid_subset in grid:\n",
    "        grid_subset = grid_subset.reshape(-1)\n",
    "        out = np.einsum(\"m,d->md\", grid_subset, omega)\n",
    "\n",
    "        emb_sin = np.sin(out)\n",
    "        emb_cos = np.cos(out)\n",
    "\n",
    "        emb = np.concatenate([emb_sin, emb_cos], axis=1)\n",
    "        position_embeddings.append(emb)\n",
    "\n",
    "    position_embeddings = np.concatenate(position_embeddings, axis=1)\n",
    "\n",
    "    return position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEPatchEmbeddings(nn.Module):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        image_size, patch_size = config.image_size, config.patch_size\n",
    "        num_channels, hidden_size = config.num_channels, config.hidden_size\n",
    "\n",
    "        if not isinstance(image_size, collections.abc.Iterable) or len(image_size) != 3:\n",
    "            raise ValueError(\"image_size must be given as 3D iterable\")\n",
    "        if not isinstance(patch_size, collections.abc.Iterable) or len(patch_size) != 3:\n",
    "            raise ValueError(\"patch_size must be given as 3D iterable\")\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.projection = nn.Conv3d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor):\n",
    "        num_channels = pixel_values.shape[1]\n",
    "        if num_channels != self.num_channels:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n",
    "                f\" Expected {self.num_channels} but got {num_channels}.\"\n",
    "            )\n",
    "\n",
    "        if self.image_size != pixel_values.shape[2:]:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the spatial dimensions of the pixel values match with the ones set in the configuration.\"\n",
    "                f\" Expected {self.image_size} but got {pixel_values.shape[2:]}.\"\n",
    "            )\n",
    "\n",
    "        # (b, c, z, y, x)\n",
    "        embeddings = self.projection(pixel_values)\n",
    "        # (b, hidden_size, z, y, x)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scan = torch.rand(1, 1, 32, 128, 128)\n",
    "\n",
    "output = ViTDet3DMAEPatchEmbeddings(sample_config)(sample_scan)\n",
    "\n",
    "assert np.prod(output.shape[2:]) == 512\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overall embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEEmbeddings(nn.Module):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.patch_embeddings = ViTDet3DMAEPatchEmbeddings(config)\n",
    "\n",
    "        # Positional embeddings\n",
    "        num_positions = self.config.num_patches  # Not adding class token as the patches can't be flattened in ViTDet3D\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.zeros(1, config.hidden_size, num_positions), requires_grad=config.learnable_position_embeddings\n",
    "        )\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        if not self.config.learnable_position_embeddings:\n",
    "            # initialize (and freeze) position embeddings by sin-cos embedding\n",
    "            position_embeddings = get_3d_position_embeddings(self.config.hidden_size, self.config.grid_size)\n",
    "            self.position_embeddings.data.copy_(torch.from_numpy(position_embeddings.T).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embeddings like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embeddings.projection.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "    def uniform_sampling(self, embeddings: torch.Tensor):\n",
    "        # embeddings: (b, hidden_size, z, y, x)\n",
    "        B, _, Z, Y, X = embeddings.shape\n",
    "        assert Z % 2 == 0 and Y % 2 == 0 and X % 2 == 0, \"Z, Y, X must be even\"\n",
    "\n",
    "        # Only works if mask_ratio is 0, 0.75, or 0.875\n",
    "        assert self.config.mask_ratio in {0, 0.75, 0.875}, \"Mask ratio must be 0.75 or 0.875 (or 0)\"\n",
    "\n",
    "        if self.config.mask_ratio == 0:\n",
    "            masks = torch.ones(B, Z, Y, X, device=embeddings.device, dtype=torch.int8)\n",
    "        else:\n",
    "            masks = []\n",
    "            for _ in range(len(embeddings)):\n",
    "                # Perform uniform sampling. Select one patch in a 2x2x2 cube.\n",
    "                # Also add the diagonally opposite patch if mask_ratio == 0.75.\n",
    "                num_decisions = X * Y * Z // 8\n",
    "                if self.config.mask_ratio == 0.75:\n",
    "                    mask1 = np.eye(4, dtype=\"int\").repeat(num_decisions, axis=0)  # (num_decisions * 4, 4)\n",
    "                    np.random.shuffle(mask1)\n",
    "                    mask1 = mask1[:num_decisions]  # (num_decisions, 4)\n",
    "                    mask2 = mask1.copy()\n",
    "\n",
    "                    mask2[:, (0, 3)] = mask2[:, (3, 0)]\n",
    "                    mask2[:, (1, 2)] = mask2[:, (2, 1)]\n",
    "                    mask = np.concatenate([mask1, mask2], axis=1)  # (num_decisions, 8)\n",
    "                else:\n",
    "                    mask = np.eye(8, dtype=\"int\").repeat(num_decisions, axis=0)  # (num_decisions * 8, 8)\n",
    "                    np.random.shuffle(mask)\n",
    "                    mask = mask[:num_decisions]  # (num_decisions, 8)\n",
    "\n",
    "                mask = rearrange(\n",
    "                    mask, \"(d h w) (p1 p2 p3) -> (d p1) (h p2) (w p3)\", d=Z // 2, h=Y // 2, w=X // 2, p1=2, p2=2, p3=2\n",
    "                )\n",
    "                masks.append(mask)\n",
    "            masks = np.stack(masks, axis=0)\n",
    "\n",
    "            masks = torch.tensor(masks, device=embeddings.device, dtype=torch.int8)\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor):\n",
    "        # Get patch embeddings\n",
    "        embeddings = self.patch_embeddings(pixel_values)\n",
    "        # (b, hidden_size, z, y, x)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        B, C, Z, Y, X = embeddings.shape\n",
    "        embeddings = embeddings + self.position_embeddings.reshape(1, C, Z, Y, X)\n",
    "\n",
    "        # Uniform sampling\n",
    "        masks = self.uniform_sampling(embeddings)\n",
    "\n",
    "        # Get masked embeddings\n",
    "        new_Z, new_Y, new_X = Z, Y, X\n",
    "        if self.config.mask_ratio == 0.875:\n",
    "            new_Z, new_Y, new_X = Z // 2, Y // 2, X // 2\n",
    "        elif self.config.mask_ratio == 0.75:\n",
    "            new_Z, new_Y, new_X = Z, Y // 2, X // 2\n",
    "        embeddings = torch.masked_select(embeddings, masks.unsqueeze(1).bool()).reshape(B, C, new_Z, new_Y, new_X)\n",
    "\n",
    "        return embeddings, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scan = torch.rand(2, 1, 32, 128, 128)\n",
    "\n",
    "embeddings, masks = ViTDet3DMAEEmbeddings(sample_config)(sample_scan)\n",
    "\n",
    "embeddings.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int8\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\n",
       "        \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "         \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int8\u001b[1m)\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(masks[0, :2, :4, :4])\n",
    "display(masks[1, :2, :4, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/arjun.agarwal/miniconda3/lib/python3.9/site-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from qer.ai.core.preprocessing.load_scan import load_scan\n",
    "from neuro_utils.visualize import plot_scans\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan = load_scan(r\"/home/users/arjun.agarwal/projects/temp/1.2.840.113619.2.81.290.1.3689.20170516.222854\")\n",
    "scan = sitk.GetArrayFromImage(scan)\n",
    "scan = np.clip(scan, 0, 80)\n",
    "scan = np.pad(scan, ((12, 12), (0, 0), (0, 0)))\n",
    "scan = torch.from_numpy(scan).float()\n",
    "scan = scan.unsqueeze(0).unsqueeze(0)\n",
    "scan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, masks = ViTDet3DMAEEmbeddings(ViTDet3DMAEConfig(num_channels=1))(scan)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_masks = masks.repeat_interleave(2, dim=1).repeat_interleave(32, dim=2).repeat_interleave(32, dim=3)\n",
    "expanded_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m512\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b684a72b074ec2955b577789bc051f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='z', max=63), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scan = scan.squeeze(0).squeeze(0)\n",
    "masked_scan = scan * expanded_masks.squeeze(0)\n",
    "plot_scans([scan, masked_scan])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEEncoder(VitDet3dEncoder):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ViTDet3DMAEModelOutput(ModelOutput):\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    masks: torch.LongTensor = None\n",
    "    hidden_states: tuple[torch.FloatTensor] = None\n",
    "    attentions: tuple[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEPreTrainedModel(VitDet3dPreTrainedModel):\n",
    "    config_class = ViTDet3DMAEConfig\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        super()._init_weights(module)\n",
    "\n",
    "        if isinstance(module, ViTDet3DMAEEmbeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config.initializer_range,\n",
    "            ).to(module.position_embeddings.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEModel(ViTDet3DMAEPreTrainedModel):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = ViTDet3DMAEEmbeddings(config)\n",
    "        self.encoder = ViTDet3DMAEEncoder(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        head_mask: torch.FloatTensor = None,\n",
    "        output_attentions: bool = None,\n",
    "        output_hidden_states: bool = None,\n",
    "        return_dict: bool = None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output, masks = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output,) + encoder_outputs[1:]\n",
    "\n",
    "        return ViTDet3DMAEModelOutput(\n",
    "            last_hidden_state=sequence_output,\n",
    "            masks=masks,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m768\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_scan = torch.rand(2, 1, 32, 128, 128)\n",
    "\n",
    "model = ViTDet3DMAEModel(sample_config)\n",
    "output = model(sample_scan)\n",
    "\n",
    "output.last_hidden_state.shape, output.masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m86234880\u001b[0m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x.numel() for x in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ViTDet3DMAEDecoderOutput(ModelOutput):\n",
    "    logits: torch.FloatTensor\n",
    "    hidden_states: tuple[torch.FloatTensor] = None\n",
    "    attentions: tuple[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEDecoder(nn.Module):\n",
    "    def __init__(self, config: ViTDet3DMAEConfig, grid_size: tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.decoder_embed = nn.Linear(config.hidden_size, config.decoder_hidden_size, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, config.decoder_hidden_size, 1, 1, 1))\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.zeros(1, config.decoder_hidden_size, grid_size[0] * grid_size[1] * grid_size[2]),\n",
    "            requires_grad=config.decoder_learnable_position_embeddings,\n",
    "        )\n",
    "\n",
    "        decoder_config = deepcopy(config)\n",
    "        decoder_config.hidden_size = config.decoder_hidden_size\n",
    "        decoder_config.num_hidden_layers = config.decoder_num_hidden_layers\n",
    "        decoder_config.num_attention_heads = config.decoder_num_attention_heads\n",
    "        decoder_config.intermediate_size = config.decoder_intermediate_size\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [ViTMAELayer(decoder_config) for _ in range(config.decoder_num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "        self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size, eps=config.layer_norm_eps)\n",
    "        self.decoder_pred = nn.Linear(\n",
    "            config.decoder_hidden_size, config.num_channels * np.prod(config.patch_size), bias=True\n",
    "        )  # encoder to decoder\n",
    "        self.gradient_checkpointing = False\n",
    "        self.config = config\n",
    "        self.initialize_weights(grid_size)\n",
    "\n",
    "    def initialize_weights(self, grid_size):\n",
    "        if not self.config.decoder_learnable_position_embeddings:\n",
    "            # initialize (and freeze) position embeddings by sin-cos embedding\n",
    "            position_embeddings = get_3d_position_embeddings(self.config.decoder_hidden_size, grid_size)\n",
    "            self.position_embeddings.data.copy_(torch.from_numpy(position_embeddings.T).float().unsqueeze(0))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.mask_token, std=self.config.initializer_range)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        masks,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        # npZ etc. is the number of patches after uniform sampling\n",
    "        # npD etc. is the number of patches in the original image\n",
    "        # pZ etc. is the patch size\n",
    "        # C is the number of channels in th eoriginal image\n",
    "        # H can be any hidden size\n",
    "\n",
    "        # embed tokens\n",
    "        # (b, encoder_hidden_size, z, y, x)\n",
    "        hidden_states = rearrange(hidden_states, \"b h npZ npY npX -> b npZ npY npX h\")\n",
    "        x = self.decoder_embed(hidden_states)\n",
    "        x = rearrange(x, \"b npZ npY npX h -> b h npZ npY npX\")\n",
    "        # (b, decoder_hidden_size, z, y, x)\n",
    "\n",
    "        # Create mask tokens array\n",
    "        B, _, npZ, npY, npX = x.shape\n",
    "        npD, npH, npW = npZ, npY, npX\n",
    "        if self.config.mask_ratio == 0.875:\n",
    "            npD, npH, npW = npZ * 2, npY * 2, npX * 2\n",
    "        elif self.config.mask_ratio == 0.75:\n",
    "            npD, npH, npW = npZ, npY * 2, npX * 2\n",
    "        mask_tokens = self.mask_token.repeat(B, 1, npD, npH, npW)\n",
    "\n",
    "        # Add hidden states to this\n",
    "        x = mask_tokens.masked_scatter(masks.unsqueeze(1).bool(), x)\n",
    "\n",
    "        # add pos embed\n",
    "        hidden_states = x + self.position_embeddings.reshape(1, -1, npD, npH, npW)\n",
    "\n",
    "        # Flatten hidden states and reorder them\n",
    "        hidden_states = rearrange(hidden_states, \"b h npD npH npW -> b (npD npH npW) h\")\n",
    "\n",
    "        # apply Transformer layers (blocks)\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        for i, layer_module in enumerate(self.decoder_layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    hidden_states,\n",
    "                    None,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(hidden_states, head_mask=None, output_attentions=output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        hidden_states = self.decoder_norm(hidden_states)\n",
    "\n",
    "        # predictor projection\n",
    "        logits = self.decoder_pred(hidden_states)\n",
    "\n",
    "        # Reshaping back to image patches\n",
    "        pD, pH, pW = self.config.patch_size\n",
    "        logits = rearrange(logits, \"b (npD npH npW) h -> b h npD npH npW\", npD=npD, npH=npH, npW=npW)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [logits, all_hidden_states, all_self_attentions] if v is not None)\n",
    "\n",
    "        return ViTDet3DMAEDecoderOutput(\n",
    "            logits=logits,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = ViTDet3DMAEDecoder(sample_config, (16, 8, 4))\n",
    "decoded = decoder(output.last_hidden_state, output.masks)\n",
    "\n",
    "decoded.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViTDet3DMAEForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class ViTDet3DMAEForPreTrainingOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    masks: torch.LongTensor = None\n",
    "    hidden_states: tuple[torch.FloatTensor] = None\n",
    "    attentions: tuple[torch.FloatTensor] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ViTDet3DMAEForPreTraining(ViTDet3DMAEPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.vitdet3d = ViTDet3DMAEModel(config)\n",
    "        self.decoder = ViTDet3DMAEDecoder(config, grid_size=config.grid_size)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def patchify(self, pixel_values):\n",
    "        B, C, _, _, _ = pixel_values.shape\n",
    "        npZ, npY, npX = self.config.grid_size\n",
    "        pZ, pY, pX = self.config.patch_size\n",
    "\n",
    "        target = rearrange(\n",
    "            pixel_values,\n",
    "            \"b c (npZ pZ) (npY pY) (npX pX) -> b (c pZ pY pX) npZ npY npX\",\n",
    "            c=C,\n",
    "            npZ=npZ,\n",
    "            npY=npY,\n",
    "            npX=npX,\n",
    "            pZ=pZ,\n",
    "            pY=pY,\n",
    "            pX=pX,\n",
    "        )\n",
    "\n",
    "        return target\n",
    "\n",
    "    def unpatchify(self, logits):\n",
    "        C = self.config.num_channels\n",
    "        B, _, npZ, npY, npX = logits.shape\n",
    "        pZ, pY, pX = self.config.patch_size\n",
    "\n",
    "        target = rearrange(\n",
    "            logits,\n",
    "            \"b (c pZ pY pX) npZ npY npX -> b c (npZ pZ) (npY pY) (npX pX)\",\n",
    "            c=C,\n",
    "            npZ=npZ,\n",
    "            npY=npY,\n",
    "            npX=npX,\n",
    "            pZ=pZ,\n",
    "            pY=pY,\n",
    "            pX=pX,\n",
    "        )\n",
    "\n",
    "        return target\n",
    "\n",
    "    def forward_loss(self, pixel_values, pred, masks_inverse):\n",
    "        target = self.patchify(pixel_values)\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=1)  # [N, L], mean loss per patch\n",
    "\n",
    "        # def create_point_symmetric_tensor_3d(shape, min_value, max_value, power=2):\n",
    "        #     \"\"\"\n",
    "        #     Creates a 3D torch tensor with point symmetry around the center, with a customizable decrease rate.\n",
    "\n",
    "        #     Args:\n",
    "        #     shape: A tuple representing the desired shape of the tensor (depth, rows, columns).\n",
    "        #     min_value: The minimum value for the tensor.\n",
    "        #     max_value: The maximum value for the tensor.\n",
    "        #     power: The exponent controlling the decrease rate (higher power leads to faster decrease).\n",
    "\n",
    "        #     Returns:\n",
    "        #     A 3D torch tensor with point symmetry and controlled decrease rate.\n",
    "        #     \"\"\"\n",
    "        #     depth, rows, cols = shape\n",
    "        #     center_depth, center_row, center_col = depth // 2, rows // 2, cols // 2\n",
    "\n",
    "        #     # Create distance grid (Corrected)\n",
    "        #     x_coords = torch.arange(cols).reshape(1, 1, -1).repeat(depth, rows, 1) - center_col\n",
    "        #     y_coords = torch.arange(rows).reshape(1, -1, 1).repeat(depth, 1, cols) - center_row\n",
    "        #     z_coords = torch.arange(depth).reshape(-1, 1, 1).repeat(1, rows, cols) - center_depth\n",
    "\n",
    "        #     distance_grid = torch.sqrt(x_coords**2 + y_coords**2 + z_coords**2)\n",
    "\n",
    "        #     # Apply power for slower/faster decrease (based on power value)\n",
    "        #     distance_grid = distance_grid**power\n",
    "\n",
    "        #     # Normalize distance grid to range between 0 and 1\n",
    "        #     distance_grid = (distance_grid - distance_grid.min()) / (distance_grid.max() - distance_grid.min())\n",
    "\n",
    "        #     # Invert distance to get decreasing values from center\n",
    "        #     inverted_distance = 1 - distance_grid\n",
    "\n",
    "        #     # Scale inverted distance to desired range\n",
    "        #     output = inverted_distance * (max_value - min_value) + min_value\n",
    "\n",
    "        #     return output\n",
    "\n",
    "        # loss *= create_point_symmetric_tensor_3d(loss.shape[1:], 0, 1, 1.5).to(loss.device)\n",
    "\n",
    "        loss = (loss * masks_inverse).sum() / masks_inverse.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        head_mask: torch.FloatTensor = None,\n",
    "        output_attentions: bool = None,\n",
    "        output_hidden_states: bool = None,\n",
    "        return_dict: bool = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.vitdet3d(\n",
    "            pixel_values,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        latent = outputs.last_hidden_state\n",
    "        masks = outputs.masks\n",
    "\n",
    "        decoder_outputs = self.decoder(latent, masks)\n",
    "        logits = decoder_outputs.logits\n",
    "\n",
    "        loss = self.forward_loss(pixel_values, logits, 1 - masks)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits, masks) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return ViTDet3DMAEForPreTrainingOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            masks=masks,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.4847\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mDivBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1024\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViTDet3DMAEForPreTraining(sample_config)\n",
    "output = model(sample_scan)\n",
    "\n",
    "output.loss, output.logits.shape, output.masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m128\u001b[0m, \u001b[1;36m128\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.unpatchify(output.logits).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m3242752\u001b[0m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = torch.rand(5, 1, 32, 128, 128)\n",
    "sample_config = ViTDet3DMAEConfig(\n",
    "    decoder_hidden_size=96,\n",
    "    decoder_intermediate_size=384,\n",
    "    decoder_num_attention_heads=2,\n",
    "    decoder_num_hidden_layers=2,\n",
    "    hidden_size=192,\n",
    "    intermediate_size=768,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    #\n",
    "    image_size=(32, 128, 128),\n",
    "    patch_size=(4, 32, 32),\n",
    "    num_channels=1,\n",
    "    mask_ratio=0.875,\n",
    ")\n",
    "\n",
    "model = ViTDet3DMAEForPreTraining(sample_config)\n",
    "\n",
    "sum(x.numel() for x in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f02c978a994521ac083208c7458d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.369298\tLR: 1.000000\n",
      "Loss: 0.347652\tLR: 1.000000\n",
      "Loss: 0.327482\tLR: 1.000000\n",
      "Loss: 0.309012\tLR: 1.000000\n",
      "Loss: 0.291880\tLR: 1.000000\n",
      "Loss: 0.275843\tLR: 0.900000\n",
      "Loss: 0.262451\tLR: 0.900000\n",
      "Loss: 0.249772\tLR: 0.900000\n",
      "Loss: 0.237945\tLR: 0.900000\n",
      "Loss: 0.226573\tLR: 0.900000\n",
      "Loss: 0.216069\tLR: 0.810000\n",
      "Loss: 0.207183\tLR: 0.810000\n",
      "Loss: 0.198803\tLR: 0.810000\n",
      "Loss: 0.190865\tLR: 0.810000\n",
      "Loss: 0.183319\tLR: 0.810000\n",
      "Loss: 0.176277\tLR: 0.729000\n",
      "Loss: 0.170235\tLR: 0.729000\n",
      "Loss: 0.164559\tLR: 0.729000\n",
      "Loss: 0.159260\tLR: 0.729000\n",
      "Loss: 0.154290\tLR: 0.729000\n",
      "Loss: 0.149537\tLR: 0.656100\n",
      "Loss: 0.145503\tLR: 0.656100\n",
      "Loss: 0.141626\tLR: 0.656100\n",
      "Loss: 0.138123\tLR: 0.656100\n",
      "Loss: 0.134621\tLR: 0.656100\n",
      "Loss: 0.131599\tLR: 0.590490\n",
      "Loss: 0.128827\tLR: 0.590490\n",
      "Loss: 0.126204\tLR: 0.590490\n",
      "Loss: 0.123743\tLR: 0.590490\n",
      "Loss: 0.121472\tLR: 0.590490\n",
      "Loss: 0.119259\tLR: 0.531441\n",
      "Loss: 0.117287\tLR: 0.531441\n",
      "Loss: 0.115563\tLR: 0.531441\n",
      "Loss: 0.113969\tLR: 0.531441\n",
      "Loss: 0.112280\tLR: 0.531441\n",
      "Loss: 0.110843\tLR: 0.478297\n",
      "Loss: 0.109535\tLR: 0.478297\n",
      "Loss: 0.108194\tLR: 0.478297\n",
      "Loss: 0.107036\tLR: 0.478297\n",
      "Loss: 0.105995\tLR: 0.478297\n",
      "Loss: 0.104851\tLR: 0.430467\n",
      "Loss: 0.104009\tLR: 0.430467\n",
      "Loss: 0.103025\tLR: 0.430467\n",
      "Loss: 0.102200\tLR: 0.430467\n",
      "Loss: 0.101404\tLR: 0.430467\n",
      "Loss: 0.100639\tLR: 0.387420\n",
      "Loss: 0.099986\tLR: 0.387420\n",
      "Loss: 0.099348\tLR: 0.387420\n",
      "Loss: 0.098772\tLR: 0.387420\n",
      "Loss: 0.098137\tLR: 0.387420\n",
      "Loss: 0.097626\tLR: 0.348678\n",
      "Loss: 0.097105\tLR: 0.348678\n",
      "Loss: 0.096610\tLR: 0.348678\n",
      "Loss: 0.096130\tLR: 0.348678\n",
      "Loss: 0.095788\tLR: 0.348678\n",
      "Loss: 0.095337\tLR: 0.313811\n",
      "Loss: 0.094929\tLR: 0.313811\n",
      "Loss: 0.094575\tLR: 0.313811\n",
      "Loss: 0.094260\tLR: 0.313811\n",
      "Loss: 0.093927\tLR: 0.313811\n",
      "Loss: 0.093611\tLR: 0.282430\n",
      "Loss: 0.093330\tLR: 0.282430\n",
      "Loss: 0.093107\tLR: 0.282430\n",
      "Loss: 0.092778\tLR: 0.282430\n",
      "Loss: 0.092555\tLR: 0.282430\n",
      "Loss: 0.092351\tLR: 0.254187\n",
      "Loss: 0.092075\tLR: 0.254187\n",
      "Loss: 0.091899\tLR: 0.254187\n",
      "Loss: 0.091653\tLR: 0.254187\n",
      "Loss: 0.091487\tLR: 0.254187\n",
      "Loss: 0.091303\tLR: 0.228768\n",
      "Loss: 0.091116\tLR: 0.228768\n",
      "Loss: 0.090984\tLR: 0.228768\n",
      "Loss: 0.090764\tLR: 0.228768\n",
      "Loss: 0.090640\tLR: 0.228768\n",
      "Loss: 0.090502\tLR: 0.205891\n",
      "Loss: 0.090338\tLR: 0.205891\n",
      "Loss: 0.090235\tLR: 0.205891\n",
      "Loss: 0.090134\tLR: 0.205891\n",
      "Loss: 0.090010\tLR: 0.205891\n",
      "Loss: 0.089858\tLR: 0.185302\n",
      "Loss: 0.089816\tLR: 0.185302\n",
      "Loss: 0.089633\tLR: 0.185302\n",
      "Loss: 0.089593\tLR: 0.185302\n",
      "Loss: 0.089445\tLR: 0.185302\n",
      "Loss: 0.089382\tLR: 0.166772\n",
      "Loss: 0.089304\tLR: 0.166772\n",
      "Loss: 0.089182\tLR: 0.166772\n",
      "Loss: 0.089091\tLR: 0.166772\n",
      "Loss: 0.089026\tLR: 0.166772\n",
      "Loss: 0.088995\tLR: 0.150095\n",
      "Loss: 0.088874\tLR: 0.150095\n",
      "Loss: 0.088791\tLR: 0.150095\n",
      "Loss: 0.088685\tLR: 0.150095\n",
      "Loss: 0.088660\tLR: 0.150095\n",
      "Loss: 0.088612\tLR: 0.135085\n",
      "Loss: 0.088535\tLR: 0.135085\n",
      "Loss: 0.088457\tLR: 0.135085\n",
      "Loss: 0.088405\tLR: 0.135085\n",
      "Loss: 0.088388\tLR: 0.135085\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(100)):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sample_batch)\n",
    "    print(f\"Loss: {output.loss:f}\\tLR: {scheduler.get_last_lr()[0]:f}\")\n",
    "    output.loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m2590656\u001b[0m"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = torch.rand(5, 1, 32, 128, 128)\n",
    "sample_config = ViTDet3DMAEConfig(\n",
    "    hidden_size=192,\n",
    "    intermediate_size=768,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    #\n",
    "    image_size=(32, 128, 128),\n",
    "    patch_size=(4, 32, 32),\n",
    "    num_channels=1,\n",
    "    mask_ratio=0,\n",
    ")\n",
    "\n",
    "model = ViTDet3DMAEModel(sample_config)\n",
    "model.eval()\n",
    "\n",
    "sum(x.numel() for x in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_batch.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m5\u001b[0m, \u001b[1;36m192\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(sample_batch)\n",
    "output.last_hidden_state.shape, sample_config.grid_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
