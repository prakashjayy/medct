{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp vitdet3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViTDet \n",
    "- Vision Transformer for Detection \n",
    "- plain and non-hierarchical backbone. \n",
    "- Uses FPN \n",
    "\n",
    "\n",
    "### Some changes we made to the original ViTDetCode\n",
    "- Simplified embeddings by removing class token and `get_absolute_positions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from typing import List, Union, Tuple, Optional, Dict\n",
    "\n",
    "from transformers.models.vitdet.configuration_vitdet import VitDetConfig\n",
    "from transformers.models.vitdet.modeling_vitdet import drop_path, VitDetDropPath, caffe2_msra_fill\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import BackboneOutput, BaseModelOutput\n",
    "from transformers.modeling_utils import  PreTrainedModel\n",
    "from transformers.utils.backbone_utils import BackboneMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VitDetConfig {\n",
       "  \"drop_path_rate\": 0.0,\n",
       "  \"dropout_prob\": 0.0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_eps\": 1e-06,\n",
       "  \"mlp_ratio\": 4,\n",
       "  \"model_type\": \"vitdet\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"out_features\": [\n",
       "    \"stage12\"\n",
       "  ],\n",
       "  \"out_indices\": [\n",
       "    12\n",
       "  ],\n",
       "  \"patch_size\": 16,\n",
       "  \"pretrain_image_size\": 224,\n",
       "  \"qkv_bias\": true,\n",
       "  \"residual_block_indices\": [],\n",
       "  \"stage_names\": [\n",
       "    \"stem\",\n",
       "    \"stage1\",\n",
       "    \"stage2\",\n",
       "    \"stage3\",\n",
       "    \"stage4\",\n",
       "    \"stage5\",\n",
       "    \"stage6\",\n",
       "    \"stage7\",\n",
       "    \"stage8\",\n",
       "    \"stage9\",\n",
       "    \"stage10\",\n",
       "    \"stage11\",\n",
       "    \"stage12\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.35.0\",\n",
       "  \"use_absolute_position_embeddings\": true,\n",
       "  \"use_relative_position_embeddings\": false,\n",
       "  \"window_block_indices\": [],\n",
       "  \"window_size\": 0\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VitDetConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VitDetEmebddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13825"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = (96, 192, 192)\n",
    "patch_size = (4, 8, 8)\n",
    "num_channels = 1\n",
    "hidden_size = 96 \n",
    "num_patches = (image_size[0]//patch_size[0]) * (image_size[1]//patch_size[1]) * (image_size[2]//patch_size[2])\n",
    "num_positions = num_patches+1\n",
    "num_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv3d(1, 96, kernel_size=(4, 8, 8), stride=(4, 8, 8))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection = nn.Conv3d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 24, 24, 24])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.randn((1, 1, )+image_size)\n",
    "out = projection(img)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 24, 24, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = out.permute((0, 2, 3, 4, 1))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = np.ceil(math.pow(num_patches, 1/3))\n",
    "size*size*size == num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.vitdet.modeling_vitdet.VitDetEmbeddings \n",
    "class ViTDet3dEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "    \n",
    "        image_size, patch_size = config.image_size, config.patch_size\n",
    "        num_channels, hidden_size = config.num_channels, config.hidden_size\n",
    "\n",
    "        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size, image_size)\n",
    "        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size, patch_size)\n",
    "        num_patches = (image_size[2] // patch_size[2]) * (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        if config.use_absolute_position_embeddings:\n",
    "            # Initialize absolute positional embedding with pretrain image size.\n",
    "            num_positions = num_patches + 1 # we are ignoring cls_token for now \n",
    "            self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, config.hidden_size))\n",
    "        else:\n",
    "            self.position_embeddings = None\n",
    "\n",
    "        self.projection = nn.Conv3d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        num_channels = pixel_values.shape[1]\n",
    "        if num_channels != self.num_channels:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n",
    "                f\" Expected {self.num_channels} but got {num_channels}.\"\n",
    "            )\n",
    "        embeddings = self.projection(pixel_values)\n",
    "\n",
    "        if self.position_embeddings is not None:\n",
    "            # (batch_size, num_channels, depth, height, width) -> (batch_size, depth, height, width, num_channels)\n",
    "            embeddings = embeddings.permute(0, 2, 3, 4, 1)\n",
    "            # add position embeddings\n",
    "            _, D, H, W , C = embeddings.shape\n",
    "            embeddings = embeddings + self.position_embeddings.reshape((1, D, H, W, C))\n",
    "            # (batch_size, depth, height, width, num_channels) -> (batch_size, num_channels, depth, height, width)\n",
    "            embeddings = embeddings.permute(0, 4, 1, 2, 3)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTDet3dEmbeddings(\n",
       "  (projection): Conv3d(1, 96, kernel_size=(4, 8, 8), stride=(4, 8, 8))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = VitDetConfig(image_size=(96, 192, 192), patch_size=(4, 8, 8), hidden_size=96, num_channels=1)\n",
    "embed = ViTDet3dEmbeddings(config)\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 192, 192)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 96, 192, 192])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 24, 24, 24])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.randn((1, 1, )+image_size)\n",
    "print(img.shape)\n",
    "embed_out = embed(img)\n",
    "embed_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "- we have [qkv matrices]x n heads matrics \n",
    "- input is multiplied with qkv and we get qkv matrices \n",
    "- we split them and get qkv matrices \n",
    "- attention scores are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 24, 24, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = embed_out.permute(0, 2, 3, 4, 1)\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets do windowing first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from medct.swin.window_partition3d\n",
    "def window_partition3d(input_feature, window_size: Union[int, List[int]]):\n",
    "    \"\"\"\n",
    "    Partitions the given input into windows.\n",
    "    \"\"\"\n",
    "    window_depth, window_height, window_width = window_size if isinstance(window_size, collections.abc.Iterable) else (window_size, window_size, window_size)\n",
    "    batch_size, depth, height, width, num_channels = input_feature.shape\n",
    "    input_feature = input_feature.view(\n",
    "        batch_size, depth // window_depth, window_depth, height // window_height, window_height, width // window_width, window_width, num_channels\n",
    "    )\n",
    "    windows = input_feature.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_depth, window_height, window_width, num_channels)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([216, 4, 4, 4, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = (4, 4, 4)\n",
    "windowed_hidden_states = window_partition3d(hidden_states, window_size)\n",
    "windowed_hidden_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Apply attention now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 12 96 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=96, out_features=288, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = config.hidden_size\n",
    "num_heads = config.num_attention_heads\n",
    "head_dim = dim // num_heads\n",
    "scale = head_dim**-0.5\n",
    "print(dim, num_heads, dim, head_dim)\n",
    "qkv = nn.Linear(dim, dim * 3, bias=config.qkv_bias)\n",
    "qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 4, 4, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, depth, height, width, _ = windowed_hidden_states.shape\n",
    "batch_size, depth, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 216, 12, 64, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_mat = qkv(hidden_states).reshape(batch_size, depth * height * width, 3, num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "qkv_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2592, 64, 8]),\n",
       " torch.Size([2592, 64, 8]),\n",
       " torch.Size([2592, 64, 8]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries, keys, values = qkv_mat.reshape(3, batch_size * num_heads, depth* height * width, -1).unbind(0)\n",
    "queries.shape, keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2592, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = (queries * scale) @ keys.transpose(-2, -1)\n",
    "attention_scores.shape #> this is a huge matrix and this is why we need window attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have to add positional embeddings to the attention matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (4, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 8]), torch.Size([7, 8]), torch.Size([7, 8]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_d = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
    "rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
    "rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[2] - 1, head_dim))\n",
    "rel_pos_d.shape, rel_pos_h.shape, rel_pos_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4, 4), (4, 4, 4), torch.Size([2592, 64, 8]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_size= k_size = (depth, height, width)\n",
    "q_size, k_size, queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qh = q_size[1]\n",
    "kh = k_size[1]\n",
    "q_coords = torch.arange(qh)[:, None] * max(kh / qh, 1.0)\n",
    "k_coords = torch.arange(kh)[None, :] * max(qh / kh, 1.0)\n",
    "relative_coords = (q_coords - k_coords) + (kh - 1) * max(qh / kh, 1.0)\n",
    "print(relative_coords.shape)\n",
    "relative_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 2., 1., 0.],\n",
       "        [4., 3., 2., 1.],\n",
       "        [5., 4., 3., 2.],\n",
       "        [6., 5., 4., 3.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from transformers.modelling.vitdet.get_rel_pos\n",
    "def get_rel_pos(q_size, k_size, rel_pos):\n",
    "    \"\"\"\n",
    "    Get relative positional embeddings according to the relative positions of query and key sizes.\n",
    "\n",
    "    Args:\n",
    "        q_size (`int`):\n",
    "            Size of query q.\n",
    "        k_size (`int`):\n",
    "            Size of key k.\n",
    "        rel_pos (`torch.Tensor`):\n",
    "            Relative position embeddings (num_embeddings, num_channels).\n",
    "\n",
    "    Returns:\n",
    "        Extracted positional embeddings according to relative positions.\n",
    "    \"\"\"\n",
    "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
    "    # Interpolate rel pos if needed.\n",
    "    if rel_pos.shape[0] != max_rel_dist:\n",
    "        # Interpolate rel position embeddings.\n",
    "        rel_pos_resized = torch.nn.functional.interpolate(\n",
    "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n",
    "            size=max_rel_dist,\n",
    "            mode=\"linear\",\n",
    "        )\n",
    "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
    "    else:\n",
    "        rel_pos_resized = rel_pos\n",
    "\n",
    "    # Scale the coords with short length if shapes for q and k are different.\n",
    "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
    "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
    "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
    "\n",
    "    return rel_pos_resized[relative_coords.long()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 8]), torch.Size([4, 4, 8]), torch.Size([4, 4, 8]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_depth, queries_height, queries_width = q_size\n",
    "keys_depth, keys_height, keys_width = k_size\n",
    "relative_depth = get_rel_pos(queries_depth, keys_depth, rel_pos_d)\n",
    "relative_height = get_rel_pos(queries_height, keys_height, rel_pos_h)\n",
    "relative_width = get_rel_pos(queries_width, keys_width, rel_pos_w)\n",
    "relative_width.shape, relative_height.shape, relative_depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2592, 4, 4, 4, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, _, dim = queries.shape\n",
    "r_q = queries.reshape(batch_size, queries_depth, queries_height, queries_width, dim)\n",
    "r_q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(20).view(5, 4)\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.arange(20).view(4, 5)*1\n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 70,  76,  82,  88,  94],\n",
       "        [190, 212, 234, 256, 278],\n",
       "        [310, 348, 386, 424, 462],\n",
       "        [430, 484, 538, 592, 646],\n",
       "        [550, 620, 690, 760, 830]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"ij, jk->ik\", x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 410, 1170, 1930, 2690, 3450])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"ij, jk->i\", x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 5]),\n",
       " tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(100).view(5, 4, 5)\n",
    "x.shape, x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5, 5]),\n",
       " tensor([[1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.arange(125).view(5, 5, 5)*0 + 1\n",
    "y.shape, y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10, 10, 10, 10],\n",
       "        [35, 35, 35, 35, 35],\n",
       "        [60, 60, 60, 60, 60],\n",
       "        [85, 85, 85, 85, 85]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"hwc,hkc->hwk\", x, y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2592, 4, 4, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_height = torch.einsum(\"bdhwc,hkc->bdhwk\", r_q, relative_height)\n",
    "relative_height.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2592, 4, 4, 4, 4]), torch.Size([2592, 4, 4, 4, 4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_depth = torch.einsum(\"bdhwc,dkc->bdhwk\", r_q, relative_depth)\n",
    "relative_width = torch.einsum(\"bdhwc,wkc->bdhwk\", r_q, relative_width)\n",
    "relative_width.shape, relative_depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2592, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = attention_scores.view(batch_size, queries_depth, queries_height, queries_width, keys_depth, keys_height, keys_width)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2592, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords = (relative_depth[:, :, :, :, :, None, None] + relative_height[:, :, :, :, None, :, None] + relative_width[:, :, :, :, None, None, :])\n",
    "coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2592, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = (attn+coords).view(batch_size,  queries_depth * queries_height * queries_width, keys_depth *keys_height * keys_width)\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "#| copied from transformers.models.vitdet.add_decomposed_relative_positions \n",
    "def add_decomposed_relative_positions(attn, queries, rel_pos_d, rel_pos_h, rel_pos_w, q_size, k_size):\n",
    "    \"\"\"\n",
    "    Calculate decomposed Relative Positional Embeddings as introduced in\n",
    "    [MViT2](https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py).\n",
    "\n",
    "    Args:\n",
    "        attn (`torch.Tensor`):\n",
    "            Attention map.\n",
    "        queries (`torch.Tensor`):\n",
    "            Query q in the attention layer with shape (batch_size,queries_depth* queries_height * queries_width, num_channels).\n",
    "        rel_pos_d (`torch.Tensor`):\n",
    "            Relative position embeddings (Ld, num_channels) for depth axis.\n",
    "        rel_pos_h (`torch.Tensor`):\n",
    "            Relative position embeddings (Lh, num_channels) for height axis.\n",
    "        rel_pos_w (`torch.Tensor`):\n",
    "            Relative position embeddings (Lw, num_channels) for width axis.\n",
    "        q_size (`Tuple[int]`):\n",
    "            Spatial sequence size of query q with (queries_depth, queries_height, queries_width).\n",
    "        k_size (`Tuple[int]`]):\n",
    "            Spatial sequence size of key k with (keys_depth, keys_height, keys_width).\n",
    "\n",
    "    Returns:\n",
    "        attn (Tensor): attention map with added relative positional embeddings.\n",
    "    \"\"\"\n",
    "    queries_depth, queries_height, queries_width = q_size\n",
    "    keys_depth, keys_height, keys_width = k_size\n",
    "    relative_depth = get_rel_pos(queries_depth, keys_depth, rel_pos_d)\n",
    "    relative_height = get_rel_pos(queries_height, keys_height, rel_pos_h)\n",
    "    relative_width = get_rel_pos(queries_width, keys_width, rel_pos_w)\n",
    "\n",
    "    batch_size, _, dim = queries.shape\n",
    "    r_q = queries.reshape(batch_size, queries_depth, queries_height, queries_width, dim)\n",
    "    relative_depth = torch.einsum(\"bdhwc,dkc->bdhwk\", r_q, relative_depth)\n",
    "    relative_height = torch.einsum(\"bdhwc,hkc->bdhwk\", r_q, relative_height)\n",
    "    relative_width = torch.einsum(\"bdhwc,wkc->bdhwk\", r_q, relative_width)\n",
    "\n",
    "    attn = (\n",
    "        attn.view(batch_size, queries_depth, queries_height, queries_width, keys_depth, keys_height, keys_width)\n",
    "        + relative_depth[:, :, :, :, :, None, None]\n",
    "        + relative_height[:, :, :, :, None, :, None]\n",
    "        + relative_width[:, :, :, :, None, None, :]\n",
    "    ).view(batch_size, queries_depth* queries_height * queries_width, keys_depth * keys_height * keys_width)\n",
    "    return attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vit3dAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from transformers.vitdet.vitdetattention\n",
    "class VitDet3dAttention(nn.Module):\n",
    "    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config, input_size=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (`VitDetConfig`):\n",
    "                Model configuration.\n",
    "            input_size (`Tuple[int]`, *optional*):\n",
    "                Input resolution, only required in case relative position embeddings are added.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=config.qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.use_relative_position_embeddings = config.use_relative_position_embeddings\n",
    "        if self.use_relative_position_embeddings:\n",
    "            # initialize relative positional embeddings\n",
    "            self.rel_pos_d = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[2] - 1, head_dim))\n",
    "\n",
    "    def forward(self, hidden_state, output_attentions=False):\n",
    "        batch_size, depth, height, width, _ = hidden_state.shape\n",
    "        # qkv with shape (3, batch_size, num_heads, height * width, num_channels)\n",
    "        qkv = self.qkv(hidden_state).reshape(batch_size, depth * height * width, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        # queries, keys and values have shape (batch_size * num_heads, height * width, num_channels)\n",
    "        queries, keys, values = qkv.reshape(3, batch_size * self.num_heads, depth* height * width, -1).unbind(0)\n",
    "\n",
    "        attention_scores = (queries * self.scale) @ keys.transpose(-2, -1)\n",
    "\n",
    "        if self.use_relative_position_embeddings:\n",
    "            attention_scores = add_decomposed_relative_positions(\n",
    "                attention_scores, queries, self.rel_pos_d, self.rel_pos_h, self.rel_pos_w, (depth, height, width), (depth, height, width)\n",
    "            )\n",
    "\n",
    "        attention_probs = attention_scores.softmax(dim=-1)\n",
    "\n",
    "        hidden_state = attention_probs @ values\n",
    "        hidden_state = hidden_state.view(batch_size, self.num_heads, depth, height, width, -1)\n",
    "        hidden_state = hidden_state.permute(0, 2, 3, 4, 1, 5)\n",
    "        hidden_state = hidden_state.reshape(batch_size, depth, height, width, -1)\n",
    "        hidden_state = self.proj(hidden_state)\n",
    "\n",
    "        if output_attentions:\n",
    "            attention_probs = attention_probs.reshape(\n",
    "                batch_size, self.num_heads, attention_probs.shape[-2], attention_probs.shape[-1]\n",
    "            )\n",
    "            outputs = (hidden_state, attention_probs)\n",
    "        else:\n",
    "            outputs = (hidden_state,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VitDet3dAttention(\n",
       "  (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "  (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = VitDetConfig(image_size=(96, 192, 192), patch_size=(4, 8, 8), hidden_size=96, num_channels=1, use_relative_position_embeddings=True)\n",
    "vdattn = VitDet3dAttention(config, input_size=window_size)\n",
    "vdattn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([216, 4, 4, 4, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowed_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([216, 4, 4, 4, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = vdattn(hidden_state=windowed_hidden_states)\n",
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([216, 4, 4, 4, 96]), torch.Size([216, 12, 64, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = vdattn(hidden_state=windowed_hidden_states, output_attentions=True)\n",
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VitDetLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from transformers.vitdet.vitdetlayernorm\n",
    "class VitDet3dLayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    A LayerNorm variant, popularized by Transformers, that performs point-wise mean and variance normalization over the\n",
    "    channel dimension for inputs that have shape (batch_size, channels, height, width).\n",
    "    https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None, None] * x + self.bias[:, None, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VitDetResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from transformers.vitdet.vitdetresbottleneckblock\n",
    "class VitDet3dResBottleneckBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The standard bottleneck residual block without the last activation layer. It contains 3 conv layers with kernels\n",
    "    1x1, 3x3, 1x1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, in_channels, out_channels, bottleneck_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (`VitDetConfig`):\n",
    "                Model configuration.\n",
    "            in_channels (`int`):\n",
    "                Number of input channels.\n",
    "            out_channels (`int`):\n",
    "                Number of output channels.\n",
    "            bottleneck_channels (`int`):\n",
    "                Number of output channels for the 3x3 \"bottleneck\" conv layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, bottleneck_channels, 1, bias=False)\n",
    "        self.norm1 = VitDet3dLayerNorm(bottleneck_channels)\n",
    "        self.act1 = ACT2FN[config.hidden_act]\n",
    "\n",
    "        self.conv2 = nn.Conv3d(bottleneck_channels, bottleneck_channels, 3, padding=1, bias=False)\n",
    "        self.norm2 = VitDet3dLayerNorm(bottleneck_channels)\n",
    "        self.act2 = ACT2FN[config.hidden_act]\n",
    "\n",
    "        self.conv3 = nn.Conv3d(bottleneck_channels, out_channels, 1, bias=False)\n",
    "        self.norm3 = VitDet3dLayerNorm(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.children():\n",
    "            out = layer(out)\n",
    "\n",
    "        out = x + out\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VitDetMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from transformers.vitdet.vitdetmlp\n",
    "class VitDet3dMlp(nn.Module):\n",
    "    def __init__(self, config, in_features: int, hidden_features: int) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = ACT2FN[config.hidden_act]\n",
    "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
    "        self.drop = nn.Dropout(config.dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Unpartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from medct.swin.window_reverse3d\n",
    "def window_reverse3d(windows, window_size, depth, height, width):\n",
    "    \"\"\"\n",
    "    Merges windows to produce higher resolution features.\n",
    "    \"\"\"\n",
    "    num_channels = windows.shape[-1]\n",
    "    window_depth, window_height, window_width = window_size if isinstance(window_size, collections.abc.Iterable) else (window_size, window_size, window_size)\n",
    "    windows = windows.view(-1, depth // window_depth, height // window_height, width // window_width, window_depth, window_height, window_width, num_channels)\n",
    "    windows = windows.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, depth, height, width, num_channels)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VitDetLayer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from transformers.vitdet.vitdetlayer\n",
    "class VitDet3dLayer(nn.Module):\n",
    "    \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, config, drop_path_rate: float = 0, window_size: Optional[Union[int, Tuple[int]]]= None, use_residual_block: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        dim = config.hidden_size\n",
    "        input_size = (config.image_size[0] // config.patch_size[0], config.image_size[1] // config.patch_size[1], \\\n",
    "                      config.image_size[2] // config.patch_size[2])\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n",
    "        \n",
    "        if window_size is None: window_size = (0, 0, 0)\n",
    "        if isinstance(window_size, int): window_size = (window_size, window_size, window_size)\n",
    "        \n",
    "        self.attention = VitDet3dAttention(\n",
    "            config, input_size=input_size if window_size[0] == 0 else window_size\n",
    "        )\n",
    "\n",
    "        self.drop_path = VitDetDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = VitDet3dMlp(config=config, in_features=dim, hidden_features=int(dim * config.mlp_ratio))\n",
    "\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.use_residual_block = use_residual_block\n",
    "        if self.use_residual_block:\n",
    "            # Use a residual block with bottleneck channel as dim // 2\n",
    "            self.residual = VitDet3dResBottleneckBlock(\n",
    "                config=config,\n",
    "                in_channels=dim,\n",
    "                out_channels=dim,\n",
    "                bottleneck_channels=dim // 2,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        #TODO: this \n",
    "        hidden_states = hidden_states.permute(0, 2, 3, 4, 1)\n",
    "\n",
    "        shortcut = hidden_states\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "\n",
    "        # Window partition\n",
    "        if self.window_size[0] > 0:\n",
    "            depth, height, width = hidden_states.shape[1], hidden_states.shape[2], hidden_states.shape[3]\n",
    "            hidden_states = window_partition3d(hidden_states, self.window_size)\n",
    "\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        # Reverse window partition\n",
    "        if self.window_size[0] > 0:\n",
    "            hidden_states = window_reverse3d(hidden_states, self.window_size, depth, height, width)\n",
    "\n",
    "        # first residual connection\n",
    "        hidden_states = shortcut + self.drop_path(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states + self.drop_path(self.mlp(self.norm2(hidden_states)))\n",
    "\n",
    "        hidden_states = hidden_states.permute(0, 4, 1, 2, 3)\n",
    "\n",
    "        if self.use_residual_block:\n",
    "            hidden_states = self.residual(hidden_states)\n",
    "\n",
    "        outputs = (hidden_states,) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VitDet3dLayer(\n",
       "  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "  (attention): VitDet3dAttention(\n",
       "    (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "    (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "  )\n",
       "  (drop_path): Identity()\n",
       "  (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): VitDet3dMlp(\n",
       "    (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdl = VitDet3dLayer(config, window_size=(4, 4, 4), use_residual_block=False)\n",
    "vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([1, 96, 24, 24, 24]), torch.Size([216, 12, 64, 64])], 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = vdl(embed_out, output_attentions=True)\n",
    "[i.shape for i in out], len(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VitDet3dEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.window_block_indices, config.residual_block_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from transformers.vitdet.vitdetencoder\n",
    "class VitDet3dEncoder(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        depth = config.num_hidden_layers\n",
    "\n",
    "        # stochastic depth decay rule\n",
    "        drop_path_rate = [x.item() for x in torch.linspace(0, config.drop_path_rate, depth)]\n",
    "\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            layers.append(\n",
    "                VitDet3dLayer(\n",
    "                    config,\n",
    "                    drop_path_rate=drop_path_rate[i],\n",
    "                    window_size=config.window_size if i in config.window_block_indices else None,\n",
    "                    use_residual_block=i in config.residual_block_indices,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.layer = nn.ModuleList(layers)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[tuple, BaseModelOutput]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    hidden_states,\n",
    "                    layer_head_mask,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VitDetConfig {\n",
       "  \"drop_path_rate\": 0.0,\n",
       "  \"dropout_prob\": 0.0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 96,\n",
       "  \"image_size\": [\n",
       "    96,\n",
       "    192,\n",
       "    192\n",
       "  ],\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_eps\": 1e-06,\n",
       "  \"mlp_ratio\": 4,\n",
       "  \"model_type\": \"vitdet\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 1,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"out_features\": [\n",
       "    \"stage12\"\n",
       "  ],\n",
       "  \"out_indices\": [\n",
       "    12\n",
       "  ],\n",
       "  \"patch_size\": [\n",
       "    4,\n",
       "    8,\n",
       "    8\n",
       "  ],\n",
       "  \"pretrain_image_size\": 224,\n",
       "  \"qkv_bias\": true,\n",
       "  \"residual_block_indices\": [],\n",
       "  \"stage_names\": [\n",
       "    \"stem\",\n",
       "    \"stage1\",\n",
       "    \"stage2\",\n",
       "    \"stage3\",\n",
       "    \"stage4\",\n",
       "    \"stage5\",\n",
       "    \"stage6\",\n",
       "    \"stage7\",\n",
       "    \"stage8\",\n",
       "    \"stage9\",\n",
       "    \"stage10\",\n",
       "    \"stage11\",\n",
       "    \"stage12\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.35.0\",\n",
       "  \"use_absolute_position_embeddings\": true,\n",
       "  \"use_relative_position_embeddings\": true,\n",
       "  \"window_block_indices\": [],\n",
       "  \"window_size\": 0\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VitDet3dEncoder(\n",
       "  (layer): ModuleList(\n",
       "    (0-5): 6 x VitDet3dLayer(\n",
       "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): VitDet3dAttention(\n",
       "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "        (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): VitDet3dMlp(\n",
       "        (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = VitDetConfig(image_size=(96, 192, 192), \n",
    "                      patch_size=(4, 8, 8), \n",
    "                      hidden_size=96,\n",
    "                      num_channels=1,\n",
    "                      use_relative_position_embeddings=True, \n",
    "                      window_block_indices=list(range(6)),\n",
    "                      window_size =(4, 4, 4), \n",
    "                      out_indices = [6], \n",
    "                      num_hidden_layers= 6,\n",
    "                      out_features = [\"stage6\"], \n",
    "                      stage_names = [\"stem\"]+[f\"stage{i}\" for i in range(1, 7)])\n",
    "enc = VitDet3dEncoder(config)\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 24, 24, 24])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.layer[0](embed_out)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 4, 4), (4, 4, 4), (4, 4, 4), (4, 4, 4), (4, 4, 4), (4, 4, 4)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.window_size for i in enc.layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = enc(embed_out, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 24, 24, 24])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([216, 12, 64, 64]),\n",
       " torch.Size([216, 12, 64, 64]),\n",
       " torch.Size([216, 12, 64, 64]),\n",
       " torch.Size([216, 12, 64, 64]),\n",
       " torch.Size([216, 12, 64, 64]),\n",
       " torch.Size([216, 12, 64, 64])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in out.attentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 96, 24, 24, 24]),\n",
       " torch.Size([1, 96, 24, 24, 24]),\n",
       " torch.Size([1, 96, 24, 24, 24]),\n",
       " torch.Size([1, 96, 24, 24, 24]),\n",
       " torch.Size([1, 96, 24, 24, 24]),\n",
       " torch.Size([1, 96, 24, 24, 24]),\n",
       " torch.Size([1, 96, 24, 24, 24])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in out.hidden_states]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from transformers.vitdet.vitdetpretrainedmodel\n",
    "class VitDet3dPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = VitDetConfig\n",
    "    base_model_prefix = \"vitdet\"\n",
    "    main_input_name = \"pixel_values\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = []\n",
    "\n",
    "    def _init_weights(self, module: Union[nn.Linear, nn.Conv3d, nn.LayerNorm]) -> None:\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Conv3d)):\n",
    "            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n",
    "            # `trunc_normal_cpu` not implemented in `half` issues\n",
    "            module.weight.data = nn.init.trunc_normal_(\n",
    "                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n",
    "            ).to(module.weight.dtype)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "        elif isinstance(module, ViTDet3dEmbeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config.initializer_range,\n",
    "            ).to(module.position_embeddings.dtype)\n",
    "\n",
    "        elif isinstance(module, VitDet3dAttention) and self.config.use_relative_position_embeddings:\n",
    "            module.rel_pos_h.data = nn.init.trunc_normal_(\n",
    "                module.rel_pos_h.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config.initializer_range,\n",
    "            )\n",
    "            module.rel_pos_w.data = nn.init.trunc_normal_(\n",
    "                module.rel_pos_w.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config.initializer_range,\n",
    "            )\n",
    "\n",
    "        elif isinstance(module, VitDet3dResBottleneckBlock):\n",
    "            for layer in [module.conv1, module.conv2, module.conv3]:\n",
    "                caffe2_msra_fill(layer)\n",
    "            for layer in [module.norm1, module.norm2]:\n",
    "                layer.weight.data.fill_(1.0)\n",
    "                layer.bias.data.zero_()\n",
    "            # zero init last norm layer.\n",
    "            module.norm3.weight.data.zero_()\n",
    "            module.norm3.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VitDet3dModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from transformers.vitdet.vitdetmodel\n",
    "class VitDet3dModel(VitDet3dPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = ViTDet3dEmbeddings(config)\n",
    "        self.encoder = VitDet3dEncoder(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> ViTDet3dEmbeddings:\n",
    "        return self.embeddings.projection\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune: Dict[int, List[int]]) -> None:\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import VitDetConfig, VitDetModel\n",
    "        >>> import torch\n",
    "\n",
    "        >>> config = VitDetConfig()\n",
    "        >>> model = VitDetModel(config)\n",
    "\n",
    "        >>> pixel_values = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "        >>> with torch.no_grad():\n",
    "        ...     outputs = model(pixel_values)\n",
    "\n",
    "        >>> last_hidden_states = outputs.last_hidden_state\n",
    "        >>> list(last_hidden_states.shape)\n",
    "        [1, 768, 14, 14]\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output,) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=sequence_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from transformers.vitdet.vitdetbackbone\n",
    "class VitDet3dBackbone(VitDet3dPreTrainedModel, BackboneMixin):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        super()._init_backbone(config)\n",
    "\n",
    "        self.embeddings = ViTDet3dEmbeddings(config)\n",
    "        self.encoder = VitDet3dEncoder(config)\n",
    "        self.num_features = [config.hidden_size for _ in range(config.num_hidden_layers + 1)]\n",
    "\n",
    "        # initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> ViTDet3dEmbeddings:\n",
    "        return self.embeddings.projection\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.Tensor,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> BackboneOutput:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import VitDetConfig, VitDetBackbone\n",
    "        >>> import torch\n",
    "\n",
    "        >>> config = VitDetConfig()\n",
    "        >>> model = VitDetBackbone(config)\n",
    "\n",
    "        >>> pixel_values = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "        >>> with torch.no_grad():\n",
    "        ...     outputs = model(pixel_values)\n",
    "\n",
    "        >>> feature_maps = outputs.feature_maps\n",
    "        >>> list(feature_maps[-1].shape)\n",
    "        [1, 768, 14, 14]\n",
    "        ```\"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "\n",
    "        embedding_output = self.embeddings(pixel_values)\n",
    "\n",
    "        outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=output_attentions,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.hidden_states if return_dict else outputs[1]\n",
    "\n",
    "        feature_maps = ()\n",
    "        for stage, hidden_state in zip(self.stage_names, hidden_states):\n",
    "            if stage in self.out_features:\n",
    "                feature_maps += (hidden_state,)\n",
    "\n",
    "        if not return_dict:\n",
    "            if output_hidden_states:\n",
    "                output = (feature_maps,) + outputs[1:]\n",
    "            else:\n",
    "                output = (feature_maps,) + outputs[2:]\n",
    "            return output\n",
    "\n",
    "        return BackboneOutput(\n",
    "            feature_maps=feature_maps,\n",
    "            hidden_states=outputs.hidden_states if output_hidden_states else None,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VitDet3dBackbone(\n",
       "  (embeddings): ViTDet3dEmbeddings(\n",
       "    (projection): Conv3d(1, 96, kernel_size=(4, 8, 8), stride=(4, 8, 8))\n",
       "  )\n",
       "  (encoder): VitDet3dEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x VitDet3dLayer(\n",
       "        (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): VitDet3dAttention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): VitDet3dMlp(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = VitDet3dBackbone(config)\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 96, 192, 192])\n"
     ]
    }
   ],
   "source": [
    "img = torch.randn((1, 1, 96, 192, 192))\n",
    "print(img.shape)\n",
    "out = enc(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 96, 24, 24, 24])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in out.feature_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
