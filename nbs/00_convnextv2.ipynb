{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convnextv2\n",
    "\n",
    "> ConvNeXt V2 is a pure convolutional model (ConvNext), inspired by the design of Vision Transformers\n",
    "\n",
    "\n",
    "- In this paper they have made some changes to the ConvNext model to support `MAE`.\n",
    "- Introduced `Global response normalization (GRN)` to enhance inter-channel feature competition. \n",
    "- ConvNext-V2 smallest variant called Atto has 3.7m and gets 76.7% top-1 accuracy on ImageNet, the 650M Huge model achieves 88.9% top-1 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp convnextv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import fastcore.all as fc\n",
    "\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers.models.convnextv2.modeling_convnextv2 import drop_path, ConvNextV2DropPath, ConvNextV2LayerNorm, BaseModelOutputWithNoAttention, BaseModelOutputWithPoolingAndNoAttention\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n",
    "from transformers.modeling_outputs import BackboneOutput\n",
    "from transformers.utils.backbone_utils import BackboneMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE \n",
    "- 60% of the 32x32 patches are masked. \n",
    "- Unlike transformers where we can ignore the masked features, in conv it is difficult to achieve this. \n",
    "    - sparse convolution\n",
    "    - apply binary masking operation before and after the dense conv operation. (theoretically more computationally expensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 64, 64, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((4, 64, 64, 64, 128))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 1, 1, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1, 1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_prob = 0.5\n",
    "random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "random_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 64, 64, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = x*random_tensor\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So we can import `drop_path` and `ConvNextV2DropPath` from transformers itself as these will work for any dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Response Normalization \n",
    "The performance of the convnext is not on par with transformers. After doing a feature space analysis, the potential issue with feature collapse at the MLP layer when training ConvNeXt directly on masked input. This norm techique tries to increase contrast and selectivity of channels.\n",
    "\n",
    "- global feature aggregation\n",
    "- feature normalization\n",
    "- feature calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4).view(2, 2).float()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [3.6056]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(x, p=2, dim=(1, ), keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so for an image of shape (N, H, W, C) we will get (N, 1, 1, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2, 48, 48, 96))\n",
    "gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)\n",
    "gx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1, 96])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx = gx/gx.mean(dim=-1, keepdim=True)\n",
    "nx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class ConvNextV2GRN3d(nn.Module):\n",
    "    \"\"\"GRN (Global Response Normalization) layer\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1, 1, 1, 1, dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(1, 1, 1, 1, dim))\n",
    "\n",
    "    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # Compute and normalize global spatial feature maps - N, H, W, D, C\n",
    "        global_features = torch.norm(hidden_states, p=2, dim=(1, 2, 3), keepdim=True)\n",
    "        norm_features = global_features / (global_features.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        hidden_states = self.weight * (hidden_states * norm_features) + self.bias + hidden_states\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNextV2GRN3d()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grn3d = ConvNextV2GRN3d(dim=128)\n",
    "grn3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(100.0001)\n",
      "tensor(100.0001)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((4, 64, 64, 64, 128))+100\n",
    "print(x.mean())\n",
    "with torch.no_grad():\n",
    "    out = grn3d(x)\n",
    "    print(out.mean()) # same because we have weight sand bias as 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm\n",
    "In ConvNext block we have LayerNorm at >\n",
    "![Layernorm in Convnext block](../assets/convnextv2-v1.png)\n",
    "\n",
    "In the original code we have channel last and channel first too. we will only implement channel last here. We can use the Norm which is present in HugggingFace itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.convnextv2.modeling_convnextv2.ConvNextV2LayerNorm \n",
    "class ConvNextV2LayerNorm3d(nn.Module):\n",
    "    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height,\n",
    "    width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError(f\"Unsupported data format: {self.data_format}\")\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.data_format == \"channels_last\":\n",
    "            x = torch.nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            input_dtype = x.dtype\n",
    "            x = x.float()\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = x.to(dtype=input_dtype)\n",
    "            x = self.weight[:, None, None, None] * x + self.bias[:, None, None, None] #Only this line changed\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNextV2LayerNorm3d()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=ConvNextV2LayerNorm3d(normalized_shape=128, data_format=\"channels_first\")\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((4, 128, 64, 64, 64))\n",
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 64, 64, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=ConvNextV2LayerNorm3d(normalized_shape=128, data_format=\"channels_last\")\n",
    "x = torch.randn((4, 64, 64, 64, 128))\n",
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNextV2Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 24, 48, 48])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = nn.Conv3d(3, 40, kernel_size=4, stride=4)\n",
    "out = layer(torch.randn((2, 3, 96, 192, 192)))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.convnextv2.modeling_convnextv2.ConvNextV2Embeddings with \n",
    "# ConvNextV2Embeddings -> ConvNextV2Embeddings3D\n",
    "class ConvNextV2Embeddings3d(nn.Module):\n",
    "    \"\"\"This class is comparable to (and inspired by) the SwinEmbeddings class\n",
    "    found in src/transformers/models/swin/modeling_swin.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.patch_embeddings = nn.Conv3d(\n",
    "            config.num_channels, config.hidden_sizes[0], kernel_size=config.patch_size, stride=config.patch_size\n",
    "        )\n",
    "        self.layernorm = ConvNextV2LayerNorm3d(config.hidden_sizes[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        self.num_channels = config.num_channels\n",
    "\n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        num_channels = pixel_values.shape[1]\n",
    "        if num_channels != self.num_channels:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n",
    "            )\n",
    "        embeddings = self.patch_embeddings(pixel_values)\n",
    "        embeddings = self.layernorm(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    num_channels = 3\n",
    "    hidden_sizes = [40, 80, 160, 320] #Atto depths=[2, 2, 6, 2], dims=[40, 80, 160, 320]\n",
    "    patch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNextV2Embeddings3d(\n",
       "  (patch_embeddings): Conv3d(3, 40, kernel_size=(4, 4, 4), stride=(4, 4, 4))\n",
       "  (layernorm): ConvNextV2LayerNorm3d()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = ConvNextV2Embeddings3d(config())\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 24, 48, 48])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((2, 3, 96, 192, 192))\n",
    "embed_out = embed(x)\n",
    "embed_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNextV2Layer3D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Copied from transformers.models.convnextv2.modeling_convnextv2.ConvNextV2Layer \n",
    "class ConvNextV2Layer3d(nn.Module):\n",
    "    \"\"\"This corresponds to the `Block` class in the original implementation.\n",
    "\n",
    "    There are two equivalent implementations: [DwConv, LayerNorm (channels_first), Conv, GELU,1x1 Conv]; all in (N, C,\n",
    "    H, W) (2) [DwConv, Permute to (N, H, W, D, C), LayerNorm (channels_last), Linear, GELU, Linear]; Permute back\n",
    "\n",
    "    The authors used (2) as they find it slightly faster in PyTorch.\n",
    "\n",
    "    Args:\n",
    "        config ([`ConvNextV2Config3D`]): Model configuration class.\n",
    "        dim (`int`): Number of input channels.\n",
    "        drop_path (`float`): Stochastic depth rate. Default: 0.0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, dim, drop_path=0):\n",
    "        super().__init__()\n",
    "        # depthwise conv\n",
    "        self.dwconv = nn.Conv3d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "        self.layernorm = ConvNextV2LayerNorm3d(dim, eps=1e-6)\n",
    "        # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
    "        self.act = ACT2FN[config.hidden_act]\n",
    "        self.grn = ConvNextV2GRN3d(4 * dim)\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.drop_path = ConvNextV2DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, hidden_states: torch.FloatTensor) -> torch.Tensor:\n",
    "        input = hidden_states\n",
    "        x = self.dwconv(hidden_states)\n",
    "        # (batch_size, num_channels, height, width, deoth) -> (batch_size, height, width, depth, num_channels)\n",
    "        x = x.permute(0, 2, 3, 4, 1)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.pwconv2(x)\n",
    "        # (batch_size, height, width, depth, num_channels) -> (batch_size, num_channels, height, width, depth)\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    num_channels = 3\n",
    "    hidden_sizes = [40, 80, 160, 320] #Atto depths=[2, 2, 6, 2], dims=[40, 80, 160, 320]\n",
    "    patch_size = 4\n",
    "    hidden_act = \"gelu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNextV2Layer3d(\n",
       "  (dwconv): Conv3d(40, 40, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=40)\n",
       "  (layernorm): ConvNextV2LayerNorm3d()\n",
       "  (pwconv1): Linear(in_features=40, out_features=160, bias=True)\n",
       "  (act): GELUActivation()\n",
       "  (grn): ConvNextV2GRN3d()\n",
       "  (pwconv2): Linear(in_features=160, out_features=40, bias=True)\n",
       "  (drop_path): Identity()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = ConvNextV2Layer3d(config, dim=40)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 24, 48, 48])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_out = layer(embed_out)\n",
    "layer_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNextV2Stage3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Copied from transformers.models.convnextv2.modeling_convnextv2.ConvNextV2Stage \n",
    "class ConvNextV2Stage3d(nn.Module):\n",
    "    \"\"\"ConvNeXTV23D stage, consisting of an optional downsampling layer + multiple residual blocks.\n",
    "\n",
    "    Args:\n",
    "        config ([`ConvNextV2Config3D`]): Model configuration class.\n",
    "        in_channels (`int`): Number of input channels.\n",
    "        out_channels (`int`): Number of output channels.\n",
    "        depth (`int`): Number of residual blocks.\n",
    "        drop_path_rates(`List[float]`): Stochastic depth rates for each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, in_channels, out_channels, kernel_size=2, stride=2, depth=2, drop_path_rates=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if in_channels != out_channels or stride > 1:\n",
    "            self.downsampling_layer = nn.Sequential(\n",
    "                ConvNextV2LayerNorm3d(in_channels, eps=1e-6, data_format=\"channels_first\"),\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
    "            )\n",
    "        else:\n",
    "            self.downsampling_layer = nn.Identity()\n",
    "        drop_path_rates = drop_path_rates or [0.0] * depth\n",
    "        self.layers = nn.Sequential(\n",
    "            *[ConvNextV2Layer3d(config, dim=out_channels, drop_path=drop_path_rates[j]) for j in range(depth)]\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states: torch.FloatTensor) -> torch.Tensor:\n",
    "        hidden_states = self.downsampling_layer(hidden_states)\n",
    "        hidden_states = self.layers(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNextV2Encoder3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Copied from transformers.models.convnextv2.modeling_convnextv2.ConvNextV2Encoder \n",
    "class ConvNextV2Encoder3d(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.stages = nn.ModuleList()\n",
    "        drop_path_rates = [\n",
    "            x.tolist() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths)).split(config.depths)\n",
    "        ]\n",
    "        prev_chs = config.hidden_sizes[0]\n",
    "        for i in range(config.num_stages):\n",
    "            out_chs = config.hidden_sizes[i]\n",
    "            stage = ConvNextV2Stage3d(\n",
    "                config,\n",
    "                in_channels=prev_chs,\n",
    "                out_channels=out_chs,\n",
    "                stride=2 if i > 0 else 1,\n",
    "                depth=config.depths[i],\n",
    "                drop_path_rates=drop_path_rates[i],\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            prev_chs = out_chs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithNoAttention]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.stages):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            hidden_states = layer_module(hidden_states)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states] if v is not None)\n",
    "\n",
    "        return BaseModelOutputWithNoAttention(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=all_hidden_states,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.convnextv2.configuration_convnextv2.ConvNextV2Config\n",
    "class ConvNextV2Config3d(BackboneConfigMixin, PretrainedConfig):\n",
    "    model_type = \"convnextv23d\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels=3,\n",
    "        patch_size=4,\n",
    "        num_stages=2,\n",
    "        hidden_sizes=None,\n",
    "        depths=None,\n",
    "        hidden_act=\"gelu\",\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        drop_path_rate=0.0,\n",
    "        image_size=224,\n",
    "        out_features=None,\n",
    "        out_indices=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.num_stages = num_stages\n",
    "        self.hidden_sizes = [40, 80] if hidden_sizes is None else hidden_sizes\n",
    "        self.depths = [3, 3] if depths is None else depths\n",
    "        self.hidden_act = hidden_act\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.image_size = image_size\n",
    "        self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, len(self.depths) + 1)]\n",
    "        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n",
    "            out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = ConvNextV2Encoder3d(ConvNextV2Config3d())\n",
    "len(encoder.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eout = encoder(embed_out, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 40, 24, 48, 48]),\n",
       " torch.Size([2, 40, 24, 48, 48]),\n",
       " torch.Size([2, 80, 12, 24, 24])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in eout.hidden_states]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pretrained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Copied from transformers.models.convnextv2.modeling_convnextv2.ConvNextV2PreTrainedModel \n",
    "class ConvNextV2PreTrainedModel3d(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = ConvNextV2Config3d\n",
    "    base_model_prefix = \"convnextv2_3d\"\n",
    "    main_input_name = \"pixel_values\"\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Conv3d)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.convnextv2.modeling_convnextv2.ConvNextV2Model \n",
    "class ConvNextV2Model3d(ConvNextV2PreTrainedModel3d):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = ConvNextV2Embeddings3d(config)\n",
    "        self.encoder = ConvNextV2Encoder3d(config)\n",
    "\n",
    "        # final layernorm layer\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_sizes[-1], eps=config.layer_norm_eps)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPoolingAndNoAttention]:\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        embedding_output = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "\n",
    "        # global average pooling, (N, C, H, W, D) -> (N, C)\n",
    "        pooled_output = self.layernorm(last_hidden_state.mean([-3, -2, -1]))\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndNoAttention(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConvNextV2Config3d(image_size=(48, 96, 96))\n",
    "model = ConvNextV2Model3d(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354960"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count = 0\n",
    "for name, params in model.named_parameters():\n",
    "    param_count+= params.shape.numel()\n",
    "param_count #1.5 million params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 s, sys: 898 ms, total: 2.49 s\n",
      "Wall time: 656 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = torch.randn((2, 3, 96, 96, 96))\n",
    "with torch.no_grad():\n",
    "    out = model(x, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 40, 24, 24, 24]),\n",
       " torch.Size([2, 40, 24, 24, 24]),\n",
       " torch.Size([2, 80, 12, 12, 12])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in out.hidden_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80, 12, 12, 12])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the bacbone itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.convnext.modeling_convnext.ConvNextBackbone with CONVNEXT->CONVNEXTV2,ConvNext->ConvNextV2,facebook/convnext-tiny-224->facebook/convnextv2-tiny-1k-224\n",
    "class ConvNextV2Backbone3d(ConvNextV2PreTrainedModel3d, BackboneMixin):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        super()._init_backbone(config)\n",
    "\n",
    "        self.embeddings = ConvNextV2Embeddings3d(config)\n",
    "        self.encoder = ConvNextV2Encoder3d(config)\n",
    "        self.num_features = [config.hidden_sizes[0]] + config.hidden_sizes\n",
    "\n",
    "        # Add layer norms to hidden states of out_features\n",
    "        hidden_states_norms = {}\n",
    "        for stage, num_channels in zip(self._out_features, self.channels):\n",
    "            hidden_states_norms[stage] = ConvNextV2LayerNorm3d(num_channels, data_format=\"channels_first\")\n",
    "        self.hidden_states_norms = nn.ModuleDict(hidden_states_norms)\n",
    "\n",
    "        # initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.Tensor,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> BackboneOutput:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "\n",
    "        embedding_output = self.embeddings(pixel_values)\n",
    "\n",
    "        outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        feature_maps = ()\n",
    "        # we skip the stem\n",
    "        for idx, (stage, hidden_state) in enumerate(zip(self.stage_names[1:], hidden_states[1:])):\n",
    "            if stage in self.out_features:\n",
    "                hidden_state = self.hidden_states_norms[stage](hidden_state)\n",
    "                feature_maps += (hidden_state,)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (feature_maps,)\n",
    "            if output_hidden_states:\n",
    "                output += (outputs.hidden_states,)\n",
    "            return output\n",
    "\n",
    "        return BackboneOutput(\n",
    "            feature_maps=feature_maps,\n",
    "            hidden_states=outputs.hidden_states if output_hidden_states else None,\n",
    "            attentions=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConvNextV2Config3d(image_size=(96, 192, 192), num_channels=1, patch_size=(2, 4, 4), out_features=[\"stage1\", \"stage2\"])\n",
    "model = ConvNextV2Backbone3d(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((2, 1, 96, 192, 192))\n",
    "with torch.no_grad():\n",
    "    out = model(x, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 40, 48, 48, 48]), torch.Size([2, 80, 24, 24, 24])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in out.feature_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
