{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4150e0",
   "metadata": {},
   "source": [
    "> Masked Image Modelling (MIM) on ConvNextV2\n",
    "\n",
    "Im not sure if this works but I am gonna give this a try. we can very well mask and fill embeddings of convnextv2 with mask tokens similar to Swin Transformers. I am not sure why ConvNextV2 implemented sparse conv to achieve the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25613ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp convnextv2mim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.utils import ModelOutput\n",
    "from medct.convnextv2 import ConvNextV2Model3d, ConvNextV2Config3d, ConvNextV2PreTrainedModel3d\n",
    "from medct.swin3dmim import PixelShuffle3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ba034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from medct.swin3dmim.mask_patches\n",
    "def mask_patches(num_patches, mask_ratio=0.5):\n",
    "    len_keep = int(num_patches * (1 - mask_ratio))\n",
    "    x = torch.cat([torch.zeros((len_keep)), torch.ones((num_patches-len_keep))])\n",
    "    x = x[torch.randperm(num_patches)].view(1, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "@dataclass\n",
    "class ConvNextV2MaskedImageModelingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Swin masked image model outputs.\n",
    "\n",
    "    Args:\n",
    "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `bool_masked_pos` is provided):\n",
    "            Masked image modeling (MLM) loss.\n",
    "        reconstruction (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
    "            Reconstructed pixel values.\n",
    "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n",
    "            shape `(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    reconstruction: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConvNextV2Config3d(num_channels=1, image_size=(96, 192, 192), patch_size=(8, 16, 16), hidden_sizes=[40, 80], depths=[2, 2])\n",
    "model = ConvNextV2Model3d(config, use_mask_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513837cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.randn((1, 1, 96, 192, 192)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a389c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 6, 6, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f246ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(config.hidden_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ff4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinForMaskedImageModeling\n",
    "class ConvNextV2ForMaskedImageModeling(ConvNextV2PreTrainedModel3d):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        num_layers = len(config.hidden_sizes)\n",
    "        config.encoder_stride = (config.patch_size[0]*num_layers, \n",
    "                                 config.patch_size[1]*num_layers, \n",
    "                                 config.patch_size[2]*num_layers)\n",
    "        self.num_patches = (config.image_size[0] // config.patch_size[0]) * \\\n",
    "                           (config.image_size[1] // config.patch_size[1]) * \\\n",
    "                           (config.image_size[2] // config.patch_size[2])\n",
    "        if len(config.encoder_stride) !=3: raise NotImplementedError(\"The length of encoder stride should be 3\")\n",
    "        self.model = ConvNextV2Model3d(config, use_mask_token=True)\n",
    "\n",
    "        num_features = config.hidden_sizes[-1]\n",
    "        d_stride, h_stride, w_stride = config.encoder_stride\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(\n",
    "                in_channels=num_features, out_channels=(d_stride*h_stride*w_stride) * config.num_channels, kernel_size=1\n",
    "            ),\n",
    "            PixelShuffle3d(config.encoder_stride),\n",
    "        )\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        bool_masked_pos: Optional[torch.BoolTensor] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, ConvNextV2MaskedImageModelingOutput]:\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.model(\n",
    "            pixel_values,\n",
    "            bool_masked_pos=bool_masked_pos,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "#         # Reshape to (batch_size, num_channels, depth, height, width)\n",
    "#         sequence_output = sequence_output.transpose(1, 2)\n",
    "#         batch_size, num_channels, sequence_length = sequence_output.shape\n",
    "#         depth= height = width = math.ceil(sequence_length**(1/3))\n",
    "#         sequence_output = sequence_output.reshape(batch_size, num_channels, depth, height, width)\n",
    "        \n",
    "\n",
    "        # Reconstruct pixel values\n",
    "        reconstructed_pixel_values = self.decoder(sequence_output)\n",
    "\n",
    "        masked_im_loss = None\n",
    "        if bool_masked_pos is not None:\n",
    "            size = (self.config.image_size[0] // self.config.patch_size[0],\n",
    "                    self.config.image_size[1] // self.config.patch_size[1], \n",
    "                    self.config.image_size[2] // self.config.patch_size[2])\n",
    "                    \n",
    "            bool_masked_pos = bool_masked_pos.reshape(-1, size[0], size[1], size[2])\n",
    "            mask = (bool_masked_pos.repeat_interleave(self.config.patch_size[0], 1)\n",
    "                    .repeat_interleave(self.config.patch_size[1], 2)\n",
    "                    .repeat_interleave(self.config.patch_size[2], 3)\n",
    "                    .unsqueeze(1)\n",
    "                    .contiguous()\n",
    "                )\n",
    "            reconstruction_loss = torch.nn.functional.l1_loss(pixel_values, reconstructed_pixel_values, reduction=\"none\")\n",
    "            masked_im_loss = (reconstruction_loss * mask).sum() / (mask.sum() + 1e-5) / self.config.num_channels\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (reconstructed_pixel_values,) + outputs[2:]\n",
    "            return ((masked_im_loss,) + output) if masked_im_loss is not None else output\n",
    "\n",
    "        return ConvNextV2MaskedImageModelingOutput(\n",
    "            loss=masked_im_loss,\n",
    "            reconstruction=reconstructed_pixel_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900b6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNextV2ForMaskedImageModeling(\n",
       "  (model): ConvNextV2Model3d(\n",
       "    (embeddings): ConvNextV2Embeddings3d(\n",
       "      (patch_embeddings): Conv3d(1, 40, kernel_size=(8, 16, 16), stride=(8, 16, 16))\n",
       "      (layernorm): ConvNextV2LayerNorm3d()\n",
       "    )\n",
       "    (encoder): ConvNextV2Encoder3d(\n",
       "      (stages): ModuleList(\n",
       "        (0): ConvNextV2Stage3d(\n",
       "          (downsampling_layer): Identity()\n",
       "          (layers): Sequential(\n",
       "            (0): ConvNextV2Layer3d(\n",
       "              (dwconv): Conv3d(40, 40, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=40)\n",
       "              (layernorm): ConvNextV2LayerNorm3d()\n",
       "              (pwconv1): Linear(in_features=40, out_features=160, bias=True)\n",
       "              (act): GELUActivation()\n",
       "              (grn): ConvNextV2GRN3d()\n",
       "              (pwconv2): Linear(in_features=160, out_features=40, bias=True)\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (1): ConvNextV2Layer3d(\n",
       "              (dwconv): Conv3d(40, 40, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=40)\n",
       "              (layernorm): ConvNextV2LayerNorm3d()\n",
       "              (pwconv1): Linear(in_features=40, out_features=160, bias=True)\n",
       "              (act): GELUActivation()\n",
       "              (grn): ConvNextV2GRN3d()\n",
       "              (pwconv2): Linear(in_features=160, out_features=40, bias=True)\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvNextV2Stage3d(\n",
       "          (downsampling_layer): Sequential(\n",
       "            (0): ConvNextV2LayerNorm3d()\n",
       "            (1): Conv3d(40, 80, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "          )\n",
       "          (layers): Sequential(\n",
       "            (0): ConvNextV2Layer3d(\n",
       "              (dwconv): Conv3d(80, 80, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=80)\n",
       "              (layernorm): ConvNextV2LayerNorm3d()\n",
       "              (pwconv1): Linear(in_features=80, out_features=320, bias=True)\n",
       "              (act): GELUActivation()\n",
       "              (grn): ConvNextV2GRN3d()\n",
       "              (pwconv2): Linear(in_features=320, out_features=80, bias=True)\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (1): ConvNextV2Layer3d(\n",
       "              (dwconv): Conv3d(80, 80, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=80)\n",
       "              (layernorm): ConvNextV2LayerNorm3d()\n",
       "              (pwconv1): Linear(in_features=80, out_features=320, bias=True)\n",
       "              (act): GELUActivation()\n",
       "              (grn): ConvNextV2GRN3d()\n",
       "              (pwconv2): Linear(in_features=320, out_features=80, bias=True)\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((80,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Conv3d(80, 16384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (1): PixelShuffle3d()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mim = ConvNextV2ForMaskedImageModeling(config)\n",
    "mim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1728, 40]) torch.Size([1, 1728, 1])\n"
     ]
    }
   ],
   "source": [
    "bool_masked_pos = bool_masked_pos = torch.randint(low=0, high=2, size=(1, mim.num_patches)).bool()\n",
    "out = mim(torch.randn((1, 1, 96, 192, 192)), bool_masked_pos=bool_masked_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a3adc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7998, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58e3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96, 192, 192])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.reconstruction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac38b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219fb7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medct",
   "language": "python",
   "name": "medct"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
