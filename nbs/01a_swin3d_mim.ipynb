{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf95d9f6",
   "metadata": {},
   "source": [
    "## We will implement MaskedImageModelling (MIM) here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acee1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp swin3dmim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf43a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import torch\n",
    "import math\n",
    "from typing import Tuple, List, Optional, Union\n",
    "from medct.swin3d import Swin3dModel, Swin3dConfig, Swin3dPreTrainedModel\n",
    "from transformers.models.swin.modeling_swin import SwinMaskedImageModelingOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c124430",
   "metadata": {},
   "source": [
    "How is encoder stride defined?\n",
    "`config.patch_size * 2 (len(config.depths)-1) in each direction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fce18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Swin3dConfig(image_size=(96, 192, 192), depths=[2, 2], num_heads=[3, 6], patch_size=(8, 16, 16), encoder_stride=(16, 32, 32))\n",
    "model = Swin3dModel(config, add_pooling_layer=False, use_mask_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8427302",
   "metadata": {},
   "source": [
    "> Total patches at the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2782f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patches = (model.config.image_size[0] // model.config.patch_size[0]) * \\\n",
    "              (model.config.image_size[1] // model.config.patch_size[1]) * \\\n",
    "              (model.config.image_size[2] // model.config.patch_size[2])\n",
    "num_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eef794",
   "metadata": {},
   "source": [
    "> How masking is done on the transformers repo? But this always give 50 50. so we will later define a custom report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9395da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False,  True]), tensor([830, 898]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()\n",
    "bool_masked_pos.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6374a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96, 192, 192])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values = torch.randn((1, 1, 96, 192, 192))\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78651cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fd1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 216, 192])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b99eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 216, 192])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3118a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 6, 6, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_output = outputs[0].transpose(1, 2)\n",
    "batch_size, num_channels, sequence_length = sequence_output.shape\n",
    "depth= height = width = math.ceil(sequence_length**(1/3))\n",
    "sequence_output = sequence_output.reshape(batch_size, num_channels, depth, height, width)\n",
    "sequence_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d2e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = int(config.embed_dim * 2 ** (config.num_layers - 1))\n",
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994b29e",
   "metadata": {},
   "source": [
    "## Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2481c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv3d(192, 16384, kernel_size=(1, 1, 1), stride=(1, 1, 1))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_stride, h_stride, w_stride = config.encoder_stride\n",
    "decoder = torch.nn.Conv3d(in_channels=num_features, out_channels=d_stride* h_stride*w_stride * config.num_channels, kernel_size=1)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e758353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16384, 6, 6, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = decoder(sequence_output)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eddccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1033,  0.8807, -0.5596,  ..., -0.7480, -0.0764,  0.6527],\n",
       "         [ 1.3849, -1.2244,  0.2439,  ...,  0.1049, -0.5222,  0.5008],\n",
       "         [-0.2783,  0.4175,  0.1892,  ..., -0.6043, -0.2852, -0.2248],\n",
       "         ...,\n",
       "         [-2.0543, -1.5189,  2.1532,  ..., -0.5779, -0.2324,  0.5024],\n",
       "         [ 0.2676, -2.0882, -1.3387,  ...,  0.2376, -0.2337,  0.2955],\n",
       "         [ 0.3487, -0.5860, -0.1518,  ...,  0.9824,  0.1284, -0.4836]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9b3a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from https://github.com/kuoweilai/pixelshuffle3d/blob/9be76091761caf3f3881eb5b3dc4b8da09315ab1/pixelshuffle3d.py#L6C1-L29C79\n",
    "# Modified to support scale when it is different on different axis. \n",
    "class PixelShuffle3d(torch.nn.Module):\n",
    "    '''\n",
    "    This class is a 3d version of pixelshuffle.\n",
    "    '''\n",
    "    def __init__(self, scale):\n",
    "        '''\n",
    "        :param scale: upsample scale\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, channels, in_depth, in_height, in_width = input.size()\n",
    "        nOut = channels // (self.scale[0]*self.scale[1]*self.scale[2])\n",
    "\n",
    "        out_depth = in_depth * self.scale[0]\n",
    "        out_height = in_height * self.scale[1]\n",
    "        out_width = in_width * self.scale[2]\n",
    "\n",
    "        input_view = input.contiguous().view(batch_size, nOut, self.scale[0], self.scale[1], self.scale[2], in_depth, in_height, in_width)\n",
    "\n",
    "        output = input_view.permute(0, 1, 5, 2, 6, 3, 7, 4).contiguous()\n",
    "\n",
    "        return output.view(batch_size, nOut, out_depth, out_height, out_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16993068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PixelShuffle3d()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PixelShuffle3d(config.encoder_stride)\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0b24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96, 192, 192])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_pixel_values = ps(out)\n",
    "reconstructed_pixel_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77dbd51",
   "metadata": {},
   "source": [
    "## Caluculate loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f380062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 12)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = (config.image_size[0] // config.patch_size[0], \\\n",
    "        config.image_size[1] // config.patch_size[1], \\\n",
    "        config.image_size[2] // config.patch_size[2])\n",
    "size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1396ca3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 12, 12])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_masked_pos = bool_masked_pos.reshape(-1, size[0], size[1], size[2])\n",
    "bool_masked_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173aa39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96, 192, 192])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (\n",
    "    bool_masked_pos.repeat_interleave(config.patch_size[0], 1)\n",
    "    .repeat_interleave(config.patch_size[1], 2)\n",
    "    .repeat_interleave(config.patch_size[2], 3)\n",
    "    .unsqueeze(1)\n",
    "    .contiguous()\n",
    ")\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b04f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9222, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss = torch.nn.functional.l1_loss(pixel_values, reconstructed_pixel_values, reduction=\"none\")\n",
    "masked_im_loss = (reconstruction_loss * mask).sum() / (mask.sum() + 1e-5) / config.num_channels\n",
    "masked_im_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6962efd",
   "metadata": {},
   "source": [
    "## Define your own masking stuff\n",
    "At the top, we used random 50-50% masking. But what if we want to mask the image by a random % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f002c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True,  ..., False,  True,  True]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()\n",
    "bool_masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feaec68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_keep = int(num_patches * (1 - 0.6))\n",
    "len_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece9770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat([torch.zeros((len_keep)), torch.ones((num_patches-len_keep))])\n",
    "x = x[torch.randperm(num_patches)].view(1, -1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125829c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1.]), tensor([ 691, 1037]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, count = x.unique(return_counts=True)\n",
    "values, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484119c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6001)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count[1]/(count.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def mask_patches(num_patches, mask_ratio=0.5):\n",
    "    len_keep = int(num_patches * (1 - mask_ratio))\n",
    "    x = torch.cat([torch.zeros((len_keep)), torch.ones((num_patches-len_keep))])\n",
    "    x = x[torch.randperm(num_patches)].view(1, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4a927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8999)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mask_patches(num_patches, mask_ratio=0.1)\n",
    "values, count = x.unique(return_counts=True)\n",
    "count[0]/(count.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c31bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1555,  173]), tensor([[0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, x #mask a particular patch=keep it one here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98aac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinForMaskedImageModeling\n",
    "class Swin3dForMaskedImageModeling(Swin3dPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        if len(config.encoder_stride) !=3: raise NotImplementedError(\"The length of encoder stride should be 3\")\n",
    "        self.swin = Swin3dModel(config, add_pooling_layer=False, use_mask_token=True)\n",
    "\n",
    "        num_features = int(config.embed_dim * 2 ** (config.num_layers - 1))\n",
    "        d_stride, h_stride, w_stride = config.encoder_stride\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(\n",
    "                in_channels=num_features, out_channels=(d_stride*h_stride*w_stride) * config.num_channels, kernel_size=1\n",
    "            ),\n",
    "            PixelShuffle3d(config.encoder_stride),\n",
    "        )\n",
    "        \n",
    "        self.num_patches = (self.config.image_size[0] // self.config.patch_size[0]) * \\\n",
    "                           (self.config.image_size[1] // self.config.patch_size[1]) * \\\n",
    "                           (self.config.image_size[2] // self.config.patch_size[2])\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        bool_masked_pos: Optional[torch.BoolTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SwinMaskedImageModelingOutput]:\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.swin(\n",
    "            pixel_values,\n",
    "            bool_masked_pos=bool_masked_pos,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        # Reshape to (batch_size, num_channels, depth, height, width)\n",
    "        sequence_output = sequence_output.transpose(1, 2)\n",
    "        batch_size, num_channels, sequence_length = sequence_output.shape\n",
    "        depth= height = width = math.ceil(sequence_length**(1/3))\n",
    "        sequence_output = sequence_output.reshape(batch_size, num_channels, depth, height, width)\n",
    "        \n",
    "\n",
    "        # Reconstruct pixel values\n",
    "        reconstructed_pixel_values = self.decoder(sequence_output)\n",
    "\n",
    "        masked_im_loss = None\n",
    "        if bool_masked_pos is not None:\n",
    "            size = (self.config.image_size[0] // self.config.patch_size[0],\n",
    "                    self.config.image_size[1] // self.config.patch_size[1], \n",
    "                    self.config.image_size[2] // self.config.patch_size[2])\n",
    "                    \n",
    "            bool_masked_pos = bool_masked_pos.reshape(-1, size[0], size[1], size[2])\n",
    "            mask = (bool_masked_pos.repeat_interleave(self.config.patch_size[0], 1)\n",
    "                    .repeat_interleave(self.config.patch_size[1], 2)\n",
    "                    .repeat_interleave(self.config.patch_size[2], 3)\n",
    "                    .unsqueeze(1)\n",
    "                    .contiguous()\n",
    "                )\n",
    "            reconstruction_loss = torch.nn.functional.l1_loss(pixel_values, reconstructed_pixel_values, reduction=\"none\")\n",
    "            masked_im_loss = (reconstruction_loss * mask).sum() / (mask.sum() + 1e-5) / self.config.num_channels\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (reconstructed_pixel_values,) + outputs[2:]\n",
    "            return ((masked_im_loss,) + output) if masked_im_loss is not None else output\n",
    "\n",
    "        return SwinMaskedImageModelingOutput(\n",
    "            loss=masked_im_loss,\n",
    "            reconstruction=reconstructed_pixel_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            reshaped_hidden_states=outputs.reshaped_hidden_states,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1290549",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Swin3dConfig(image_size=(96, 192, 192), depths=[2, 2], num_heads=[3, 6], patch_size=(8, 16, 16), encoder_stride=(16, 32, 32))\n",
    "model = Swin3dForMaskedImageModeling(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58713d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d533fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_masked_pos = mask_patches(model.num_patches, 0.4)\n",
    "bool_masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.randn((1, 1, )+model.config.image_size), bool_masked_pos=bool_masked_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d76322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8276, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474cf82",
   "metadata": {},
   "source": [
    "> TODO Say we want to calculate this at multiple levels? How do we do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c570ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medct",
   "language": "python",
   "name": "medct"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
