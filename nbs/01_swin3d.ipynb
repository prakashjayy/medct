{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db89f76",
   "metadata": {},
   "source": [
    "we are trying to modify the original 2D implementation of Swin in transformers and trying to make it work with 3D. \n",
    "\n",
    "\n",
    "Along the way we will try to understand a few concepts of Swin transformers. Specifically\n",
    "- window attention\n",
    "- shifted window attention \n",
    "- what a relative positional bias table is\n",
    "\n",
    "> [Swin Paper](https://arxiv.org/pdf/2103.14030.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970642b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp swin3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "from typing import Union, Optional, List, Tuple\n",
    "\n",
    "\n",
    "from transformers.modeling_outputs import BackboneOutput\n",
    "from transformers.modeling_utils import  PreTrainedModel\n",
    "from transformers.models.swin.configuration_swin import SwinConfig\n",
    "from transformers.pytorch_utils import meshgrid, find_pruneable_heads_and_indices\n",
    "from transformers.models.swin.modeling_swin import SwinEncoderOutput, SwinModelOutput, SwinMaskedImageModelingOutput, SwinImageClassifierOutput, drop_path, SwinDropPath\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.utils.backbone_utils import BackboneMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac52fa",
   "metadata": {},
   "source": [
    "## window partition\n",
    "\n",
    "this take input features of shape `(batch_size, depth, height, width, num_channels)` and window_size in `(d, h, w)` as inputs and output windows of shape ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8baf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = torch.randn((1, 56, 56, 56, 128))\n",
    "window_size = 7 #same in all direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3c421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 7, 8, 7, 8, 7, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, depth, height, width, num_channels = fe.shape\n",
    "fe = fe.view(\n",
    "        batch_size, depth // window_size, window_size, height // window_size, window_size, width // window_size, window_size, num_channels\n",
    "    )\n",
    "fe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e514a22",
   "metadata": {},
   "source": [
    "> we will have (8, 8, 8) windows and since batch size is 1, we will have total of 512 windows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a83d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 7, 7, 7, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = fe.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size, window_size, window_size, num_channels)\n",
    "windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.window_partition\n",
    "def window_partition3d(input_feature, window_size: Union[int, List[int]]):\n",
    "    \"\"\"\n",
    "    Partitions the given input into windows.\n",
    "    \"\"\"\n",
    "    window_depth, window_height, window_width = window_size if isinstance(window_size, collections.abc.Iterable) else (window_size, window_size, window_size)\n",
    "    batch_size, depth, height, width, num_channels = input_feature.shape\n",
    "    input_feature = input_feature.view(\n",
    "        batch_size, depth // window_depth, window_depth, height // window_height, window_height, width // window_width, window_width, num_channels\n",
    "    )\n",
    "    windows = input_feature.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_depth, window_height, window_width, num_channels)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a8617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 7, 7, 7, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe = torch.randn((1, 56, 56, 56, 128))\n",
    "window_size = 7 #same in all direction\n",
    "window_partition3d(fe, window_size).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd0c90",
   "metadata": {},
   "source": [
    "## Window reverse \n",
    "\n",
    "we have to reverse the same process. usually the way this happens . take features -> apply windowing -> apply attention -> reverse windows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d2548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_channels = windows.shape[-1]\n",
    "depth, height, width = 56, 56, 56\n",
    "num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef2d0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8, 8, 7, 7, 7, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = windows.view(-1, depth // window_size, height // window_size, width // window_size, window_size, window_size, window_size, num_channels)\n",
    "windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7096d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = windows.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, depth, height, width, num_channels)\n",
    "windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d4c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.window_reverse\n",
    "def window_reverse3d(windows, window_size, depth, height, width):\n",
    "    \"\"\"\n",
    "    Merges windows to produce higher resolution features.\n",
    "    \"\"\"\n",
    "    num_channels = windows.shape[-1]\n",
    "    window_depth, window_height, window_width = window_size if isinstance(window_size, collections.abc.Iterable) else (window_size, window_size, window_size)\n",
    "    windows = windows.view(-1, depth // window_depth, height // window_height, width // window_width, window_depth, window_height, window_width, num_channels)\n",
    "    windows = windows.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, depth, height, width, num_channels)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5966bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = torch.randn((1, 56, 56, 56, 128))\n",
    "window_size = 7 #same in all direction\n",
    "windows = window_partition3d(fe, window_size)\n",
    "num_channels = windows.shape[-1]\n",
    "depth, height, width = 56, 56, 56\n",
    "window_reverse3d(windows, window_size, depth, height, width).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe76a2",
   "metadata": {},
   "source": [
    "## SwinPatch Embeddings \n",
    "\n",
    "> First lets check How padding works for each axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1, 91, 187, 189)) # the original image is (92, 192, 192)\n",
    "batch_size, channels, depth, height, width = x.shape\n",
    "patch_size = (2, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "width % patch_size[2] != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_values = (0, 0, patch_size[1] - width % patch_size[1])\n",
    "pad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29244e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.pad(x, (0, 3)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da149b4",
   "metadata": {},
   "source": [
    "> how padding works `p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.pad(x, (0, 0, 0, 3)).shape #> pad along H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.pad(x, (0, 0, 0, 0, 0, 1)).shape #> pad along D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swin3dConfig:\n",
    "    num_channels = 1\n",
    "    image_size = (96, 192, 192)\n",
    "    patch_size = (2, 4, 4)\n",
    "    embed_dim = 128\n",
    "    use_absolute_embeddings=True\n",
    "    hidden_dropout_prob=0.0\n",
    "    qkv_bias=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinPatchEmbeddings\n",
    "class Swin3dPatchEmbeddings(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n",
    "    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n",
    "    Transformer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        image_size, patch_size = config.image_size, config.patch_size\n",
    "        num_channels, hidden_size = config.num_channels, config.embed_dim\n",
    "        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size, image_size)\n",
    "        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size, patch_size)\n",
    "        num_patches = (image_size[2] // patch_size[2])* (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.num_patches = num_patches\n",
    "        self.grid_size = (image_size[0] // patch_size[0], image_size[1] // patch_size[1], image_size[2] // patch_size[2])\n",
    "\n",
    "        self.projection = torch.nn.Conv3d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def maybe_pad(self, pixel_values, depth, height, width):\n",
    "        if width % self.patch_size[2] != 0:\n",
    "            pad_values = (0, self.patch_size[1] - width % self.patch_size[1])\n",
    "            pixel_values = nn.functional.pad(pixel_values, pad_values)\n",
    "        if height % self.patch_size[1] != 0:\n",
    "            pad_values = (0, 0, 0, self.patch_size[0] - height % self.patch_size[0])\n",
    "            pixel_values = nn.functional.pad(pixel_values, pad_values)\n",
    "        if depth % self.patch_size[0] !=0:\n",
    "            pad_values = (0, 0, 0, 0, 0, self.patch_size[0] - depth % self.patch_size[0])\n",
    "            pixel_values = nn.functional.pad(pixel_values, pad_values)\n",
    "        return pixel_values\n",
    "\n",
    "    def forward(self, pixel_values: Optional[torch.FloatTensor]) -> Tuple[torch.Tensor, Tuple[int]]:\n",
    "        _, num_channels, depth, height, width = pixel_values.shape\n",
    "        if num_channels != self.num_channels:\n",
    "            raise ValueError(\n",
    "                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n",
    "            )\n",
    "        # pad the input to be divisible by self.patch_size, if needed\n",
    "        pixel_values = self.maybe_pad(pixel_values, depth, height, width)\n",
    "        embeddings = self.projection(pixel_values)\n",
    "        _, _, depth, height, width = embeddings.shape\n",
    "        output_dimensions = (depth, height, width)\n",
    "        embeddings = embeddings.flatten(2).transpose(1, 2)\n",
    "        return embeddings, output_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf708bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Swin3dConfig()\n",
    "pe = Swin3dPatchEmbeddings(cfg)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b72b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1, 96, 192, 192))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b62275",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, od = pe(x)\n",
    "embeddings.shape, od"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df07e6",
   "metadata": {},
   "source": [
    "## SwinEmbeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e91f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinEmbeddings\n",
    "class Swin3dEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct the patch and position embeddings. Optionally, also the mask token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, use_mask_token=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embeddings = Swin3dPatchEmbeddings(config)\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.patch_grid = self.patch_embeddings.grid_size\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, config.embed_dim)) if use_mask_token else None\n",
    "\n",
    "        if config.use_absolute_embeddings:\n",
    "            self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.embed_dim))\n",
    "        else:\n",
    "            self.position_embeddings = None\n",
    "\n",
    "        self.norm = nn.LayerNorm(config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(\n",
    "        self, pixel_values: Optional[torch.FloatTensor], bool_masked_pos: Optional[torch.BoolTensor] = None\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        embeddings, output_dimensions = self.patch_embeddings(pixel_values)\n",
    "        embeddings = self.norm(embeddings)\n",
    "        batch_size, seq_len, _ = embeddings.size()\n",
    "\n",
    "        if bool_masked_pos is not None:\n",
    "            mask_tokens = self.mask_token.expand(batch_size, seq_len, -1)\n",
    "            # replace the masked visual tokens by mask_tokens\n",
    "            mask = bool_masked_pos.unsqueeze(-1).type_as(mask_tokens)\n",
    "            embeddings = embeddings * (1.0 - mask) + mask_tokens * mask\n",
    "\n",
    "        if self.position_embeddings is not None:\n",
    "            embeddings = embeddings + self.position_embeddings[:, 1:, :]\n",
    "\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings, output_dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "se = Swin3dEmbeddings(Swin3dConfig())\n",
    "se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, output_dimensions = se(x)\n",
    "embeddings.shape, output_dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6259ed",
   "metadata": {},
   "source": [
    "## SwinPatch Merging \n",
    "\n",
    "From the paper \n",
    "\n",
    "```\n",
    "To produce a hierarchical representation, the number of\n",
    "tokens is reduced by patch merging layers as the network\n",
    "gets deeper. The first patch merging layer concatenates the\n",
    "features of each group of 2 × 2 neighboring patches, and\n",
    "applies a linear layer on the 4C-dimensional concatenated\n",
    "features. This reduces the number of tokens by a multiple\n",
    "of 2 × 2 = 4 (2× downsampling of resolution), and the out-\n",
    "put dimension is set to 2C. Swin Transformer blocks are\n",
    "applied afterwards for feature transformation, with the res-\n",
    "olution kept at H8 × W8 . This first block of patch merging\n",
    "and feature transformation is denoted as “Stage 2”. The pro-\n",
    "cedure is repeated twice, as “Stage 3” and “Stage 4”, with\n",
    "output resolutions of H × W and H × W , respectively. 16 16 32 32\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6095a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth, height, width = od\n",
    "batch_size, dim, num_channels = embeddings.shape\n",
    "batch_size, dim, num_channels, depth, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b066e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature = embeddings.view(batch_size, depth, height, width, num_channels)\n",
    "input_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b937ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.pad(input_feature, (0, 1, 0, 1, 0, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((4, 4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0::2, 0::2], x[1::2, 0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1301c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0::2, 1::2], x[1::2, 1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.concat([x[0::2, 0::2], x[1::2, 0::2], x[0::2, 1::2], x[1::2, 1::2]], -1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c73e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b4bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.reshape(-1).sort().values == x.reshape(-1).sort().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c13465",
   "metadata": {},
   "source": [
    "> [:, 0::2, 0::2, :], [:, 1::2, 0::2, :], [:, 0::2, 1::2, :], [:, 1::2, 1::2, :]\n",
    "\n",
    "```\n",
    "0, 0\n",
    "1, 0\n",
    "0, 1\n",
    "1, 1\n",
    "```\n",
    "\n",
    "For 3d \n",
    "```\n",
    "0, 0, 0\n",
    "\n",
    "1, 0, 0\n",
    "0, 1, 0\n",
    "0, 0, 1\n",
    "\n",
    "1, 1, 0\n",
    "0, 1, 1\n",
    "1, 0, 1\n",
    "\n",
    "1, 1, 1\n",
    "```\n",
    "\n",
    "> [:, 0::2, 0::2, 0::2, :] ["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((4, 4, 4, 16))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b797913",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0::2, 0::2, 0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.cat([x[0::2, 0::2, 0::2, :],\n",
    "               x[1::2, 0::2, 0::2, :], \n",
    "               x[0::2, 1::2, 0::2, :], \n",
    "               x[0::2, 0::2, 1::2, :], \n",
    "               x[1::2, 1::2, 0::2, :], \n",
    "               x[0::2, 1::2, 1::2, :], \n",
    "               x[1::2, 0::2, 1::2, :], \n",
    "               x[1::2, 1::2, 1::2, :]], axis=-1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42719735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinPatchMerging\n",
    "class Swin3dPatchMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (`Tuple[int]`):\n",
    "            Resolution of input feature.\n",
    "        dim (`int`):\n",
    "            Number of input channels.\n",
    "        norm_layer (`nn.Module`, *optional*, defaults to `nn.LayerNorm`):\n",
    "            Normalization layer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution: Tuple[int], dim: int, norm_layer: nn.Module = nn.LayerNorm) -> None:\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(8 * dim)\n",
    "\n",
    "    def maybe_pad(self, input_feature, depth, height, width):\n",
    "        should_pad =  (depth % 2 == 1) or (height % 2 == 1) or (width % 2 == 1)\n",
    "        if should_pad:\n",
    "            pad_values = (0, width % 2, 0, height % 2, 0, depth % 2)\n",
    "            input_feature = nn.functional.pad(input_feature, pad_values)\n",
    "\n",
    "        return input_feature\n",
    "\n",
    "    def forward(self, input_feature: torch.Tensor, input_dimensions: Tuple[int, int]) -> torch.Tensor:\n",
    "        depth, height, width = input_dimensions\n",
    "        # `dim` is depth* height * width\n",
    "        batch_size, dim, num_channels = input_feature.shape\n",
    "\n",
    "        input_feature = input_feature.view(batch_size, depth, height, width, num_channels)\n",
    "        # pad input to be disible by depth, width and height, if needed\n",
    "        input_feature = self.maybe_pad(input_feature, depth, height, width)\n",
    "        \n",
    "        # batch_size depth/2 height/2 width/2 4*num_channels\n",
    "        input_feature = torch.cat([input_feature[:, 0::2, 0::2, 0::2, :],\n",
    "                                   input_feature[:, 1::2, 0::2, 0::2, :], \n",
    "                                   input_feature[:, 0::2, 1::2, 0::2, :], \n",
    "                                   input_feature[:, 0::2, 0::2, 1::2, :], \n",
    "                                   input_feature[:, 1::2, 1::2, 0::2, :], \n",
    "                                   input_feature[:, 0::2, 1::2, 1::2, :], \n",
    "                                   input_feature[:, 1::2, 0::2, 1::2, :], \n",
    "                                   input_feature[:, 1::2, 1::2, 1::2, :]], axis=-1)\n",
    "        input_feature = input_feature.view(batch_size, -1, 8 * num_channels)  # batch_size depth/2*height/2*width/2 8*C\n",
    "\n",
    "        input_feature = self.norm(input_feature)\n",
    "        input_feature = self.reduction(input_feature)\n",
    "\n",
    "        return input_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d36f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "patchmerge = Swin3dPatchMerging((48, 48, 48), dim=128)\n",
    "patchmerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 48* 48 *48, 128))\n",
    "patchmerge(x, (48, 48, 48)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff8829",
   "metadata": {},
   "outputs": [],
   "source": [
    "24*24*24 == 13824"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20510932",
   "metadata": {},
   "source": [
    "## SwinAttentions \n",
    "\n",
    "### Relative positional bias \n",
    "\n",
    "![](../assets/swin_relative_positional_bias.png)\n",
    "\n",
    "so the relative positonal bias is a matrix of [2M-1] x [2M-1] where m is the window size. Since we are using 3D. we will have [2M-1] x [2M-1] x [2M-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3eab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = (2, 4, 4)\n",
    "(2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1) #3*7*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d50c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_d = torch.arange(window_size[0])\n",
    "coords_h = torch.arange(window_size[1])\n",
    "coords_w = torch.arange(window_size[2])\n",
    "coords = torch.stack(meshgrid([coords_d, coords_h, coords_w], indexing=\"ij\"))\n",
    "coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab037ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_flatten = torch.flatten(coords, 1)\n",
    "coords_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_flatten.permute(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d62891",
   "metadata": {},
   "source": [
    "we have `2*4*4` (32) coordinates. Now each coordinate we need to have relative position to anotther coordinate, so we get 32 x 32 matrix . also since each coordinate is represented by (x, y, z) we will have (3, 32, 32) matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90443150",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "relative_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c49762",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "relative_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41405c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_coords[0, 0, :], relative_coords[31, 0, :], relative_coords[0, 31, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ea926",
   "metadata": {},
   "source": [
    "> relative coordinates can be negative or positive. we have to make them all positive. so shift them by M-1 in respective dimesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5974100",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_coords[:, :, 0].min(), relative_coords[:, :, 1].min(), relative_coords[:, :, 2].min()\n",
    "# this is equal to window_size-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91020bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_coords[:, :, 0] += window_size[0] - 1\n",
    "relative_coords[:, :, 1] += window_size[1] - 1\n",
    "relative_coords[:, :, 2] += window_size[2] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14674d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_coords[0, 0, :], relative_coords[31, 0, :], relative_coords[0, 31, :]\n",
    "#relative distance still the same but the all values are same now "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf8605",
   "metadata": {},
   "source": [
    "> It has to become [2, 6, 6] if we add should become 146 in order for this to work so that we can pick up that index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb882e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_coords[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_coords[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c771fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_coords[:, :, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad19568",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_coords[:, :, 0] *= (2 * window_size[1] - 1) * (2 * window_size[2] - 1)\n",
    "relative_coords[:, :, 1] *= 2 * window_size[2] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = relative_coords.sum(-1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e75fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "colors_list = ['#0099ff', '#33cc33']\n",
    "cmap = colors.ListedColormap(colors_list)\n",
    "mask=mask.numpy()\n",
    "plt.imshow(mask,cmap=cmap)\n",
    "for i in range(mask.shape[0]):\n",
    "    for j in range(mask.shape[1]):\n",
    "        plt.annotate(str(int(mask[i, j])), xy=(j+0.25, i+0.25),\n",
    "                     ha='center', va='center', color='white', size=6)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def create_relative_positional_index_3d(window_size):\n",
    "    coords_d = torch.arange(window_size[0])\n",
    "    coords_h = torch.arange(window_size[1])\n",
    "    coords_w = torch.arange(window_size[2])\n",
    "    coords = torch.stack(meshgrid([coords_d, coords_h, coords_w], indexing=\"ij\"))\n",
    "    coords_flatten = torch.flatten(coords, 1)\n",
    "    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "    relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "    relative_coords[:, :, 0] += window_size[0] - 1\n",
    "    relative_coords[:, :, 1] += window_size[1] - 1\n",
    "    relative_coords[:, :, 2] += window_size[2] - 1\n",
    "    relative_coords[:, :, 0] *= (2 * window_size[1] - 1) * (2 * window_size[2] - 1)\n",
    "    relative_coords[:, :, 1] *= 2 * window_size[2] - 1\n",
    "    relative_coords = relative_coords.sum(-1)\n",
    "    return relative_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d5f1e",
   "metadata": {},
   "source": [
    "> Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swin3dConfig:\n",
    "    num_channels = 1\n",
    "    image_size = (96, 192, 192)\n",
    "    patch_size = (2, 4, 4)\n",
    "    embed_dim = 128\n",
    "    use_absolute_embeddings=True\n",
    "    hidden_dropout_prob=0.0\n",
    "    qkv_bias=True\n",
    "    attention_probs_dropout_prob= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinSelfAttention\n",
    "class Swin3dSelfAttention(nn.Module):\n",
    "    def __init__(self, config, dim, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        if dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({dim}) is not a multiple of the number of attention heads ({num_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = num_heads\n",
    "        self.attention_head_size = int(dim / num_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.window_size = (\n",
    "            window_size if isinstance(window_size, collections.abc.Iterable) else (window_size, window_size, window_size)\n",
    "        )\n",
    "\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1), num_heads)\n",
    "        )\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        \n",
    "        relative_position_index = create_relative_positional_index_3d(self.window_size)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.query = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n",
    "        self.key = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n",
    "        self.value = nn.Linear(self.all_head_size, self.all_head_size, bias=config.qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        batch_size, dim, num_channels = hidden_states.shape\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)]\n",
    "        relative_position_bias = relative_position_bias.view(\n",
    "            self.window_size[0] * self.window_size[1] * self.window_size[2], \n",
    "            self.window_size[0] * self.window_size[1] * self.window_size[2], -1\n",
    "        )\n",
    "\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attention_scores = attention_scores + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in SwinModel forward() function)\n",
    "            mask_shape = attention_mask.shape[0]\n",
    "            attention_scores = attention_scores.view(\n",
    "                batch_size // mask_shape, mask_shape, self.num_attention_heads, dim, dim\n",
    "            )\n",
    "            attention_scores = attention_scores + attention_mask.unsqueeze(1).unsqueeze(0)\n",
    "            attention_scores = attention_scores.view(-1, self.num_attention_heads, dim, dim)\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssa = Swin3dSelfAttention(Swin3dConfig(), window_size=(2, 4, 4), dim=128*4, num_heads=4)\n",
    "ssa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deadcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.randn((3456, 32, 512))\n",
    "outputs = ssa(hidden_states)\n",
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569934b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "#Copied from transformers.models.swin.modeling_swin.SwinSelfOutput\n",
    "class SwinSelfOutput(nn.Module):\n",
    "    def __init__(self, config, dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a692a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export  \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinAttention\n",
    "class Swin3dAttention(nn.Module):\n",
    "    def __init__(self, config, dim, num_heads, window_size):\n",
    "        super().__init__()\n",
    "        self.self = Swin3dSelfAttention(config, dim, num_heads, window_size)\n",
    "        self.output = SwinSelfOutput(config, dim)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        self_outputs = self.self(hidden_states, attention_mask, head_mask, output_attentions)\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd305b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinIntermediate\n",
    "class SwinIntermediate(nn.Module):\n",
    "    def __init__(self, config, dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(dim, int(config.mlp_ratio * dim))\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinOutput\n",
    "class SwinOutput(nn.Module):\n",
    "    def __init__(self, config, dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(int(config.mlp_ratio * dim), dim)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9399a7",
   "metadata": {},
   "source": [
    "## Swin Layer \n",
    "\n",
    "First look at what shifted window means in 2D. Lets implement the same in 3D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f099128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swin3dConfig:\n",
    "    num_channels = 1\n",
    "    image_size = (96, 192, 192)\n",
    "    patch_size = (2, 4, 4)\n",
    "    window_size = (2, 4, 4)\n",
    "    embed_dim = 128\n",
    "    use_absolute_embeddings=True\n",
    "    hidden_dropout_prob=0.0\n",
    "    qkv_bias=True\n",
    "    attention_probs_dropout_prob= True\n",
    "    chunk_size_feed_forward=True\n",
    "    layer_norm_eps=1e-6\n",
    "    drop_path_rate=0\n",
    "    mlp_ratio=4\n",
    "    hidden_act=\"gelu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d92e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resolution = (24, 24, 24)\n",
    "window_size = (2, 4, 4)\n",
    "min(input_resolution) <= min(window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinLayer\n",
    "class Swin3dLayer(nn.Module):\n",
    "    def __init__(self, config, dim, input_resolution, num_heads, shift_size=0):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.shift_size = shift_size\n",
    "        self.window_size = config.window_size\n",
    "        self.input_resolution = input_resolution\n",
    "        self.layernorm_before = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n",
    "        self.attention = Swin3dAttention(config, dim, num_heads, window_size=self.window_size)\n",
    "        self.drop_path = SwinDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n",
    "        self.layernorm_after = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n",
    "        self.intermediate = SwinIntermediate(config, dim)\n",
    "        self.output = SwinOutput(config, dim)\n",
    "\n",
    "    def set_shift_and_window_size(self, input_resolution):\n",
    "        if input_resolution[0] <= self.window_size[0]:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size[0] = 0\n",
    "            self.window_size[0] = input_resolution[0]\n",
    "\n",
    "        if input_resolution[1] <= self.window_size[1]:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size[1] = 0\n",
    "            self.window_size[1] = input_resolution[1]\n",
    "\n",
    "        if input_resolution[2] <= self.window_size[2]:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size[2] = 0\n",
    "            self.window_size[2] = input_resolution[2]\n",
    "\n",
    "    def get_attn_mask(self, depth, height, width, dtype):\n",
    "        if (self.shift_size[0] > 0) or (self.shift_size[1] > 0) or (self.shift_size[2] > 0):\n",
    "            # calculate attention mask for SW-MSA\n",
    "            img_mask = torch.zeros((1, depth, height, width, 1), dtype=dtype)\n",
    "            depth_slices = (\n",
    "                slice(0, -self.window_size[0]),\n",
    "                slice(-self.window_size[0], -self.shift_size[0]),\n",
    "                slice(-self.shift_size[0], None),\n",
    "            )\n",
    "            height_slices = (\n",
    "                slice(0, -self.window_size[1]),\n",
    "                slice(-self.window_size[1], -self.shift_size[1]),\n",
    "                slice(-self.shift_size[1], None),\n",
    "            )\n",
    "            width_slices = (\n",
    "                slice(0, -self.window_size[2]),\n",
    "                slice(-self.window_size[2], -self.shift_size[2]),\n",
    "                slice(-self.shift_size[2], None),\n",
    "            )\n",
    "            count = 0\n",
    "            for d in depth_slices:\n",
    "                for h in height_slices:\n",
    "                    for w in width_slices:\n",
    "                        img_mask[:, d, h, w, :] = count\n",
    "                        count += 1\n",
    "\n",
    "            mask_windows = window_partition3d(img_mask, self.window_size)\n",
    "            mask_windows = mask_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2])\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "        return attn_mask\n",
    "\n",
    "    def maybe_pad(self, hidden_states, depth, height, width):\n",
    "        pad_right = (self.window_size[2] - width % self.window_size[2]) % self.window_size[2]\n",
    "        pad_bottom = (self.window_size[1] - height % self.window_size[1]) % self.window_size[1]\n",
    "        pad_depth = (self.window_size[0] - height % self.window_size[0]) % self.window_size[0]\n",
    "        pad_values = (0, 0, 0, pad_right, 0, pad_bottom, 0, pad_depth)\n",
    "        hidden_states = nn.functional.pad(hidden_states, pad_values)\n",
    "        return hidden_states, pad_values\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        input_dimensions: Tuple[int, int],\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        always_partition: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if not always_partition:\n",
    "            self.set_shift_and_window_size(input_dimensions)\n",
    "        else:\n",
    "            pass\n",
    "        depth, height, width = input_dimensions\n",
    "        batch_size, _, channels = hidden_states.size()\n",
    "        shortcut = hidden_states\n",
    "\n",
    "        hidden_states = self.layernorm_before(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states.view(batch_size, depth, height, width, channels)\n",
    "\n",
    "        # pad hidden_states to multiples of window size\n",
    "        hidden_states, pad_values = self.maybe_pad(hidden_states, depth, height, width)\n",
    "\n",
    "        _, depth_pad, height_pad, width_pad, _ = hidden_states.shape\n",
    "        # cyclic shift\n",
    "        if (self.shift_size[0] > 0) or (self.shift_size[1] > 0) or (self.shift_size[2] > 0):\n",
    "            shifted_hidden_states = torch.roll(hidden_states, shifts=(-self.shift_size[0], -self.shift_size[1], -self.shift_size[2]), dims=(1, 2, 3))\n",
    "        else:\n",
    "            shifted_hidden_states = hidden_states\n",
    "\n",
    "        # partition windows\n",
    "        hidden_states_windows = window_partition3d(shifted_hidden_states, self.window_size)\n",
    "        hidden_states_windows = hidden_states_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2], channels)\n",
    "        attn_mask = self.get_attn_mask(depth_pad, height_pad, width_pad, dtype=hidden_states.dtype)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.to(hidden_states_windows.device)\n",
    "\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states_windows, attn_mask, head_mask, output_attentions=output_attentions\n",
    "        )\n",
    "\n",
    "        attention_output = attention_outputs[0]\n",
    "\n",
    "        attention_windows = attention_output.view(-1, self.window_size[0], self.window_size[1], self.window_size[2], channels)\n",
    "        shifted_windows = window_reverse3d(attention_windows, self.window_size, depth_pad, height_pad, width_pad, )\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if (self.shift_size[0] > 0) or (self.shift_size[1] > 0) or (self.shift_size[2] > 0):\n",
    "            attention_windows = torch.roll(shifted_windows, shifts=(self.shift_size[0], self.shift_size[1], self.shift_size[2]), dims=(1, 2, 3))\n",
    "        else:\n",
    "            attention_windows = shifted_windows\n",
    "\n",
    "        was_padded = pad_values[3] > 0 or pad_values[5] > 0 or pad_values[7]>0\n",
    "        if was_padded:\n",
    "            attention_windows = attention_windows[:, :depth, :height, :width, :].contiguous()\n",
    "\n",
    "        attention_windows = attention_windows.view(batch_size, height * width * depth, channels)\n",
    "\n",
    "        hidden_states = shortcut + self.drop_path(attention_windows)\n",
    "\n",
    "        layer_output = self.layernorm_after(hidden_states)\n",
    "        layer_output = self.intermediate(layer_output)\n",
    "        layer_output = hidden_states + self.output(layer_output)\n",
    "\n",
    "        layer_outputs = (layer_output, attention_outputs[1]) if output_attentions else (layer_output,)\n",
    "        return layer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bebeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = Swin3dLayer(Swin3dConfig(), dim=128, input_resolution=(24, 24, 24), num_heads=4, shift_size=(2, 2, 2))\n",
    "sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3098eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl(torch.randn((1, 24*24*24, 128)), (24, 24, 24))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e70151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinStage\n",
    "class Swin3dStage(nn.Module):\n",
    "    def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, downsample):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dim = dim\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Swin3dLayer(\n",
    "                    config=config,\n",
    "                    dim=dim,\n",
    "                    input_resolution=input_resolution,\n",
    "                    num_heads=num_heads,\n",
    "                    shift_size=(0 if (i % 2 == 0) else config.window_size[0] // 2, \n",
    "                                0 if (i % 2 == 0) else config.window_size[1] // 2, \n",
    "                                0 if (i % 2 == 0) else config.window_size[2] // 2),\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=nn.LayerNorm)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        self.pointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        input_dimensions: Tuple[int, int],\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        always_partition: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        depth, height, width = input_dimensions\n",
    "        for i, layer_module in enumerate(self.blocks):\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "        hidden_states_before_downsampling = hidden_states\n",
    "        if self.downsample is not None:\n",
    "            depth_downsampled, height_downsampled, width_downsampled = (depth + 1) // 2, (height + 1) // 2, (width + 1) // 2\n",
    "            output_dimensions = (depth, height, width, depth_downsampled, height_downsampled, width_downsampled)\n",
    "            hidden_states = self.downsample(hidden_states_before_downsampling, input_dimensions)\n",
    "        else:\n",
    "            output_dimensions = (depth, height, width, depth, height, width)\n",
    "\n",
    "        stage_outputs = (hidden_states, hidden_states_before_downsampling, output_dimensions)\n",
    "\n",
    "        if output_attentions:\n",
    "            stage_outputs += layer_outputs[1:]\n",
    "        return stage_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class Swin3dConfig(SwinConfig):\n",
    "    def __init__(self,\n",
    "        image_size=(92, 192, 192),\n",
    "        patch_size=(2, 4, 4),\n",
    "        num_channels=1,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=(2, 4, 4),\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        hidden_dropout_prob=0.0,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "        drop_path_rate=0.1,\n",
    "        hidden_act=\"gelu\",\n",
    "        use_absolute_embeddings=False,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-5,\n",
    "        encoder_stride=32,\n",
    "        out_features=None,\n",
    "        out_indices=None,\n",
    "        **kwargs):\n",
    "        super().__init__(image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        num_channels=num_channels,\n",
    "        embed_dim=embed_dim,\n",
    "        depths=depths,\n",
    "        num_heads=num_heads,\n",
    "        window_size=window_size,\n",
    "        mlp_ratio=mlp_ratio,\n",
    "        qkv_bias=qkv_bias,\n",
    "        hidden_dropout_prob=hidden_dropout_prob,\n",
    "        attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "        drop_path_rate=drop_path_rate,\n",
    "        hidden_act=hidden_act,\n",
    "        use_absolute_embeddings=use_absolute_embeddings,\n",
    "        initializer_range=initializer_range,\n",
    "        layer_norm_eps=layer_norm_eps,\n",
    "        encoder_stride=encoder_stride,\n",
    "        out_features=out_features,\n",
    "        out_indices=out_indices,\n",
    "        **kwargs)\n",
    "        assert len(image_size) == 3, \"we need 3 dimesional image size\"\n",
    "        assert len(patch_size) == 3, \"we need 3 dimesional patch size\"\n",
    "        assert len(window_size) == 3, \"we need 3 dimesional window size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Swin3dConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b435d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class Swin3dEncoder(nn.Module):\n",
    "    def __init__(self, config, grid_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(config.depths)\n",
    "        self.config = config\n",
    "        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                Swin3dStage(\n",
    "                    config=config,\n",
    "                    dim=int(config.embed_dim * 2**i_layer),\n",
    "                    input_resolution=(grid_size[0] // (2**i_layer), \n",
    "                                      grid_size[1] // (2**i_layer), \n",
    "                                      grid_size[2] // (2**i_layer)),\n",
    "                    depth=config.depths[i_layer],\n",
    "                    num_heads=config.num_heads[i_layer],\n",
    "                    drop_path=dpr[sum(config.depths[:i_layer]) : sum(config.depths[: i_layer + 1])],\n",
    "                    downsample=Swin3dPatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                )\n",
    "                for i_layer in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        input_dimensions: Tuple[int, int],\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        output_hidden_states_before_downsampling: Optional[bool] = False,\n",
    "        always_partition: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple, SwinEncoderOutput]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_reshaped_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "\n",
    "        if output_hidden_states:\n",
    "            batch_size, _, hidden_size = hidden_states.shape\n",
    "            # rearrange b (d h w) c -> b c d h w\n",
    "            reshaped_hidden_state = hidden_states.view(batch_size, *input_dimensions, hidden_size)\n",
    "            reshaped_hidden_state = reshaped_hidden_state.permute(0, 4, 1, 2, 3)\n",
    "            all_hidden_states += (hidden_states,)\n",
    "            all_reshaped_hidden_states += (reshaped_hidden_state,)\n",
    "\n",
    "        for i, layer_module in enumerate(self.layers):\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__, hidden_states, input_dimensions, layer_head_mask, output_attentions\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            hidden_states_before_downsampling = layer_outputs[1]\n",
    "            output_dimensions = layer_outputs[2]\n",
    "\n",
    "            input_dimensions = (output_dimensions[-3], output_dimensions[-2], output_dimensions[-1])\n",
    "\n",
    "            if output_hidden_states and output_hidden_states_before_downsampling:\n",
    "                batch_size, _, hidden_size = hidden_states_before_downsampling.shape\n",
    "                # rearrange b (d h w) c -> b c d h w\n",
    "                # here we use the original (not downsampled) height and width\n",
    "                reshaped_hidden_state = hidden_states_before_downsampling.view(\n",
    "                    batch_size, *(output_dimensions[0], output_dimensions[1], output_dimensions[2]), hidden_size\n",
    "                )\n",
    "                reshaped_hidden_state = reshaped_hidden_state.permute(0, 4, 1, 2, 3)\n",
    "                all_hidden_states += (hidden_states_before_downsampling,)\n",
    "                all_reshaped_hidden_states += (reshaped_hidden_state,)\n",
    "            elif output_hidden_states and not output_hidden_states_before_downsampling:\n",
    "                batch_size, _, hidden_size = hidden_states.shape\n",
    "                # rearrange b (d h w) c -> b c d h w\n",
    "                reshaped_hidden_state = hidden_states.view(batch_size, *input_dimensions, hidden_size)\n",
    "                reshaped_hidden_state = reshaped_hidden_state.permute(0, 4, 1, 2, 3)\n",
    "                all_hidden_states += (hidden_states,)\n",
    "                all_reshaped_hidden_states += (reshaped_hidden_state,)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions += layer_outputs[3:]\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
    "\n",
    "        return SwinEncoderOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            reshaped_hidden_states=all_reshaped_hidden_states,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "Swin3dEncoder(Swin3dConfig(), grid_size=(48, 48, 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinPreTrainedModel\n",
    "class Swin3dPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = SwinConfig\n",
    "    base_model_prefix = \"swin\"\n",
    "    main_input_name = \"pixel_values\"\n",
    "    supports_gradient_checkpointing = True\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Conv3d)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.SwinModel\n",
    "class Swin3dModel(Swin3dPreTrainedModel):\n",
    "    def __init__(self, config, add_pooling_layer=True, use_mask_token=False):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_layers = len(config.depths)\n",
    "        self.num_features = int(config.embed_dim * 2 ** (self.num_layers - 1))\n",
    "\n",
    "        self.embeddings = Swin3dEmbeddings(config, use_mask_token=use_mask_token)\n",
    "        self.encoder = Swin3dEncoder(config, self.embeddings.patch_grid)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(self.num_features, eps=config.layer_norm_eps)\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.patch_embeddings\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.FloatTensor] = None,\n",
    "        bool_masked_pos: Optional[torch.BoolTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SwinModelOutput]:\n",
    "        r\"\"\"\n",
    "        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n",
    "            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, len(self.config.depths))\n",
    "\n",
    "        embedding_output, input_dimensions = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            input_dimensions,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states, \n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        sequence_output = self.layernorm(sequence_output)\n",
    "\n",
    "        pooled_output = None\n",
    "        if self.pooler is not None:\n",
    "            pooled_output = self.pooler(sequence_output.transpose(1, 2))\n",
    "            pooled_output = torch.flatten(pooled_output, 1)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "            return output\n",
    "\n",
    "        return SwinModelOutput(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            reshaped_hidden_states=encoder_outputs.reshaped_hidden_states,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b66f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Swin3dConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3m = Swin3dModel(Swin3dConfig(), False, False)\n",
    "s3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462750bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3m.encoder.layers[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645dc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3m.encoder.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1, 96, 192, 192))\n",
    "out = s3m(x, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e79b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.shape for i in out.hidden_states]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3aa97d",
   "metadata": {},
   "source": [
    "The following modifications are done. \n",
    "- we have to output the `hidden_states_before downsampling` as per the paper but here `hidden_state` True will give ouputs of `hidden_state_after_downsampling` . so we made changes accordingly\n",
    "- Swin3D basically works with only 3D images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f8db9",
   "metadata": {},
   "source": [
    "The following is what is defined in the paper\n",
    "• Swin-T: C = 96, layer numbers = {2, 2, 6, 2}\n",
    "• Swin-S: C = 96, layer numbers ={2, 2, 18, 2}\n",
    "• Swin-B: C = 128, layer numbers ={2, 2, 18, 2} \n",
    "• Swin-L: C = 192, layer numbers ={2, 2, 18, 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88456e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "swint = Swin3dModel(Swin3dConfig(embed_dim=96, depths=(2, 2, 6, 2)), False, False)\n",
    "count = 0\n",
    "for name, params in swint.named_parameters():\n",
    "    count += params.numel()\n",
    "count #29,068,446"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0228f2",
   "metadata": {},
   "source": [
    "> 29 million parameters will be an overkill may be. we will see how to keep it to only 2 layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "swint = Swin3dModel(Swin3dConfig(embed_dim=96, depths=(2, 6), num_heads=[3, 6], stage_names=[\"stem\", \"stage1\", \"stage2\"]), False, False)\n",
    "count = 0\n",
    "for name, params in swint.named_parameters():\n",
    "    count += params.numel()\n",
    "count #3051774 this came down to 3 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1, 96, 192, 192))\n",
    "out = swint(x, output_hidden_states=True)\n",
    "[i.shape for i in out.hidden_states]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8458f",
   "metadata": {},
   "source": [
    "The following is what we need to test for 3D \n",
    "\n",
    "• Swin-T: C = 96, layer numbers = {2, 6}  \n",
    "• Swin-S: C = 96, layer numbers ={2, 18}  \n",
    "• Swin-B: C = 128, layer numbers ={2, 18}   \n",
    "• Swin-L: C = 192, layer numbers ={2, 18}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b34c80",
   "metadata": {},
   "source": [
    "> we will primarily use this repo for backbones and so lets define a backbone here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693716b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# Copied from transformers.models.swin.modeling_swin.Swin3dBackbone\n",
    "class Swin3dBackbone(Swin3dPreTrainedModel, BackboneMixin):\n",
    "    def __init__(self, config: Swin3dConfig):\n",
    "        super().__init__(config)\n",
    "        super()._init_backbone(config)\n",
    "\n",
    "        self.num_features = [config.embed_dim] + [int(config.embed_dim * 2**i) for i in range(len(config.depths))]\n",
    "        self.embeddings = Swin3dEmbeddings(config)\n",
    "        self.encoder = Swin3dEncoder(config, self.embeddings.patch_grid)\n",
    "\n",
    "        # Add layer norms to hidden states of out_features\n",
    "        hidden_states_norms = {}\n",
    "        for stage, num_channels in zip(self._out_features, self.channels):\n",
    "            hidden_states_norms[stage] = nn.LayerNorm(num_channels)\n",
    "        self.hidden_states_norms = nn.ModuleDict(hidden_states_norms)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.patch_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.Tensor,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> BackboneOutput:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "\n",
    "        embedding_output, input_dimensions = self.embeddings(pixel_values)\n",
    "\n",
    "        outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            input_dimensions,\n",
    "            head_mask=None,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=True,\n",
    "            output_hidden_states_before_downsampling=True,\n",
    "            always_partition=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.reshaped_hidden_states\n",
    "\n",
    "        feature_maps = ()\n",
    "        for stage, hidden_state in zip(self.stage_names, hidden_states):\n",
    "            if stage in self.out_features:\n",
    "                batch_size, num_channels, depth, height, width = hidden_state.shape\n",
    "                hidden_state = hidden_state.permute(0, 2, 3, 4, 1).contiguous()\n",
    "                hidden_state = hidden_state.view(batch_size, height * width * depth, num_channels)\n",
    "                hidden_state = self.hidden_states_norms[stage](hidden_state)\n",
    "                hidden_state = hidden_state.view(batch_size, depth, height, width, num_channels)\n",
    "                hidden_state = hidden_state.permute(0, 4, 1, 2, 3).contiguous()\n",
    "                feature_maps += (hidden_state,)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (feature_maps,)\n",
    "            if output_hidden_states:\n",
    "                output += (outputs.hidden_states,)\n",
    "            return output\n",
    "\n",
    "        return BackboneOutput(\n",
    "            feature_maps=feature_maps,\n",
    "            hidden_states=outputs.hidden_states if output_hidden_states else None,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edfb3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Swin3dConfig(embed_dim=96, depths=(2, 4), out_features=[\"stage1\", \"stage2\"])\n",
    "model = Swin3dBackbone(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bbdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.randn((1, 1, 96, 192, 192)), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92996433",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.shape for i in out.feature_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medct",
   "language": "python",
   "name": "medct"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
