# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00a_convnextv2mim.ipynb.

# %% auto 0
__all__ = ['mask_patches', 'ConvNextV2MaskedImageModelingOutput', 'ConvNextV2ForMaskedImageModeling']

# %% ../nbs/00a_convnextv2mim.ipynb 2
import torch

from dataclasses import dataclass
from typing import List, Optional, Tuple, Union
from transformers.utils import ModelOutput
from .convnextv2 import ConvNextV2Model3d, ConvNextV2Config3d, ConvNextV2PreTrainedModel3d
from .swin3dmim import PixelShuffle3d

# %% ../nbs/00a_convnextv2mim.ipynb 3
# Copied from medct.swin3dmim.mask_patches
def mask_patches(num_patches, mask_ratio=0.5):
    len_keep = int(num_patches * (1 - mask_ratio))
    x = torch.cat([torch.zeros((len_keep)), torch.ones((num_patches-len_keep))])
    x = x[torch.randperm(num_patches)].view(1, -1)
    return x

# %% ../nbs/00a_convnextv2mim.ipynb 4
@dataclass
class ConvNextV2MaskedImageModelingOutput(ModelOutput):
    """
    Swin masked image model outputs.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `bool_masked_pos` is provided):
            Masked image modeling (MLM) loss.
        reconstruction (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):
            Reconstructed pixel values.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of
            shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
    """

    loss: Optional[torch.FloatTensor] = None
    reconstruction: torch.FloatTensor = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None

# %% ../nbs/00a_convnextv2mim.ipynb 9
# Copied from transformers.models.swin.modeling_swin.SwinForMaskedImageModeling
class ConvNextV2ForMaskedImageModeling(ConvNextV2PreTrainedModel3d):
    def __init__(self, config):
        super().__init__(config)
        num_layers = len(config.hidden_sizes)
        config.encoder_stride = (config.patch_size[0]*num_layers, 
                                 config.patch_size[1]*num_layers, 
                                 config.patch_size[2]*num_layers)
        self.num_patches = (config.image_size[0] // config.patch_size[0]) * \
                           (config.image_size[1] // config.patch_size[1]) * \
                           (config.image_size[2] // config.patch_size[2])
        if len(config.encoder_stride) !=3: raise NotImplementedError("The length of encoder stride should be 3")
        self.model = ConvNextV2Model3d(config, use_mask_token=True)

        num_features = config.hidden_sizes[-1]
        d_stride, h_stride, w_stride = config.encoder_stride
        self.decoder = torch.nn.Sequential(
            torch.nn.Conv3d(
                in_channels=num_features, out_channels=(d_stride*h_stride*w_stride) * config.num_channels, kernel_size=1
            ),
            PixelShuffle3d(config.encoder_stride),
        )
        
        # Initialize weights and apply final processing
        self.post_init()

    def forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        bool_masked_pos: Optional[torch.BoolTensor] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, ConvNextV2MaskedImageModelingOutput]:
        
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.model(
            pixel_values,
            bool_masked_pos=bool_masked_pos,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        sequence_output = outputs[0]
#         # Reshape to (batch_size, num_channels, depth, height, width)
#         sequence_output = sequence_output.transpose(1, 2)
#         batch_size, num_channels, sequence_length = sequence_output.shape
#         depth= height = width = math.ceil(sequence_length**(1/3))
#         sequence_output = sequence_output.reshape(batch_size, num_channels, depth, height, width)
        

        # Reconstruct pixel values
        reconstructed_pixel_values = self.decoder(sequence_output)

        masked_im_loss = None
        if bool_masked_pos is not None:
            size = (self.config.image_size[0] // self.config.patch_size[0],
                    self.config.image_size[1] // self.config.patch_size[1], 
                    self.config.image_size[2] // self.config.patch_size[2])
                    
            bool_masked_pos = bool_masked_pos.reshape(-1, size[0], size[1], size[2])
            mask = (bool_masked_pos.repeat_interleave(self.config.patch_size[0], 1)
                    .repeat_interleave(self.config.patch_size[1], 2)
                    .repeat_interleave(self.config.patch_size[2], 3)
                    .unsqueeze(1)
                    .contiguous()
                )
            reconstruction_loss = torch.nn.functional.l1_loss(pixel_values, reconstructed_pixel_values, reduction="none")
            masked_im_loss = (reconstruction_loss * mask).sum() / (mask.sum() + 1e-5) / self.config.num_channels

        if not return_dict:
            output = (reconstructed_pixel_values,) + outputs[2:]
            return ((masked_im_loss,) + output) if masked_im_loss is not None else output

        return ConvNextV2MaskedImageModelingOutput(
            loss=masked_im_loss,
            reconstruction=reconstructed_pixel_values,
            hidden_states=outputs.hidden_states,
        )
