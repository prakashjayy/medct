# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01a_swin3d_mim.ipynb.

# %% auto 0
__all__ = ['PixelShuffle3d', 'mask_patches', 'Swin3dForMaskedImageModeling']

# %% ../nbs/01a_swin3d_mim.ipynb 2
import torch
import math
from typing import Tuple, List, Optional, Union
from .swin3d import Swin3dModel, Swin3dConfig, Swin3dPreTrainedModel
from transformers.models.swin.modeling_swin import SwinMaskedImageModelingOutput

# %% ../nbs/01a_swin3d_mim.ipynb 19
# Copied from https://github.com/kuoweilai/pixelshuffle3d/blob/9be76091761caf3f3881eb5b3dc4b8da09315ab1/pixelshuffle3d.py#L6C1-L29C79
# Modified to support scale when it is different on different axis. 
class PixelShuffle3d(torch.nn.Module):
    '''
    This class is a 3d version of pixelshuffle.
    '''
    def __init__(self, scale):
        '''
        :param scale: upsample scale
        '''
        super().__init__()
        self.scale = scale

    def forward(self, input):
        batch_size, channels, in_depth, in_height, in_width = input.size()
        nOut = channels // (self.scale[0]*self.scale[1]*self.scale[2])

        out_depth = in_depth * self.scale[0]
        out_height = in_height * self.scale[1]
        out_width = in_width * self.scale[2]

        input_view = input.contiguous().view(batch_size, nOut, self.scale[0], self.scale[1], self.scale[2], in_depth, in_height, in_width)

        output = input_view.permute(0, 1, 5, 2, 6, 3, 7, 4).contiguous()

        return output.view(batch_size, nOut, out_depth, out_height, out_width)

# %% ../nbs/01a_swin3d_mim.ipynb 33
def mask_patches(num_patches, mask_ratio=0.5):
    len_keep = int(num_patches * (1 - mask_ratio))
    x = torch.cat([torch.zeros((len_keep)), torch.ones((num_patches-len_keep))])
    x = x[torch.randperm(num_patches)].view(1, -1)
    return x

# %% ../nbs/01a_swin3d_mim.ipynb 36
# Copied from transformers.models.swin.modeling_swin.SwinForMaskedImageModeling
class Swin3dForMaskedImageModeling(Swin3dPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        if len(config.encoder_stride) !=3: raise NotImplementedError("The length of encoder stride should be 3")
        self.swin = Swin3dModel(config, add_pooling_layer=False, use_mask_token=True)

        num_features = int(config.embed_dim * 2 ** (config.num_layers - 1))
        d_stride, h_stride, w_stride = config.encoder_stride
        self.decoder = torch.nn.Sequential(
            torch.nn.Conv3d(
                in_channels=num_features, out_channels=(d_stride*h_stride*w_stride) * config.num_channels, kernel_size=1
            ),
            PixelShuffle3d(config.encoder_stride),
        )
        
        self.num_patches = (config.image_size[0] // config.patch_size[0]) * \
                           (config.image_size[1] // config.patch_size[1]) * \
                           (config.image_size[2] // config.patch_size[2])

        # Initialize weights and apply final processing
        self.post_init()

    def forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        bool_masked_pos: Optional[torch.BoolTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, SwinMaskedImageModelingOutput]:
        
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.swin(
            pixel_values,
            bool_masked_pos=bool_masked_pos,
            head_mask=head_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        sequence_output = outputs[0]
        # Reshape to (batch_size, num_channels, depth, height, width)
        sequence_output = sequence_output.transpose(1, 2)
        batch_size, num_channels, sequence_length = sequence_output.shape
        depth= height = width = math.ceil(sequence_length**(1/3))
        sequence_output = sequence_output.reshape(batch_size, num_channels, depth, height, width)
        

        # Reconstruct pixel values
        reconstructed_pixel_values = self.decoder(sequence_output)

        masked_im_loss = None
        if bool_masked_pos is not None:
            size = (self.config.image_size[0] // self.config.patch_size[0],
                    self.config.image_size[1] // self.config.patch_size[1], 
                    self.config.image_size[2] // self.config.patch_size[2])
                    
            bool_masked_pos = bool_masked_pos.reshape(-1, size[0], size[1], size[2])
            mask = (bool_masked_pos.repeat_interleave(self.config.patch_size[0], 1)
                    .repeat_interleave(self.config.patch_size[1], 2)
                    .repeat_interleave(self.config.patch_size[2], 3)
                    .unsqueeze(1)
                    .contiguous()
                )
            reconstruction_loss = torch.nn.functional.l1_loss(pixel_values, reconstructed_pixel_values, reduction="none")
            masked_im_loss = (reconstruction_loss * mask).sum() / (mask.sum() + 1e-5) / self.config.num_channels

        if not return_dict:
            output = (reconstructed_pixel_values,) + outputs[2:]
            return ((masked_im_loss,) + output) if masked_im_loss is not None else output

        return SwinMaskedImageModelingOutput(
            loss=masked_im_loss,
            reconstruction=reconstructed_pixel_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            reshaped_hidden_states=outputs.reshaped_hidden_states,
        )
